<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络 | Andrew的个人博客</title><meta name="author" content="Andrew Wang"><meta name="copyright" content="Andrew Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言笔者有一定的机器学习和深度学习理论基础，对 Pytorch 的实战还不够熟悉，打算入职前专项突击一下 本文内容为笔者学习《动手学深度学习》一书的学习笔记 主要记录了代码的实现和实现过程遇到的问题，不完全包括其理论知识 引用： 《动手学深度学习》 一、预备知识1. 数据操作1.1 入门创建行向量 12# 创建行向量x &#x3D; torch.arange(12) 1tensor([ 0,  1,  2,">
<meta property="og:type" content="article">
<meta property="og:title" content="《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络">
<meta property="og:url" content="https://andreww1219.github.io/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Andrew的个人博客">
<meta property="og:description" content="前言笔者有一定的机器学习和深度学习理论基础，对 Pytorch 的实战还不够熟悉，打算入职前专项突击一下 本文内容为笔者学习《动手学深度学习》一书的学习笔记 主要记录了代码的实现和实现过程遇到的问题，不完全包括其理论知识 引用： 《动手学深度学习》 一、预备知识1. 数据操作1.1 入门创建行向量 12# 创建行向量x &#x3D; torch.arange(12) 1tensor([ 0,  1,  2,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://andreww1219.github.io/img/2024/10/%E5%AF%B9%E7%AD%96%E5%A7%94%E5%91%98%E4%BC%9A2.jpg">
<meta property="article:published_time" content="2024-10-07T13:15:00.000Z">
<meta property="article:modified_time" content="2024-10-07T13:23:53.944Z">
<meta property="article:author" content="Andrew Wang">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://andreww1219.github.io/img/2024/10/%E5%AF%B9%E7%AD%96%E5%A7%94%E5%91%98%E4%BC%9A2.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://andreww1219.github.io/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-07 21:23:53'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/loading.png" data-original="/img/%E5%A4%B4%E5%83%8F.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">50</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 博客</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://github.com/andreww1219"><span> GitHub</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://juejin.cn/user/3309987442983911"><span> 掘金</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2024/10/%E5%AF%B9%E7%AD%96%E5%A7%94%E5%91%98%E4%BC%9A2.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Andrew的个人博客"><span class="site-name">Andrew的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 博客</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://github.com/andreww1219"><span> GitHub</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://juejin.cn/user/3309987442983911"><span> 掘金</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-07T13:15:00.000Z" title="发表于 2024-10-07 21:15:00">2024-10-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-07T13:23:53.944Z" title="更新于 2024-10-07 21:23:53">2024-10-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>笔者有一定的机器学习和深度学习理论基础，对 Pytorch 的实战还不够熟悉，打算入职前专项突击一下</p>
<p>本文内容为笔者学习《动手学深度学习》一书的学习笔记</p>
<p>主要记录了代码的实现和实现过程遇到的问题，不完全包括其理论知识</p>
<p>引用：</p>
<p><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">《动手学深度学习》</a></p>
<h1 id="一、预备知识"><a href="#一、预备知识" class="headerlink" title="一、预备知识"></a>一、预备知识</h1><h2 id="1-数据操作"><a href="#1-数据操作" class="headerlink" title="1. 数据操作"></a>1. 数据操作</h2><h3 id="1-1-入门"><a href="#1-1-入门" class="headerlink" title="1.1 入门"></a>1.1 入门</h3><p>创建行向量</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建行向量</span></span><br><span class="line">x = torch.arange(<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>报错：module ‘numpy’ has no attribute ‘array’</p>
<p>解决方案：</p>
<p>numpy 版本过高，原版本为 1.21.5，使用以下命令安装 1.21.0 的 numpy</p>
<p>pip uninstall numpy</p>
<p>pip install numpy==1.21</p>
</blockquote>
<p>张量的基本操作</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 元素的数量</span></span><br><span class="line"><span class="built_in">print</span>(x.numel())</span><br><span class="line"><span class="comment"># 张量的形状</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># 更改形状</span></span><br><span class="line"><span class="built_in">print</span>(x.reshape(<span class="number">3</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">12</span><br><span class="line">torch.Size([12])</span><br><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure>
<p>创建张量</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全零张量</span></span><br><span class="line"><span class="built_in">print</span>(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))</span><br><span class="line"><span class="comment"># 全一张量</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))</span><br><span class="line"><span class="comment"># 随机数张量</span></span><br><span class="line"><span class="built_in">print</span>(torch.rand((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))</span><br><span class="line"><span class="comment"># 创建时初始化</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]]])</span><br><span class="line">tensor([[[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]]])</span><br><span class="line">tensor([[[0.2715, 0.4234, 0.4764, 0.5638],</span><br><span class="line">         [0.0958, 0.8449, 0.0129, 0.3975],</span><br><span class="line">         [0.4510, 0.2093, 0.6003, 0.6838]],</span><br><span class="line"></span><br><span class="line">        [[0.7996, 0.2331, 0.8481, 0.6440],</span><br><span class="line">         [0.6056, 0.7846, 0.6360, 0.6849],</span><br><span class="line">         [0.0169, 0.4028, 0.7457, 0.1688]]])</span><br><span class="line">tensor([[2, 1, 4, 3],</span><br><span class="line">        [1, 2, 3, 4],</span><br><span class="line">        [4, 3, 2, 1]])</span><br></pre></td></tr></table></figure>
<h3 id="1-2-运算符"><a href="#1-2-运算符" class="headerlink" title="1.2 运算符"></a>1.2 运算符</h3><p>基本运算</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 四则运算</span></span><br><span class="line"><span class="built_in">print</span>(x + y, x - y, x * y, x / y, x ** y)</span><br><span class="line"><span class="comment"># 自然指数</span></span><br><span class="line"><span class="built_in">print</span>(torch.exp(x))</span><br><span class="line"><span class="comment"># 求和</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(x))</span><br><span class="line"><span class="comment"># 逻辑运算</span></span><br><span class="line"><span class="built_in">print</span>(x == y)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 3.,  4.,  6., 10.]) tensor([-1.,  0.,  2.,  6.]) tensor([ 2.,  4.,  8., 16.]) tensor([0.5000, 1.0000, 2.0000, 4.0000]) tensor([ 1.,  4., 16., 64.])</span><br><span class="line">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span><br><span class="line">tensor(15.)</span><br><span class="line">tensor([False,  True, False, False])</span><br></pre></td></tr></table></figure>
<p>连接运算</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量连接</span></span><br><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 按行 (3, 4) 和 (3, 4) 连接成 (6, 4)</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat((X, Y), dim=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># 按列 (3, 4) 和 (3, 4) 连接成 (3, 8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat((X, Y), dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.],</span><br><span class="line">        [ 2.,  1.,  4.,  3.],</span><br><span class="line">        [ 1.,  2.,  3.,  4.],</span><br><span class="line">        [ 4.,  3.,  2.,  1.]])</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="line">        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])</span><br></pre></td></tr></table></figure>
<h3 id="1-3-广播机制"><a href="#1-3-广播机制" class="headerlink" title="1.3 广播机制"></a>1.3 广播机制</h3><p>pytorch 允许不同维度的张量做运算</p>
<p>当两个张量满足以下规则时，允许将维度较小的张量广播至维度较大的张量：</p>
<p>从<strong>尾部</strong>的维度起，两个张量的维度：</p>
<ol>
<li><strong>相等</strong></li>
<li>或 <strong>其中一个维度为1</strong></li>
<li>或 <strong>其中一个维度不存在</strong></li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广播机制</span></span><br><span class="line"><span class="comment"># 简单版</span></span><br><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(a + b, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 复杂版</span></span><br><span class="line"><span class="comment"># 从尾部维度数起，d最后一个维度为1，c与d倒数第二个维度相等，d倒数第三个维度不存在。故可广播</span></span><br><span class="line">c = torch.arange(<span class="number">12</span>).reshape((<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">d = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(c + d)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0],</span><br><span class="line">        [1],</span><br><span class="line">        [2]])</span><br><span class="line">tensor([[0, 1]])</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [1, 2],</span><br><span class="line">        [2, 3]]) </span><br><span class="line"></span><br><span class="line">tensor([[[ 0,  1],</span><br><span class="line">         [ 2,  3],</span><br><span class="line">         [ 4,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7],</span><br><span class="line">         [ 8,  9],</span><br><span class="line">         [10, 11]]])</span><br><span class="line">tensor([[0],</span><br><span class="line">        [1],</span><br><span class="line">        [2]])</span><br><span class="line">tensor([[[ 0,  1],</span><br><span class="line">         [ 3,  4],</span><br><span class="line">         [ 6,  7]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7],</span><br><span class="line">         [ 9, 10],</span><br><span class="line">         [12, 13]]])</span><br></pre></td></tr></table></figure>
<h3 id="1-4-索引和切片"><a href="#1-4-索引和切片" class="headerlink" title="1.4 索引和切片"></a>1.4 索引和切片</h3><p>用法同 numpy</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引和切片，同 numpy</span></span><br><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="comment"># 利用切片取值</span></span><br><span class="line"><span class="built_in">print</span>(X[-<span class="number">1</span>])    <span class="comment"># 相当于 X[X.shape[0] - 1]</span></span><br><span class="line"><span class="built_in">print</span>(X[<span class="number">1</span>: <span class="number">3</span>])  <span class="comment"># 左闭右开 X[a, b] 相当于 [a, b)</span></span><br><span class="line"><span class="comment"># 利用切片赋值</span></span><br><span class="line">X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br><span class="line">tensor([ 8.,  9., 10., 11.])</span><br><span class="line">tensor([[ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  9.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br><span class="line">tensor([[12., 12., 12., 12.],</span><br><span class="line">        [12., 12., 12., 12.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure>
<h3 id="1-5-节省内存"><a href="#1-5-节省内存" class="headerlink" title="1.5 节省内存"></a>1.5 节省内存</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浪费内存的写法</span></span><br><span class="line">X = X + Y</span><br><span class="line"><span class="comment"># 节省内存的写法</span></span><br><span class="line">X += Y</span><br><span class="line">X[:] = X + Y</span><br></pre></td></tr></table></figure>
<h3 id="1-6-转换为其它-Python-对象"><a href="#1-6-转换为其它-Python-对象" class="headerlink" title="1.6 转换为其它 Python 对象"></a>1.6 转换为其它 Python 对象</h3><p>使用 .numpy() 将 ndarray 转 tensor</p>
<p>使用 .item() 取单个元素为 Python 基本类型元素</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line"><span class="comment"># ndarray 转 tensor</span></span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(A), <span class="built_in">type</span>(B))</span><br><span class="line"><span class="comment"># 使用 .item() 取单个元素为 Python 基本类型元素</span></span><br><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line"><span class="built_in">print</span>(a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt; &lt;class <span class="string">&#x27;torch.Tensor&#x27;</span>&gt;</span><br><span class="line">tensor([3.5000]) 3.5 3.5 3</span><br></pre></td></tr></table></figure>
<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1 读取数据集"></a>2.1 读取数据集</h3><p>利用 pandas 读取数据集</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;datas&#x27;</span>, <span class="string">&#x27;heart&#x27;</span>, <span class="string">&#x27;heart.csv&#x27;</span>)</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data.head())</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   age  sex  <span class="built_in">cp</span>  trestbps  chol  ...  oldpeak  slope  ca        thal  target</span><br><span class="line">0   63    1   1       145   233  ...      2.3      3   0       fixed       0</span><br><span class="line">1   67    1   4       160   286  ...      1.5      2   3      normal       1</span><br><span class="line">2   67    1   4       120   229  ...      2.6      2   2  reversible       0</span><br><span class="line">3   37    1   3       130   250  ...      3.5      3   0      normal       0</span><br><span class="line">4   41    0   2       130   204  ...      1.4      1   0      normal       0</span><br><span class="line"></span><br><span class="line">[5 rows x 14 columns]</span><br></pre></td></tr></table></figure>
<h3 id="2-2-处理缺失值"><a href="#2-2-处理缺失值" class="headerlink" title="2.2 处理缺失值"></a>2.2 处理缺失值</h3><p>详见：<a target="_blank" rel="noopener" href="https://juejin.cn/post/7358649535381651496#heading-36">Pandas数据分析学习笔记- 掘金 (juejin.cn)</a></p>
<h3 id="2-3-转换为张量"><a href="#2-3-转换为张量" class="headerlink" title="2.3 转换为张量"></a>2.3 转换为张量</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x, y = data.iloc[:, :-<span class="number">2</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(x.head())</span><br><span class="line"><span class="built_in">print</span>(y.head())</span><br><span class="line"><span class="comment"># 需要先转换为 ndarray 再转换为 tensor</span></span><br><span class="line">X = torch.tensor(x.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">Y = torch.tensor(y.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(X), X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(Y), Y.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[5 rows x 14 columns]</span><br><span class="line">   age  sex  <span class="built_in">cp</span>  trestbps  chol  ...  thalach  exang  oldpeak  slope  ca</span><br><span class="line">0   63    1   1       145   233  ...      150      0      2.3      3   0</span><br><span class="line">1   67    1   4       160   286  ...      108      1      1.5      2   3</span><br><span class="line">2   67    1   4       120   229  ...      129      1      2.6      2   2</span><br><span class="line">3   37    1   3       130   250  ...      187      0      3.5      3   0</span><br><span class="line">4   41    0   2       130   204  ...      172      0      1.4      1   0</span><br><span class="line"></span><br><span class="line">[5 rows x 12 columns]</span><br><span class="line">0    0</span><br><span class="line">1    1</span><br><span class="line">2    0</span><br><span class="line">3    0</span><br><span class="line">4    0</span><br><span class="line">Name: target, dtype: int64</span><br><span class="line">&lt;class <span class="string">&#x27;torch.Tensor&#x27;</span>&gt; torch.Size([303, 12])</span><br><span class="line">&lt;class <span class="string">&#x27;torch.Tensor&#x27;</span>&gt; torch.Size([303])</span><br></pre></td></tr></table></figure>
<h2 id="3-线性代数"><a href="#3-线性代数" class="headerlink" title="3. 线性代数"></a>3. 线性代数</h2><p>标量、向量、矩阵、张量、张量运算性质、降维部分与 numpy 相似，故略过</p>
<p>求和操作及其应用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求和</span></span><br><span class="line"><span class="built_in">print</span>(x, x.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 非降维求和</span></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(A / A.<span class="built_in">sum</span>(axis=<span class="number">0</span>))    <span class="comment"># 利用广播归一化</span></span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 2., 3.]) tensor(6.)</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.],</span><br><span class="line">        [12., 13., 14., 15.],</span><br><span class="line">        [16., 17., 18., 19.]])</span><br><span class="line">tensor([40., 45., 50., 55.])</span><br><span class="line">tensor([[0.0000, 0.0222, 0.0400, 0.0545],</span><br><span class="line">        [0.1000, 0.1111, 0.1200, 0.1273],</span><br><span class="line">        [0.2000, 0.2000, 0.2000, 0.2000],</span><br><span class="line">        [0.3000, 0.2889, 0.2800, 0.2727],</span><br><span class="line">        [0.4000, 0.3778, 0.3600, 0.3455]])</span><br></pre></td></tr></table></figure></p>
<p>矩阵、向量相关运算</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">y = torch.ones(<span class="number">4</span>, dtype = torch.float32)</span><br><span class="line"><span class="comment"># 点积</span></span><br><span class="line"><span class="built_in">print</span>(x, y, torch.dot(x, y))</span><br><span class="line"><span class="comment"># 矩阵-向量积</span></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.mv(A, x))</span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">B = torch.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.mm(A, B))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 2., 3.]) tensor([1., 1., 1., 1.]) tensor(6.)</span><br><span class="line">tensor([ 14.,  38.,  62.,  86., 110.])</span><br><span class="line">tensor([[ 6.,  6.,  6.],</span><br><span class="line">        [22., 22., 22.],</span><br><span class="line">        [38., 38., 38.],</span><br><span class="line">        [54., 54., 54.],</span><br><span class="line">        [70., 70., 70.]])</span><br></pre></td></tr></table></figure>
<p>范数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量的 L2-范数 及 矩阵的 F-范数</span></span><br><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.norm(u))</span><br><span class="line">A = torch.ones((<span class="number">4</span>, <span class="number">9</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.norm(A))</span><br><span class="line"><span class="comment"># L1-范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(5.)</span><br><span class="line">tensor(6.)</span><br><span class="line">tensor(7.)</span><br></pre></td></tr></table></figure>
<h2 id="4-微积分"><a href="#4-微积分" class="headerlink" title="4. 微积分"></a>4. 微积分</h2><p>绘制图线</p>
<p>参考资料：<br>xscale 和 yscale 的使用：<a target="_blank" rel="noopener" href="https://matplotlib.net.cn/stable/users/explain/axes/axes_scales.html">坐标轴刻度 — Matplotlib 3.9.0 文档 - Matplotlib 中文</a><br>ptl.gca 的含义：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45759229/article/details/137739775#:~:text=%60.g%20(">matplotlib plt.gca()学习-CSDN博客</a>%60%20%E6%98%AFP)<br>fmts 详解：<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_73832859/article/details/134061198">matplotlib.pyplot中的plot函数</a><br>fig、axes 等的含义：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38048756/article/details/117741677#:~:text=%E5%A6%82%E6%9E%9C%E5%B0%86Matplot#:~:text=%E5%A6%82%E6%9E%9C%E5%B0%86Matplot">plt、fig、axes、axis的含义_fig, axes-CSDN博客</a></p>
<p>绘制图线的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X, Y=<span class="literal">None</span>, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param X: 自变量</span></span><br><span class="line"><span class="string">    :param Y: 因变量</span></span><br><span class="line"><span class="string">    :param xlabel: 自变量的名称</span></span><br><span class="line"><span class="string">    :param ylabel: 因变量的名称</span></span><br><span class="line"><span class="string">    :param legend: 图例</span></span><br><span class="line"><span class="string">    :param xlim: X轴的取值范围</span></span><br><span class="line"><span class="string">    :param ylim: Y轴的取值范围</span></span><br><span class="line"><span class="string">    :param xscale: X轴的缩放方式，默认为 linear</span></span><br><span class="line"><span class="string">    :param yscale: Y轴的缩放方式，默认为 linear</span></span><br><span class="line"><span class="string">    :param fmts: 图线的类型，默认 &#x27;-&#x27;为实线, &#x27;m--&#x27;为红色虚线, &#x27;g-.&#x27;为绿色点划线, &#x27;r:&#x27;为红色点线</span></span><br><span class="line"><span class="string">    :param figsize: 整张图像的大小</span></span><br><span class="line"><span class="string">    :param axes: 已有的图像，默认为 None</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 确定图像大小</span></span><br><span class="line">    plt.figure(figsize=figsize)</span><br><span class="line">    <span class="comment"># 确定坐标轴</span></span><br><span class="line">    <span class="keyword">if</span> xlim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.xlim(xlim)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">        plt.ylim(ylim)</span><br><span class="line">    <span class="comment"># label为标记</span></span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    <span class="comment"># scale为缩放方式</span></span><br><span class="line">    plt.xscale(xscale)</span><br><span class="line">    plt.yscale(yscale)</span><br><span class="line">    <span class="comment"># plot为绘制图像的函数，，scale为缩放方式</span></span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        plt.plot(x, y, fmt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将标记绘制图例</span></span><br><span class="line">    plt.legend(legend)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.close()</span><br></pre></td></tr></table></figure>
<p>练习一：绘制函数 y = f(x) = x^3 - 1/x 及其在 x = 1 处切线的图像</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">练习一</span></span><br><span class="line"><span class="string">绘制函数 y = f(x) = x^3 - 1/x 及其在 x = 1 处切线的图像</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">f(1) = 0</span></span><br><span class="line"><span class="string">f&#x27;(x) = 3x^2 + 1/x^2    f&#x27;(1) = 3</span></span><br><span class="line"><span class="string">那么，x = 1 处切线方程为 y = 3x - 3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">3</span> - <span class="number">1</span>/x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0.1</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">plot(X=[x, x], Y=[f(x), <span class="number">3</span> * x - <span class="number">3</span>],</span><br><span class="line">     xlabel=<span class="string">&#x27;x&#x27;</span>, ylabel=<span class="string">&#x27;f(x)&#x27;</span>,</span><br><span class="line">     legend=[<span class="string">&#x27;f(x)&#x27;</span>, <span class="string">&#x27;Tangent Line(x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/1.png" alt="image.png"></p>
<h2 id="5-自动微分"><a href="#5-自动微分" class="headerlink" title="5. 自动微分"></a>5. 自动微分</h2><h3 id="5-1-基本用法"><a href="#5-1-基本用法" class="headerlink" title="5.1 基本用法"></a>5.1 基本用法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置自动微分</span></span><br><span class="line"><span class="comment"># 方式一：定义时设置</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 方式二：.requeres_grad(True）</span></span><br><span class="line"><span class="comment"># x = torch.arange(4.0)</span></span><br><span class="line"><span class="comment"># x.requires_grad(True)</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line"><span class="built_in">print</span>(y)    <span class="comment"># 此时，y 是一个计算图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可对 y 求导</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认梯度会积累，一般需要将梯度清空</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(28., grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([ 0.,  4.,  8., 12.])</span><br><span class="line">tensor([0., 0., 0., 0.])</span><br></pre></td></tr></table></figure>
<h3 id="5-2-非标量的反向传播"><a href="#5-2-非标量的反向传播" class="headerlink" title="5.2 非标量的反向传播"></a>5.2 非标量的反向传播</h3><p>$y = x \odot x$</p>
<p>得</p>
<p>$\frac{\partial y}{\partial x_i} = 2 x_i$</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非标量的反向传播 需要 .sum() 求和</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * x</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">x.grad.zero_()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 2., 4., 6.])</span><br></pre></td></tr></table></figure>
<h3 id="5-3-分离计算"><a href="#5-3-分离计算" class="headerlink" title="5.3 分离计算"></a>5.3 分离计算</h3><p>当 z = y <em> x ，y = x </em> x 时，并且我们希望将 y 视为常数，只考虑到 x 在 y 被计算后发挥的作用</p>
<p>需要分离 y 获得一个新变量 u，丢弃计算图中如何计算 y 的信息</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分离计算</span></span><br><span class="line"><span class="comment"># 没有使用分离计算</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * x</span><br><span class="line">z = y * x</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad, x.grad == y)</span><br><span class="line"><span class="comment"># 使用分离计算</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad, x.grad == u)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.,  3., 12., 27.]) tensor([ True, False, False, False])</span><br><span class="line">tensor([0., 1., 4., 9.]) tensor([True, True, True, True])</span><br></pre></td></tr></table></figure>
<h3 id="5-4-Python-控制流得梯度计算"><a href="#5-4-Python-控制流得梯度计算" class="headerlink" title="5.4 Python 控制流得梯度计算"></a>5.4 Python 控制流得梯度计算</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f(a) 是关于 a 得分段线性函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad == d / a)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(True)</span><br></pre></td></tr></table></figure>
<h2 id="6-概率"><a href="#6-概率" class="headerlink" title="6. 概率"></a>6. 概率</h2><p>参考资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/610992971">Pytorch中的多项分布multinomial.Multinomial().sample()解析 - 知乎 (zhihu.com)</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">probs = torch.ones(<span class="number">6</span>)</span><br><span class="line"><span class="comment"># total_count 为抽样次数，probs为样本，是一个tensor</span></span><br><span class="line">multinomial_distribution = multinomial.Multinomial(total_count=<span class="number">1</span>, probs=probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样</span></span><br><span class="line"><span class="built_in">print</span>(multinomial_distribution.sample())</span><br><span class="line"><span class="comment"># 对数概率分布</span></span><br><span class="line"><span class="built_in">print</span>(multinomial_distribution.logits)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 0., 1., 0., 0., 0.])</span><br><span class="line">tensor([-1.7918, -1.7918, -1.7918, -1.7918, -1.7918, -1.7918])</span><br></pre></td></tr></table></figure>
<h1 id="二、线性神经网络"><a href="#二、线性神经网络" class="headerlink" title="二、线性神经网络"></a>二、线性神经网络</h1><h2 id="1-线性回归从零开始实现"><a href="#1-线性回归从零开始实现" class="headerlink" title="1. 线性回归从零开始实现"></a>1. 线性回归从零开始实现</h2><h3 id="1-1-生成数据集"><a href="#1-1-生成数据集" class="headerlink" title="1.1 生成数据集"></a>1.1 生成数据集</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察第二个维度与标签的关系</span></span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/2.png" alt="image.png"></p>
<h3 id="1-2-读取数据集"><a href="#1-2-读取数据集" class="headerlink" title="1.2 读取数据集"></a>1.2 读取数据集</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小批量读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))     <span class="comment"># 样本下标</span></span><br><span class="line">    random.shuffle(indices)                 <span class="comment"># 打乱顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.0637,  0.3883],</span><br><span class="line">        [ 1.3318,  0.7545],</span><br><span class="line">        [ 1.0563,  1.2710],</span><br><span class="line">        [-0.6162, -0.2641],</span><br><span class="line">        [ 0.2506,  1.1129]]) </span><br><span class="line"> tensor([[5.0095],</span><br><span class="line">        [4.3107],</span><br><span class="line">        [1.9846],</span><br><span class="line">        [3.8708],</span><br><span class="line">        [0.9319]])</span><br></pre></td></tr></table></figure>
<h3 id="1-3-模型定义和训练"><a href="#1-3-模型定义和训练" class="headerlink" title="1.3 模型定义和训练"></a>1.3 模型定义和训练</h3><p>参考资料：</p>
<p>with torch.no_grad() 的作用：<a target="_blank" rel="noopener" href="https://blog.csdn.net/sazass/article/details/116668755">【pytorch】 with torch.no_grad():用法详解_pytorch with no grad-CSDN博客</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_reg</span>(<span class="params">w, b, X</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y - y_hat.reshape(y.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    小批量梯度下降</span></span><br><span class="line"><span class="string">    :param params: 参数</span></span><br><span class="line"><span class="string">    :param lr: 学习率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># with torch.no_grad(): 以内的空间计算结果得 requires_grad 为 False</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linear_reg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(w, b, X), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(w, b, features), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.292531</span><br><span class="line">epoch 2, loss 0.005235</span><br><span class="line">epoch 3, loss 0.000142</span><br></pre></td></tr></table></figure>
<h2 id="2-线性回归简洁实现"><a href="#2-线性回归简洁实现" class="headerlink" title="2. 线性回归简洁实现"></a>2. 线性回归简洁实现</h2><p>参考资料：</p>
<p>Python 星号的作用：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zkk9527/article/details/88675129">Python中的<em>（星号）和**(双星号）完全详解_python </em>-CSDN博客</a></p>
<p>TensorDataset 和 DataLoader：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qlkaicx/article/details/138503481#:~:text=torch.util">PyTorch中 DataLoader 和 TensorDataset 的详细解析_tensordataset会打乱顺序吗-CSDN博客</a></p>
<p>torch 中实现了更方便地读取数据的方法，只需要我们将 tensor 封装到 <strong>TensorDataset</strong> 中，再与 <strong>DataLoader</strong> 结合使用，即可实现前面 data_iter 的效果</p>
<p>DataLoader 的核心功能有：<strong>批量加载、打乱顺序、并行处理</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure>
<p>仍然按照步骤：定义模型 -&gt; 初始化模型参数 -&gt; 定义损失函数 -&gt; 定义优化算法 -&gt; 训练</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.552002</span><br><span class="line">epoch 2, loss 0.009066</span><br><span class="line">epoch 3, loss 0.000246</span><br></pre></td></tr></table></figure>
<h2 id="3-softmax"><a href="#3-softmax" class="headerlink" title="3. softmax"></a>3. softmax</h2><p>由交叉熵损失</p>
<p>$loss(y, \widehat{y}) = - \sum_{j=1}^{q} y_j log \hat{y_j}$</p>
<p>以及使用 softmax 函数时</p>
<p>$\hat{y} = softmax(o) = \frac{e^{o_j}}{\sum_{k=1}^q e^{o_k}}$</p>
<p>得</p>
<script type="math/tex; mode=display">
\begin{align}
loss(y, \widehat{y}) =& - \sum_{j=1}^{q} y_j log \frac{e^{o_j}}{\sum_{k=1}^q e^{o_k}} \\
=& \sum_{j=1}^{q} y_j log \sum_{k=1}^q e^{o_k} - \sum_{j=1}^{q} y_j o_j \\
=& log \sum_{k=1}^q e^{o_k} - \sum_{j=1}^{q} y_j o_j \\
\end{align}</script><p>那么</p>
<script type="math/tex; mode=display">
\begin{align}
\partial_{o_j} loss(y, \widehat{y}) =& \frac{e^{o_j}}{\sum_{k=1}^q e^{o_k}} - y_j \\
=& softmax(o)_j - y_j
\end{align}</script><h2 id="4-softmax-回归从零开始实现"><a href="#4-softmax-回归从零开始实现" class="headerlink" title="4. softmax 回归从零开始实现"></a>4. softmax 回归从零开始实现</h2><p>torchvision 的 transforms详解： <a target="_blank" rel="noopener" href="https://blog.csdn.net/lsb2002/article/details/134895212">pytorch中数据预处理模块：transforms详解</a></p>
<p>读取 Fashion-MINST 数据集</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">18</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=64a)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([18, 1, 64, 64])</span><br><span class="line">torch.Size([18])</span><br></pre></td></tr></table></figure>
<p>定义模型 + 初始化模型参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 求和后保留被求和的维度</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">W, b, X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure>
<p>定义损失函数 以及 预测准确率的函数</p>
<p>由于数据集的 y 不是独热编码，在得到 y_hat 时利用 argmax 得到最大数的下标</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y, y_hat</span>):</span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)        <span class="comment"># 得到每一行最大的下标</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())   <span class="comment"># 预测正确返回1，否则返回0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(W, b, X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>训练，同线性回归</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">loss = cross_entropy</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l = loss(net(W, b, X), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([W, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;acc:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, acc 0.801700</span><br><span class="line">epoch 2, acc 0.818800</span><br><span class="line">epoch 3, acc 0.822200</span><br></pre></td></tr></table></figure>
<p>动态展示 loss 和 acc</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时追踪 loss 和 acc</span></span><br><span class="line"><span class="comment"># 初始化绘图</span></span><br><span class="line">plt.ion()  <span class="comment"># 开启交互模式</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">loss_list = []</span><br><span class="line">acc_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l = loss(net(W, b, X), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([W, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, acc <span class="subst">&#123;acc:f&#125;</span>&#x27;</span>)</span><br><span class="line">        loss_list.append(l.<span class="built_in">sum</span>().item())</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line">        <span class="comment"># 更新绘图</span></span><br><span class="line">        ax1.clear()</span><br><span class="line">        ax1.plot(loss_list, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">        ax1.set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        ax1.set_ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">        ax2.clear()</span><br><span class="line">        ax2.plot(acc_list, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">        ax2.set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        ax2.set_ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)  <span class="comment"># 暂停一小段时间以更新图形</span></span><br><span class="line">plt.ioff()  <span class="comment"># 关闭交互模式</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/3.png" alt="image.png"></p>
<h2 id="5-softmax-回归简洁实现"><a href="#5-softmax-回归简洁实现" class="headerlink" title="5. softmax 回归简洁实现"></a>5. softmax 回归简洁实现</h2><p>nn.Linear 于 nn.Dense 等价</p>
<p>model.apply(fn) 会递归地将函数 fn 应用到父模块的每个子模块 submodule</p>
<p>nn.CrossEntropyLoss() 的参数 reduction 可以指定输出的归约方式。默认为’mean’，详见：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42426841/article/details/139889247">PyTorch nn.CrossEntropyLoss() 交叉熵损失函数详解和要点提醒</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l = loss(net(X), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.mean().backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, acc <span class="subst">&#123;acc:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, acc 0.800500</span><br><span class="line">epoch 2, acc 0.818300</span><br><span class="line">epoch 3, acc 0.825700</span><br><span class="line">epoch 4, acc 0.824500</span><br><span class="line">epoch 5, acc 0.828200</span><br><span class="line">epoch 6, acc 0.825700</span><br><span class="line">epoch 7, acc 0.834300</span><br><span class="line">epoch 8, acc 0.836000</span><br><span class="line">epoch 9, acc 0.833700</span><br><span class="line">epoch 10, acc 0.831000</span><br></pre></td></tr></table></figure>
<h1 id="三、多层感知机"><a href="#三、多层感知机" class="headerlink" title="三、多层感知机"></a>三、多层感知机</h1><h2 id="1-多层感知机"><a href="#1-多层感知机" class="headerlink" title="1. 多层感知机"></a>1. 多层感知机</h2><h3 id="1-1-多层感知机引入"><a href="#1-1-多层感知机引入" class="headerlink" title="1.1 多层感知机引入"></a>1.1 多层感知机引入</h3><p>多层感知机在每层输出利用激活函数，使之得到非线性的结果，不至于退化为线性</p>
<p>不使用激活函数：</p>
<script type="math/tex; mode=display">
\begin{align}
H =& XW^{(1)} + b^{(1)} \\
O =& HW^{(2)} + b^{(2)}  
\end{align}</script><p>则有</p>
<script type="math/tex; mode=display">
\begin{align}
O =& HW^{(2)} + b^{(2)}  \\
=& (XW^{(1)} + b^{(1)}) W^{(2)} + b^{(2)} \\
=& XW^{(1)}W^{(2)} + b^{(1)}W^{(2)} + b^{(2)} \\
=& XW + b
\end{align}</script><p>使用激活函数：</p>
<script type="math/tex; mode=display">
\begin{align}
H =& \sigma_1(XW^{(1)} + b^{(1)}) \\
O =& \sigma_2(HW^{(2)} + b^{(2)})  
\end{align}</script><h3 id="1-2-常用激活函数"><a href="#1-2-常用激活函数" class="headerlink" title="1.2 常用激活函数"></a>1.2 常用激活函数</h3><p><strong>relu(x)</strong></p>
<script type="math/tex; mode=display">y = relu(x) = max(0, x)</script><script type="math/tex; mode=display">
y'(x) =
\begin{cases}
1,\,\,x>0\\
0,\,\,x\le0\\
\end{cases}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用激活函数</span></span><br><span class="line">x = torch.arange(-<span class="number">5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># relu</span></span><br><span class="line">y = torch.relu(x)</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x_np = x.detach().numpy()</span><br><span class="line">plot([x_np, x_np], [y.detach().numpy(), x.grad.numpy()],</span><br><span class="line">        xlabel=<span class="string">&#x27;x&#x27;</span>, ylabel=<span class="string">&#x27;relu(x)&#x27;</span>,</span><br><span class="line">        figsize=(<span class="number">12</span>, <span class="number">12</span>), legend=[<span class="string">&#x27;relu(x)&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>(x)<span class="string">&#x27;])</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/4.png" alt="image.png"></p>
<p><strong>sigmoid(x)</strong></p>
<script type="math/tex; mode=display">y = sigmoid(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">y'(x) = y(1 - y)</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">plot([x_np, x_np], [y.detach().numpy(), x.grad.numpy()],</span><br><span class="line">        xlabel=<span class="string">&#x27;x&#x27;</span>, ylabel=<span class="string">&#x27;sigmoid(x)&#x27;</span>,</span><br><span class="line">        figsize=(<span class="number">12</span>, <span class="number">12</span>), legend=[<span class="string">&#x27;sigmoid(x)&#x27;</span>, <span class="string">&#x27;sigmoid&#x27;</span>(x)<span class="string">&#x27;])</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/5.png" alt="image.png"></p>
<p><strong>tanh(x)</strong></p>
<script type="math/tex; mode=display">y = tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}</script><script type="math/tex; mode=display">y'(x) = \frac{4 e^{-2x}}{(1 + e^{-2x})^2} = 1 - y^2</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tanh</span></span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">plot([x_np, x_np], [y.detach().numpy(), x.grad.numpy()],</span><br><span class="line">        xlabel=<span class="string">&#x27;x&#x27;</span>, ylabel=<span class="string">&#x27;tanh(x)&#x27;</span>,</span><br><span class="line">        figsize=(<span class="number">12</span>, <span class="number">12</span>), legend=[<span class="string">&#x27;tanh(x)&#x27;</span>, <span class="string">&#x27;tanh&#x27;</span>(x)<span class="string">&#x27;])</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/loading.png" data-original="/img/2024/10/torch1/6.png" alt="image.png"></p>
<h2 id="2-多层感知机从零实现"><a href="#2-多层感知机从零实现" class="headerlink" title="2. 多层感知机从零实现"></a>2. 多层感知机从零实现</h2><p>初始化模型参数</p>
<p>nn.Parameter 的对象的 requires_grad 属性的默认值是 True</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(</span><br><span class="line">    num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(</span><br><span class="line">    num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure>
<p>定义模型，结果记得要加 softmax</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    <span class="keyword">return</span> softmax(H @ W2 + b2)</span><br></pre></td></tr></table></figure>
<p>定义损失函数和优化算法及训练同上一章 softmax 回归</p>
<h2 id="3-多层感知机简洁实现"><a href="#3-多层感知机简洁实现" class="headerlink" title="3. 多层感知机简洁实现"></a>3. 多层感知机简洁实现</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型，初始化模型参数</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l = loss(net(X), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.mean().backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, acc <span class="subst">&#123;acc:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-正则化技术"><a href="#4-正则化技术" class="headerlink" title="4. 正则化技术"></a>4. 正则化技术</h2><h3 id="4-1-添加正则化项"><a href="#4-1-添加正则化项" class="headerlink" title="4.1 添加正则化项"></a>4.1 添加正则化项</h3><p>从零实现</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加了L2范数惩罚项，</span></span><br><span class="line"><span class="comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class="line">l = loss(net(X), y) + lambd * l2_penalty(w)</span><br></pre></td></tr></table></figure>
<p>简洁实现</p>
<p>定义优化算法时，设置参数的 weight_decay，表示正则化项的系数。</p>
<p>pytorch 默认只提供 L2 范数，如果需要 L1 范数，需要自行在损失函数上添加<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD([</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br></pre></td></tr></table></figure></p>
<h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>Dropout 一般仅在训练过程中使用</p>
<p>按 dropout 比例丢弃元素，并进行缩放（除以 1 - dropout）</p>
<p>torch.rand 生成 [0, 1)的均匀分布的随机数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = torch.rand(X.shape) &gt; dropout</span><br><span class="line">    <span class="keyword">return</span> mask.<span class="built_in">float</span>() * X / (<span class="number">1</span> - dropout)</span><br><span class="line"></span><br><span class="line">X= torch.arange(<span class="number">16</span>, dtype = torch.float32).reshape((<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">1.</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span><br><span class="line">tensor([[ 0.,  2.,  4.,  0.,  0.,  0., 12., 14.],</span><br><span class="line">        [ 0.,  0.,  0.,  0.,  0., 26., 28., 30.]])</span><br><span class="line">tensor([[0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure>
<p>定义模型时，前向传播使用 Dropout</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> softmax(out)</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure>
<p>简洁实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>
<h2 id="5-模型初始化"><a href="#5-模型初始化" class="headerlink" title="5. 模型初始化"></a>5. 模型初始化</h2><p>参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27919794">深度前馈网络与Xavier初始化原理 - 知乎 (zhihu.com)</a></p>
<p>默认初始化为 $U[0, 1]$ 可以满足中等难度的问题</p>
<p>但是问题的规模增大时，后续层的输出的方差可能接近 0</p>
<p>为了尽可能让样本空间和类别空间的方差相近，可使用 Xavier 初始化：</p>
<script type="math/tex; mode=display">w \sim U[-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}} ]</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># 均匀分布的 Xavier 初始化</span></span><br><span class="line">        <span class="comment"># nn.init.xavier_normal_(m.weight)  # 正态分布的 Xavier 初始化</span></span><br><span class="line">        <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)  <span class="comment"># 偏置项初始化为 0</span></span><br></pre></td></tr></table></figure>
<h2 id="6-环境和分布偏移"><a href="#6-环境和分布偏移" class="headerlink" title="6. 环境和分布偏移"></a>6. 环境和分布偏移</h2><h3 id="6-1-分布偏移"><a href="#6-1-分布偏移" class="headerlink" title="6.1 分布偏移"></a>6.1 分布偏移</h3><p>训练集和测试集的分布不一致叫做分布偏移</p>
<p>分布偏移包含三种类型：</p>
<ol>
<li><strong>协变量偏移</strong>：输入的分布 $q(x, y)$ 发生变化</li>
<li><strong>标签偏移</strong>：标签的边缘概率 $P(y)$ 发生变化</li>
<li><strong>概念偏移</strong>：标签的定义发生变化</li>
</ol>
<h3 id="6-2-经验风险和实际风险"><a href="#6-2-经验风险和实际风险" class="headerlink" title="6.2 经验风险和实际风险"></a>6.2 经验风险和实际风险</h3><p>我们在最小化损失函数时使用的损失函数叫<strong>经验风险</strong>，例如：</p>
<script type="math/tex; mode=display">min_f \frac{1}{n} \sum_{i=1}^n l(f(x_i), y_i)</script><p>经验风险实际上是为了去近似<strong>实际风险</strong>，即从实际分布 $p(x, y)$ 去抽取数据进行预测的总损失的期望值：</p>
<script type="math/tex; mode=display">E_{p(x, y)}[l(f(x), y)] = \int \int l(f(x), y) p(x, y) dxdy</script><h3 id="6-3-纠正协变量偏移"><a href="#6-3-纠正协变量偏移" class="headerlink" title="6.3 纠正协变量偏移"></a>6.3 纠正协变量偏移</h3><p>由 $p(y| x) = q(y| x)$，利用以下等式进行纠正</p>
<script type="math/tex; mode=display">\int \int l(f(x), y) p(y| x) p(x) dxdy = \int \int l(f(x), y) q(y| x) \frac{p(x)}{q(x)} dxdy</script><p>定义</p>
<script type="math/tex; mode=display">\beta_i = \frac{p(x_i)}{q(x_i)}</script><p>令损失函数为</p>
<script type="math/tex; mode=display">min_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(x_i), y_i)</script><p>再进行训练</p>
<p>实际操作中，会利用 $p(x)$ 和 $q(x)$训练一个分类器：</p>
<script type="math/tex; mode=display">\frac{p(x)}{q(x)} = \frac{ 1 / (1 + e^{-h(x_i)}) }{ 1 - 1 / (1 + e^{-h(x_i)}) } = e^{-h(x_i)}</script><p>即 $\beta_i = e^{-h(x_i)}$</p>
<h3 id="6-4-纠正标签偏移"><a href="#6-4-纠正标签偏移" class="headerlink" title="6.4 纠正标签偏移"></a>6.4 纠正标签偏移</h3><p>同理有</p>
<script type="math/tex; mode=display">\int \int l(f(x), y) p(x | y) p(y) dxdy = \int \int l(f(x), y) q(x | y) \frac{p(y)}{q(y)} dxdy</script><script type="math/tex; mode=display">\beta_i = \frac{p(y_i)}{q(y_i)}</script><p>定义混淆矩阵$C$</p>
<p>$C_{i, j}$ 为 模型预测为标签 $i$ 但是真实标签为 $j$ 的数据所占比例</p>
<p>记 $\mu(y)$ 是模型在测试时的预测的平均输出，则有</p>
<script type="math/tex; mode=display">C p(y) = \mu(y)</script><p>那么</p>
<script type="math/tex; mode=display">p(y) = C^{-1} \mu(y)</script><p>又易得$q(y)$，故可求得$\frac{p(y)}{q(y)}$</p>
<h1 id="四、深度学习计算"><a href="#四、深度学习计算" class="headerlink" title="四、深度学习计算"></a>四、深度学习计算</h1><h2 id="1-层和块"><a href="#1-层和块" class="headerlink" title="1. 层和块"></a>1. 层和块</h2><p>一个层可以由多个层或多个块组成</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>块中可以编写任意的计算，包括 Python 代码</p>
<h2 id="2-参数操作"><a href="#2-参数操作" class="headerlink" title="2. 参数操作"></a>2. 参数操作</h2><h3 id="2-1-参数访问"><a href="#2-1-参数访问" class="headerlink" title="2.1 参数访问"></a>2.1 参数访问</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数访问</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="comment"># 只访问参数的值，即 requires_grad = False</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment"># 访问包含所有参数的字典</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.1868, -0.2378, -0.1849,  0.1916, -0.0438,  0.0436,  0.0416,  0.2273]],</span><br><span class="line">       requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.2626], requires_grad=True)</span><br><span class="line">tensor([0.2626])</span><br><span class="line">OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[-0.1868, -0.2378, -0.1849,  0.1916, -0.0438,  0.0436,  0.0416,  0.2273]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([0.2626]))])</span><br></pre></td></tr></table></figure>
<p>named_parameters 返回的是 (name, param) 元组的列表</p>
<p>parameters 仅返回参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>([(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>([(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="built_in">print</span>([param.shape <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters()])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;weight&#x27;</span>, torch.Size([8, 4])), (<span class="string">&#x27;bias&#x27;</span>, torch.Size([8]))]</span><br><span class="line">[(<span class="string">&#x27;0.weight&#x27;</span>, torch.Size([8, 4])), (<span class="string">&#x27;0.bias&#x27;</span>, torch.Size([8])), (<span class="string">&#x27;2.weight&#x27;</span>, torch.Size([1, 8])), (<span class="string">&#x27;2.bias&#x27;</span>, torch.Size([1]))]</span><br><span class="line">[torch.Size([8, 4]), torch.Size([8]), torch.Size([1, 8]), torch.Size([1])]</span><br></pre></td></tr></table></figure>
<p>嵌套块的参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌套块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rgnet)</span><br><span class="line"><span class="built_in">print</span>([(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].named_parameters()])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(in_features=4, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">[(<span class="string">&#x27;weight&#x27;</span>, torch.Size([8, 4])), (<span class="string">&#x27;bias&#x27;</span>, torch.Size([8]))]</span><br></pre></td></tr></table></figure>
<h3 id="2-2-参数初始化"><a href="#2-2-参数初始化" class="headerlink" title="2.2 参数初始化"></a>2.2 参数初始化</h3><script type="math/tex; mode=display">
w \sim
\begin{cases}
U(5, 10), \,\, 概率为 \frac{1}{4} \\
0, \,\, 概率为\frac{1}{2} \\
U(-5, -10), \,\, 概率为 \frac{1}{4} \\
\end{cases}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 7.5637, -9.7594, -0.0000, -6.0625],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000, -5.8875]], grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-参数共享"><a href="#2-3-参数共享" class="headerlink" title="2.3 参数共享"></a>2.3 参数共享</h3><p>多次指定同一个 Module 对象</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数共享</span></span><br><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br></pre></td></tr></table></figure>
<h3 id="2-4-延后初始化"><a href="#2-4-延后初始化" class="headerlink" title="2.4 延后初始化"></a>2.4 延后初始化</h3><p>允许只指定输出维度</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 延后初始化</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.LazyLinear(<span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.LazyLinear(<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">X = torch.rand((<span class="number">5</span>, <span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(net(X).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 10])</span><br></pre></td></tr></table></figure>
<h2 id="3-自定义层"><a href="#3-自定义层" class="headerlink" title="3. 自定义层"></a>3. 自定义层</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MeanLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean(axis=<span class="number">1</span>)</span><br><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">layer = MeanLayer()</span><br><span class="line"><span class="built_in">print</span>(layer(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0670, 0.8884, 0.4443, 0.3047, 0.5332]])</span><br><span class="line">tensor([[-0.3805,  0.4409, -0.0032, -0.1429,  0.0857]])</span><br></pre></td></tr></table></figure>
<h2 id="4-保存模型"><a href="#4-保存模型" class="headerlink" title="4. 保存模型"></a>4. 保存模型</h2><p>保存/读取张量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 保存/读取张量</span><br><span class="line">x = torch.arange(4)</span><br><span class="line">torch.save(x, &#x27;x-file&#x27;)</span><br><span class="line">y = torch.load(&#x27;x-file&#x27;)</span><br><span class="line">print(x, y)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 1, 2, 3]) tensor([0, 1, 2, 3])</span><br></pre></td></tr></table></figure>
<p>保存/读取字典</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 保存/读取字典</span><br><span class="line">mydict = &#123;&#x27;x&#x27;: x, &#x27;y&#x27;: y&#125;</span><br><span class="line">torch.save(mydict, &#x27;mydict&#x27;)</span><br><span class="line">mydict2 = torch.load(&#x27;mydict&#x27;)</span><br><span class="line">print(mydict, mydict2)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 1, 2, 3]) tensor([0, 1, 2, 3])</span><br><span class="line">&#123;<span class="string">&#x27;x&#x27;</span>: tensor([0, 1, 2, 3]), <span class="string">&#x27;y&#x27;</span>: tensor([0, 1, 2, 3])&#125; &#123;<span class="string">&#x27;x&#x27;</span>: tensor([0, 1, 2, 3]), <span class="string">&#x27;y&#x27;</span>: tensor([0, 1, 2, 3])&#125;</span><br></pre></td></tr></table></figure>
<p>保存/读取模型参数</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存/读取模型参数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line">net = MLP()</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp_state_dict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp_state_dict&#x27;</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(net(X) == clone(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True, True, True, True, True, True, True, True]])</span><br></pre></td></tr></table></figure>
<h2 id="5-GPU"><a href="#5-GPU" class="headerlink" title="5. GPU"></a>5. GPU</h2><p>查看设备的 GPU 运行状况</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@Andrew:~<span class="comment"># watch -n 2 nvidia-smi</span></span><br></pre></td></tr></table></figure>
<p>查看 Pytorch 中 GPU的可用情况</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPU 使用情况</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.device_count())</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>指定张量的存储设备</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定张量的环境</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> i &lt; torch.cuda.device_count():</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line"><span class="built_in">print</span>(X.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpu</span><br><span class="line">cuda:0</span><br></pre></td></tr></table></figure>
<p>张量需在同一个设备中才能进行运算，需复制张量到不同设备</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复制张量到不同设备</span></span><br><span class="line"></span><br><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Z = X.cuda(<span class="number">1</span>)  <span class="comment"># 之后才能将 Y 和 Z 做运算</span></span><br></pre></td></tr></table></figure>
<p>将神经网络保存在同一设备</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 神经网络与 GPU</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net(X))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.1118],</span><br><span class="line">        [-1.1118]], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">cuda:0</span><br></pre></td></tr></table></figure>
<h1 id="五、卷积神经网络"><a href="#五、卷积神经网络" class="headerlink" title="五、卷积神经网络"></a>五、卷积神经网络</h1><h2 id="1-从全连接层到卷积"><a href="#1-从全连接层到卷积" class="headerlink" title="1. 从全连接层到卷积"></a>1. 从全连接层到卷积</h2><p>如何从“全连接”引入到卷积？</p>
<p>需要看在图像处理中，有什么需求</p>
<p>在多层感知机中：</p>
<script type="math/tex; mode=display">
\begin{align}
H_{i, j} =& U_{i, j} + \sum_k \sum_j W_{i, j, k, l} X_{k, l}  \\
=& U_{i, j} + \sum_a \sum_b V_{i, j, a, b} X_{i + a, j + b}
\end{align}</script><p>其中，$k = i + a, l = j + b, V_{i, j, a, b} = W_{i, j, i + a, j + b}$</p>
<p>图像处理中，我们希望识别对象 X 的平移，仅导致其隐藏含义的平移 H（位置的改变而非值），即 <strong>平移不变性</strong>，数学表示如下：</p>
<p>引入平移不变性，那么令$V_{i, j, a, b} = V_{a, b}$，$U_{i, j} = u$，得</p>
<script type="math/tex; mode=display">H_{i, j} = u + \sum_a \sum_b V_{a, b} X_{i + a, j + b}</script><p>另外，我们希望神经网络的前几层只探索图像的局部信息，不希望 $H_{i, j}$ 的值被偏离的很远的像素影响，即<strong>局部性</strong>，数学表示为：</p>
<script type="math/tex; mode=display">H_{i, j} = u + \sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta V_{a, b} X_{i + a, j + b}</script><p>多通道情况下，令输入通道为 $c$，输出通道为 $d$，则有：</p>
<script type="math/tex; mode=display">H_{i, j, d} = u + \sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta \sum_c V_{a, b, c, d} X_{i + a, j + b, c}</script><h2 id="2-图像卷积"><a href="#2-图像卷积" class="headerlink" title="2. 图像卷积"></a>2. 图像卷积</h2><p>手撕卷积层</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积操作定义</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i+h, j:j+w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"><span class="comment"># 卷积层定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.kernel = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> conv2d(X, self.kernel) + self.bias</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">16</span>).reshape(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">conv2dLayer = Conv2D(kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(conv2dLayer.kernel)</span><br><span class="line"><span class="built_in">print</span>(conv2dLayer(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15]])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[0.2574, 0.7522],</span><br><span class="line">        [0.7505, 0.0085]], requires_grad=True)</span><br><span class="line">tensor([[ 3.7968,  5.5654,  7.3340],</span><br><span class="line">        [10.8712, 12.6398, 14.4084],</span><br><span class="line">        [17.9456, 19.7142, 21.4828]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>卷积层用于边缘检测</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 边缘检测</span></span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">1</span></span><br><span class="line">K = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, K)</span><br><span class="line"><span class="built_in">print</span>(conv2d(X, K))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 1., 1., 1., 1., 0., 0.],</span><br><span class="line">        [0., 0., 1., 1., 1., 1., 0., 0.],</span><br><span class="line">        [0., 0., 1., 1., 1., 1., 0., 0.],</span><br><span class="line">        [0., 0., 1., 1., 1., 1., 0., 0.]]) </span><br><span class="line"> tensor([[ 1, -1]])</span><br><span class="line">tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">        [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])</span><br></pre></td></tr></table></figure>
<p>卷积层的输出又叫做<strong>特征映射(feature map)</strong>，其覆盖的输入又称之<strong>感受野(receptive field)</strong></p>
<h2 id="3-填充和步幅"><a href="#3-填充和步幅" class="headerlink" title="3. 填充和步幅"></a>3. 填充和步幅</h2><p>由于 nn.Conv2D 只能接收 3D or 4D 的张量，即前两个维度为 批大小、通道大小。故编写函数先将输入 X reshape，再作卷积操作</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br></pre></td></tr></table></figure>
<p>填充和步幅示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d, X).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步幅</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d, X).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([8, 8])</span><br><span class="line">torch.Size([3, 3])</span><br></pre></td></tr></table></figure>
<p>复杂示例：</p>
<script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - K}{S} + 1</script><p>其中，$H_in$ 为输入维度，$H_out$为输出维度，$P$为填充，$K$为卷积核维度，$S$为步幅</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复杂示例</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># X.shape = (8, 8)</span></span><br><span class="line"><span class="comment"># 正常经过 conv2d 输出的 shape 为 （6, 4)</span></span><br><span class="line"><span class="comment"># 由于 padding 是 (0, 1) 那么输入的 shape 变为 (8, 10)</span></span><br><span class="line"><span class="comment"># 那么 stride 为 1 时输出 (6, 6)</span></span><br><span class="line"><span class="comment"># stride 为 (3, 4) 时，输出为 (2, 2)</span></span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d, X).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([8, 8])</span><br><span class="line">torch.Size([3, 3])</span><br><span class="line">torch.Size([2, 2])</span><br></pre></td></tr></table></figure>
<h2 id="4-多输入多输出通道"><a href="#4-多输入多输出通道" class="headerlink" title="4. 多输入多输出通道"></a>4. 多输入多输出通道</h2><p><strong>多输入</strong>通道，图像 X 和卷积核都是 3D</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(conv2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(conv2d_multi_in(X, K))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 56.,  72.],</span><br><span class="line">        [104., 120.]])</span><br></pre></td></tr></table></figure>
<p>多输入 + <strong>多输出</strong>通道，卷积核是 4D</p>
<p><strong>torch.stack</strong> 会将列表沿指定的<strong>新维度</strong>进行堆叠</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多输入 + 多输出 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.stack([conv2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(K.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(conv2d_multi_in_out(X, K))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 2, 2, 2])</span><br><span class="line">tensor([[[ 56.,  72.],</span><br><span class="line">         [104., 120.]],</span><br><span class="line"></span><br><span class="line">        [[ 76., 100.],</span><br><span class="line">         [148., 172.]],</span><br><span class="line"></span><br><span class="line">        [[ 96., 128.],</span><br><span class="line">         [192., 224.]]])</span><br></pre></td></tr></table></figure>
<p><strong>$1 \times 1$ 的卷积核</strong>常用于调整通道数、控制模型复杂性</p>
<p>$1 \times 1$ 的卷积核的计算可以使用类似全连接层的方式实现</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 * 1 卷积</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = conv2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = conv2d_multi_in_out(X, K)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure>
<h2 id="5-汇聚层"><a href="#5-汇聚层" class="headerlink" title="5. 汇聚层"></a>5. 汇聚层</h2><p>其实就是池化层啦</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    h, w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + h, j: j + w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + h, j: j + w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line"><span class="comment"># 最大池化</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment"># 平均池化</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), mode=<span class="string">&#x27;avg&#x27;</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[4., 5.],</span><br><span class="line">        [7., 8.]])</span><br><span class="line">tensor([[2., 3.],</span><br><span class="line">        [5., 6.]])</span><br></pre></td></tr></table></figure>
<p>同卷积层，池化层也可指定填充、步幅</p>
<p>输入输出维度及填充步幅之间的关系式同卷积层</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充和步幅</span></span><br><span class="line">X = torch.arange(<span class="number">9.</span>).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># ((3, 3) + 2 * (0, 1) - (2, 3)) / (2, 3) + （1, 1) = (1, 1)</span></span><br><span class="line"><span class="built_in">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[4.]]]])</span><br></pre></td></tr></table></figure>
<p>池化层使用多个通道的输入时，只会分别对每个通道做池化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个通道</span></span><br><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"><span class="built_in">print</span>(pool2d(X).shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 2, 4, 4])</span><br><span class="line">torch.Size([1, 2, 2, 2])</span><br></pre></td></tr></table></figure>
<h2 id="6-LeNet"><a href="#6-LeNet" class="headerlink" title="6. LeNet"></a>6. LeNet</h2><p>模型定义</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pool = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.sigmoid(self.conv1(X))</span><br><span class="line">        X = self.pool(X)</span><br><span class="line">        X = self.sigmoid(self.conv2(X))</span><br><span class="line">        X = self.pool(X)</span><br><span class="line">        X = self.flatten(X)</span><br><span class="line">        X = self.sigmoid(self.fc1(X))</span><br><span class="line">        X = self.sigmoid(self.fc2(X))</span><br><span class="line">        <span class="keyword">return</span> self.fc3(X)</span><br><span class="line"></span><br><span class="line">net = LeNet()</span><br></pre></td></tr></table></figure>
<p>评估函数将测试集移动到 gpu</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device    <span class="comment"># 未指定设备时，自动寻找设备</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment"># 将数据复制到网络所在的设备</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>在 gpu 中进行训练，并使用 xavier 初始化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    <span class="comment"># 定义损失函数和优化算法</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, acc <span class="subst">&#123;acc:f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(net.state_dict(), <span class="string">&#x27;lenet_state_dict&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, <span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="六、现代卷积神经网络"><a href="#六、现代卷积神经网络" class="headerlink" title="六、现代卷积神经网络"></a>六、现代卷积神经网络</h1><h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Conv2d output shape:	 torch.Size([1, 96, 54, 54])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])</span><br><span class="line">Conv2d output shape:	 torch.Size([1, 256, 26, 26])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])</span><br><span class="line">Conv2d output shape:	 torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d output shape:	 torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d output shape:	 torch.Size([1, 256, 12, 12])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 256, 12, 12])</span><br><span class="line">MaxPool2d output shape:	 torch.Size([1, 256, 5, 5])</span><br><span class="line">Flatten output shape:	 torch.Size([1, 6400])</span><br><span class="line">Linear output shape:	 torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:	 torch.Size([1, 4096])</span><br><span class="line">Linear output shape:	 torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:	 torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:	 torch.Size([1, 4096])</span><br><span class="line">Linear output shape:	 torch.Size([1, 10])</span><br></pre></td></tr></table></figure>
<h2 id="2-VGG"><a href="#2-VGG" class="headerlink" title="2. VGG"></a>2. VGG</h2><p>VGG 引入了使用块去构建网络</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积和池化层组成的块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每一块的 卷积层数/输出通道数</span></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建 VGG 网络</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<h2 id="3-NiN"><a href="#3-NiN" class="headerlink" title="3. NiN"></a>3. NiN</h2><p>NiN 将通道维度视作不同特征，NiN 块在每个卷积层后使用两个$1 \times 1$卷积核，作为在每个像素上独立作用的全连接层</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NiN 块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NiN 网络</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>
<h2 id="4-GoogLeNet"><a href="#4-GoogLeNet" class="headerlink" title="4. GoogLeNet"></a>4. GoogLeNet</h2><p>GoogLeNet 的 Inception 将一个输入经过多个不同的卷积层得到多个输出，再通过通道堆叠</p>
<p>定义 Inception</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7x7卷积 + 3x3池化</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 1x1卷积 + 3x3卷积 + 3x3池化</span></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 两个Inception + 3x3池化</span></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 五个Inception + 3x3池化</span></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 两个Inception + 全局平均池化</span></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"><span class="comment"># 增加全连接层</span></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h2 id="5-Batch-Normalization"><a href="#5-Batch-Normalization" class="headerlink" title="5. Batch Normalization"></a>5. Batch Normalization</h2><p>BatchNorm 层计算每个小批量数据集的均值和方差，并利用两个参数 $\gamma$ 和 $\beta$ 对其做缩放和平移</p>
<script type="math/tex; mode=display">BN(x) = \gamma \odot \frac{x - \hat\mu_B}{\hat\sigma_B^2} + \beta</script><p>其中，$x \in B$ 表示 $x$ 来自一个小批量的输入 $B$</p>
<p>BatchNorm 层在<strong>训练</strong>过程中利用<strong>指数移动平均</strong>的方式去近似整个数据集的均值和方差，并在<strong>推理</strong>过程中利用累计的结果直接作为数据集的均值和方差</p>
<p>移动平均可参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/151786842">移动平均(Moving Average) - 知乎 (zhihu.com)</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="comment"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span></span><br><span class="line">    <span class="comment"># num_dims：2表示完全连接层，4表示卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var</span></span><br><span class="line">        <span class="comment"># 复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h2 id="6-ResNet"><a href="#6-ResNet" class="headerlink" title="6. ResNet"></a>6. ResNet</h2><p>残差块使得每一块更容易包含上一块的输出</p>
<p>设上一块的输出为 $x$，当前块期望的输出为 $f(x)$，假设 $f(x) = x$（可以是任意函数）</p>
<p>那么当前块需要拟合一个 $f(x) = x$ 就比拟合一个 $h(x) = f(x) - x$ 要困难</p>
<p>因为 $h(x) = f(x) - x = 0$，只需要让参数都为 0</p>
<p>实际上，如果希望这一块的输出能够更容易包含上一块的输出，那么只让这一块去拟合一个残差函数 $h(x)$，再与原始输入 $x$ 相加会更高效</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure>
<h2 id="7-DenseNet"><a href="#7-DenseNet" class="headerlink" title="7. DenseNet"></a>7. DenseNet</h2><p>DenseNet 定义了一种<strong>稠密块</strong>，包含多个卷积块，每个卷积块的输入是前面所有卷积块的输出的堆叠</p>
<p>DenseNet 的卷积块中包含 Batch Normalization 操作</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>输入通道为3，带 2 个输出通道为 10 的卷积块的稠密块的输出通道为 3 + 2 * 10 = 23。卷积块的输出通道又叫增长率</p>
<p>由于稠密块会带来通道数的增加，所以需要<strong>过渡层</strong>利用 $1 \times 1$ 的卷积核降低模型的复杂度</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>作者: </span><span class="post-copyright-info"><a href="https://andreww1219.github.io">Andrew Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>链接: </span><span class="post-copyright-info"><a href="https://andreww1219.github.io/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://andreww1219.github.io/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></span></div><!--.post-copyright__notice--><!--  span.post-copyright-meta--><!--    i.fas.fa-circle-exclamation.fa-fw--><!--    = _p('post.copyright.copyright_notice') + ": "span.post-copyright-info!= info--></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><!--include includes/third-party/share/index.pug--></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/03/02/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E5%A4%A7%E7%99%BD%E8%AF%9D%E7%AE%80%E7%AD%94%EF%BC%8C%E5%85%B3%E4%BA%8E%E8%AE%A4%E8%AF%86Redis%E5%8F%8A%E5%85%B6%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%81%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/" title="Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化"><img class="cover" src="/img/loading.png" data-original="/img/2025/03/%E7%99%BD%E5%AD%901.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/18/%E8%85%BE%E8%AE%AF%20IEG%20%E6%B8%B8%E6%88%8F%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%20%E4%BA%8C%E9%9D%A2%E5%A4%8D%E7%9B%98/" title="腾讯 IEG 游戏前沿技术 二面复盘"><img class="cover" src="/img/loading.png" data-original="/img/2024/09/azusa3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">腾讯 IEG 游戏前沿技术 二面复盘</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/14/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%8A%E6%96%AD%E6%B3%95/" title="一文讲清楚机器学习中的正则化、神经网络、机器学习诊断法"><img class="cover" src="/img/loading.png" data-original="/img/2024/01/toki2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-14</div><div class="title">一文讲清楚机器学习中的正则化、神经网络、机器学习诊断法</div></div></a></div><div><a href="/2024/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/" title="深度学习笔记——卷积神经网络、自编码器"><img class="cover" src="/img/loading.png" data-original="/img/2024/01/azusa1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-22</div><div class="title">深度学习笔记——卷积神经网络、自编码器</div></div></a></div><div><a href="/2024/01/20/%E6%B7%B1%E5%85%A5%E5%88%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%BD%9C%E4%B8%9A%E6%A8%A1%E6%9D%BF%E4%BB%A3%E7%A0%81%E2%80%94%E2%80%94%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="深入分解机器学习实战作业模板代码——二分类、卷积神经网络"><img class="cover" src="/img/loading.png" data-original="/img/2024/01/arona1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">深入分解机器学习实战作业模板代码——二分类、卷积神经网络</div></div></a></div><div><a href="/2024/05/24/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Robust%20Recovery%20of%20Subspace%20Structures%20by%20Low-Rank%20Representation/" title="【机器学习论文阅读笔记】Robust Recovery of Subspace Structures by Low-Rank Representation"><img class="cover" src="/img/loading.png" data-original="/img/2024/05/%E7%99%BE%E9%AC%BC%E5%A4%9C%E8%A1%8C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-24</div><div class="title">【机器学习论文阅读笔记】Robust Recovery of Subspace Structures by Low-Rank Representation</div></div></a></div><div><a href="/2024/01/11/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(Linear%20Regression)%E3%80%81%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic%20Regression%EF%BC%89/" title="一文讲清楚线性回归(Linear Regression)、逻辑回归（Logistic Regression）"><img class="cover" src="/img/loading.png" data-original="/img/2024/01/FOX1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-11</div><div class="title">一文讲清楚线性回归(Linear Regression)、逻辑回归（Logistic Regression）</div></div></a></div><div><a href="/2024/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94LDA%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" title="机器学习笔记——LDA线性判别分析"><img class="cover" src="/img/loading.png" data-original="/img/2024/03/%E5%85%94%E5%AD%90%E5%B0%8F%E9%98%9F5.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-25</div><div class="title">机器学习笔记——LDA线性判别分析</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><a href="/"><img src="/img/loading.png" data-original="/img/%E5%A4%B4%E5%83%8F.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="author-info__name">Andrew Wang</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">50</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">一、预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">1. 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%85%A5%E9%97%A8"><span class="toc-text">1.1 入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">1.2 运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-text">1.3 广播机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-text">1.4 索引和切片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="toc-text">1.5 节省内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%85%B6%E5%AE%83-Python-%E5%AF%B9%E8%B1%A1"><span class="toc-text">1.6 转换为其它 Python 对象</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2. 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.1 读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-text">2.2 处理缺失值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F"><span class="toc-text">2.3 转换为张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">3. 线性代数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">4. 微积分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-text">5. 自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-text">5.1 基本用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E9%9D%9E%E6%A0%87%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">5.2 非标量的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-text">5.3 分离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Python-%E6%8E%A7%E5%88%B6%E6%B5%81%E5%BE%97%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">5.4 Python 控制流得梯度计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%A6%82%E7%8E%87"><span class="toc-text">6. 概率</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">二、线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">1. 线性回归从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">1.1 生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">1.2 读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">1.3 模型定义和训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2. 线性回归简洁实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-softmax"><span class="toc-text">3. softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-softmax-%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">4. softmax 回归从零开始实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-softmax-%E5%9B%9E%E5%BD%92%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">5. softmax 回归简洁实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">三、多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">1. 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%BC%95%E5%85%A5"><span class="toc-text">1.1 多层感知机引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">1.2 常用激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-text">2. 多层感知机从零实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3. 多层感知机简洁实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">4. 正则化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%B7%BB%E5%8A%A0%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9"><span class="toc-text">4.1 添加正则化项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Dropout"><span class="toc-text">4.2 Dropout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">5. 模型初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-text">6. 环境和分布偏移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-text">6.1 分布偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E5%92%8C%E5%AE%9E%E9%99%85%E9%A3%8E%E9%99%A9"><span class="toc-text">6.2 经验风险和实际风险</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E7%BA%A0%E6%AD%A3%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-text">6.3 纠正协变量偏移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E7%BA%A0%E6%AD%A3%E6%A0%87%E7%AD%BE%E5%81%8F%E7%A7%BB"><span class="toc-text">6.4 纠正标签偏移</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="toc-text">四、深度学习计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%B1%82%E5%92%8C%E5%9D%97"><span class="toc-text">1. 层和块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0%E6%93%8D%E4%BD%9C"><span class="toc-text">2. 参数操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="toc-text">2.1 参数访问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">2.2 参数初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-text">2.3 参数共享</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%BB%B6%E5%90%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">2.4 延后初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-text">3. 自定义层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-text">4. 保存模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-GPU"><span class="toc-text">5. GPU</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">五、卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%88%B0%E5%8D%B7%E7%A7%AF"><span class="toc-text">1. 从全连接层到卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="toc-text">2. 图像卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-text">3. 填充和步幅</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-text">4. 多输入多输出通道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-text">5. 汇聚层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-LeNet"><span class="toc-text">6. LeNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">六、现代卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-AlexNet"><span class="toc-text">1. AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-VGG"><span class="toc-text">2. VGG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-NiN"><span class="toc-text">3. NiN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-GoogLeNet"><span class="toc-text">4. GoogLeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Batch-Normalization"><span class="toc-text">5. Batch Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-ResNet"><span class="toc-text">6. ResNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-DenseNet"><span class="toc-text">7. DenseNet</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/07/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%8F%8A%E9%80%89%E5%9E%8B%EF%BC%8C%E5%B8%B8%E8%A7%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E5%8C%85%E6%8B%AC%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7%E3%80%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F%E3%80%81%E6%B6%88%E6%81%AF%E5%A0%86%E7%A7%AF%E3%80%81%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E3%80%81%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/" title="消息队列基础概念及选型，常见解决方案包括消息可靠性、消息有序、消息堆积、重复消费、事务消息"><img src="/img/loading.png" data-original="/img/2025/04/%E5%8D%83%E7%BA%B8%E9%B9%A4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="消息队列基础概念及选型，常见解决方案包括消息可靠性、消息有序、消息堆积、重复消费、事务消息"/></a><div class="content"><a class="title" href="/2025/04/07/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%8F%8A%E9%80%89%E5%9E%8B%EF%BC%8C%E5%B8%B8%E8%A7%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E5%8C%85%E6%8B%AC%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7%E3%80%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F%E3%80%81%E6%B6%88%E6%81%AF%E5%A0%86%E7%A7%AF%E3%80%81%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E3%80%81%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/" title="消息队列基础概念及选型，常见解决方案包括消息可靠性、消息有序、消息堆积、重复消费、事务消息">消息队列基础概念及选型，常见解决方案包括消息可靠性、消息有序、消息堆积、重复消费、事务消息</a><time datetime="2025-04-06T16:10:00.000Z" title="发表于 2025-04-07 00:10:00">2025-04-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/%E6%9A%91%E6%9C%9F%E7%AC%AC%E4%B8%80%E9%9D%A2o%CF%89o,%20TME%E4%B8%80%E9%9D%A2%E9%9D%A2%E7%BB%8F/" title="暑期第一面oωo, TME一面面经"><img src="/img/loading.png" data-original="/img/2025/03/%E7%99%BE%E9%AC%BC%E5%A4%9C%E8%A1%8C2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="暑期第一面oωo, TME一面面经"/></a><div class="content"><a class="title" href="/2025/03/11/%E6%9A%91%E6%9C%9F%E7%AC%AC%E4%B8%80%E9%9D%A2o%CF%89o,%20TME%E4%B8%80%E9%9D%A2%E9%9D%A2%E7%BB%8F/" title="暑期第一面oωo, TME一面面经">暑期第一面oωo, TME一面面经</a><time datetime="2025-03-11T15:25:00.000Z" title="发表于 2025-03-11 23:25:00">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/02/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E5%A4%A7%E7%99%BD%E8%AF%9D%E7%AE%80%E7%AD%94%EF%BC%8C%E5%85%B3%E4%BA%8E%E8%AE%A4%E8%AF%86Redis%E5%8F%8A%E5%85%B6%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%81%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/" title="Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化"><img src="/img/loading.png" data-original="/img/2025/03/%E7%99%BD%E5%AD%901.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化"/></a><div class="content"><a class="title" href="/2025/03/02/Redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E5%A4%A7%E7%99%BD%E8%AF%9D%E7%AE%80%E7%AD%94%EF%BC%8C%E5%85%B3%E4%BA%8E%E8%AE%A4%E8%AF%86Redis%E5%8F%8A%E5%85%B6%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%81%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/" title="Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化">Redis常见面试题大白话简答，关于认识Redis及其数据结构、线程模型和持久化</a><time datetime="2025-03-02T07:00:00.000Z" title="发表于 2025-03-02 15:00:00">2025-03-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络"><img src="/img/loading.png" data-original="/img/2024/10/%E5%AF%B9%E7%AD%96%E5%A7%94%E5%91%98%E4%BC%9A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络"/></a><div class="content"><a class="title" href="/2024/10/07/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8BPytorch%20%E7%89%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9A%E4%BB%8E%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%88%B0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络">《动手学深度学习》Pytorch 版学习笔记一：从预备知识到现代卷积神经网络</a><time datetime="2024-10-07T13:15:00.000Z" title="发表于 2024-10-07 21:15:00">2024-10-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/18/%E8%85%BE%E8%AE%AF%20IEG%20%E6%B8%B8%E6%88%8F%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%20%E4%BA%8C%E9%9D%A2%E5%A4%8D%E7%9B%98/" title="腾讯 IEG 游戏前沿技术 二面复盘"><img src="/img/loading.png" data-original="/img/2024/09/azusa3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="腾讯 IEG 游戏前沿技术 二面复盘"/></a><div class="content"><a class="title" href="/2024/09/18/%E8%85%BE%E8%AE%AF%20IEG%20%E6%B8%B8%E6%88%8F%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%20%E4%BA%8C%E9%9D%A2%E5%A4%8D%E7%9B%98/" title="腾讯 IEG 游戏前沿技术 二面复盘">腾讯 IEG 游戏前沿技术 二面复盘</a><time datetime="2024-09-18T14:15:00.000Z" title="发表于 2024-09-18 22:15:00">2024-09-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Andrew Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>