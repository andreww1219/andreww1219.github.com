<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习笔记——卷积神经网络、自编码器</title>
      <link href="/2024/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
      <url>/2024/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>发现虽然玩了一次卷积神经网络，但还没有写文章分析下它在干什么。</p><p>and今天瞅了眼自编码器和深度生成模型，生成宝可梦感觉真好玩啊（x</p><p>视频链接：<a href="https://study.163.com/course/courseMain.htm?courseId=1208946807">李宏毅机器学习中文课程 - 网易云课堂 (163.com)</a></p><h1 id="一、卷积神经网络"><a href="#一、卷积神经网络" class="headerlink" title="一、卷积神经网络"></a>一、卷积神经网络</h1><h2 id="1-应用场景"><a href="#1-应用场景" class="headerlink" title="1. 应用场景"></a>1. 应用场景</h2><ol><li>在图片处理中，我们想要捕捉的特征远比整张图片的尺寸要小，无需对所有像素作线性组合</li><li>我们想要捕捉的特征可以在图片的不同位置出现，因此一些hidden layer的参数会相等导致冗余</li><li>降低图片的分辨率几乎不会影响判断结果，可以降低数据维度加快计算</li></ol><p>我们将以上三点更加抽象地描述为：</p><p><strong>1. 特定信息只存在于完整数据中的一部分</strong><br><strong>2. 特定信息在完整数据中多次出现</strong><br><strong>3. 降采样对结果不会有影响</strong></p><h2 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h2><ol><li>在<strong>卷积层</strong>：<br>利用卷积核抽取我们需要的特定信息，卷积核只作用于输入数据中的一定区域（解决了<strong>问题1</strong>），并以一定步长在输入数据上移动（解决了<strong>问题2</strong>）。</li></ol><ol><li>在<strong>池化层</strong>：<br>用某个数值代表特定区域的数值，成倍地降低数据地空间大小（解决了<strong>问题3</strong>）</li></ol><h2 id="3-更多示例"><a href="#3-更多示例" class="headerlink" title="3. 更多示例"></a>3. 更多示例</h2><p>当一个问题满足上述的三个条件时，就可以利用卷积神经网络来train一个合适的模型。</p><p>实际上，卷积和池化不一定要同时出现。</p><p>比如，在下五子棋时，判断当前棋局的问题<strong>满足问题1和问题2</strong>，但是<strong>不满足问题3</strong></p><p>由2可知，卷积层解决的是问题1和问题2，池化层解决的是问题3。</p><p>因此，我们只需要使用卷积来处理五子棋的问题。</p><h1 id="二、自编码器"><a href="#二、自编码器" class="headerlink" title="二、自编码器"></a>二、自编码器</h1><h2 id="1-何为自编码器"><a href="#1-何为自编码器" class="headerlink" title="1. 何为自编码器"></a>1. 何为自编码器</h2><p>在使用PCA时，我们得到一个矩阵$M$将原始的高维数据$x$转换为低维数据$z$，同时可以利用$M^T$对低维数据$z$作逆变换得到$\widehat{x}$。但是在高维向低维投影的过程中出现了损失，所以逆变换得到的$\widehat{x}$与原来的$x$存在<strong>不小的偏差</strong></p><p>PCA原理参考：<a href="https://juejin.cn/post/7324200561917378579#heading-12">机器学习笔记——机器学习系统设计、支持向量机(SVM)、K-Means算法、主成分分析法(PCA) - 掘金 (juejin.cn)</a></p><p>在神经网络中，一个hidden layer的参数就是一个矩阵，所以我们可以将上述过程描述为:</p><script type="math/tex; mode=display">Input Layer(x) \rightarrow Hidden Layer(参数为M, 得到z) \rightarrow Output Layer(参数为 M^T 得到,  \widehat{x} )</script><p>我们将前面的过程称作编码(Encode)，后面的过程称作(解码)，中间输出的编码后的低维数据称作code</p><p>我们现在希望编码和解码的过程，能够<strong>尽可能地减小损失，最大程度地还原数据</strong></p><p>不妨分别将编码和解码过程抽象为神经网络，然后在训练的过程最小化$x$ 和 $ \widehat{x} $之间的偏差：</p><script type="math/tex; mode=display">x  \rightarrow NN\space Encoder \rightarrow code \rightarrow NN\space Decoder \rightarrow \widehat{x}</script><p>于是我们就利用了没有任何标签的数据，训练了一个能自动解码和编码的神经网络，而且解码和编码模块都能单独拎出来用，这就是自编码器</p><h2 id="2-自编码器的神奇之处"><a href="#2-自编码器的神奇之处" class="headerlink" title="2. 自编码器的神奇之处"></a>2. 自编码器的神奇之处</h2><h3 id="2-1-CNN与自编码器"><a href="#2-1-CNN与自编码器" class="headerlink" title="2.1 CNN与自编码器"></a>2.1 CNN与自编码器</h3><p>在卷积神经网络中，我们可以将卷积和池化操作也看作是编码过程，本质是提取低维的特定信息。</p><p>对应地，将解码过程称作反卷积和反池化：</p><ol><li>反卷积的操作与卷积一致</li><li>反池化将数据扩大时，多出来的空间可以置零，或取相同的值</li></ol><h3 id="2-2-预训练-Pre-training"><a href="#2-2-预训练-Pre-training" class="headerlink" title="2.2 预训练(Pre-training)"></a>2.2 预训练(Pre-training)</h3><p>在神经网络层次比较深时，需要拟合大量的参数。</p><p>我们可以将每个Hidden Layer（注意：不包括输出层）视作一个编码器，先利用<strong>大量的无标签数据</strong>依次对每个Hidden Layer的参数作初始化，这个过程叫预训练(Pre-training)。</p><p>最后在随机初始化输出层的参数，再利用<strong>少量标签数据</strong>训练模型。在这里，大部分的参数已经被预训练确定了七七八八，所以训练阶段只是在做微调(fine-tuning)</p><h3 id="2-3-生成新数据"><a href="#2-3-生成新数据" class="headerlink" title="2.3 生成新数据"></a>2.3 生成新数据</h3><p>特别有趣的是，我们可以自己设定一些code，丢进解码器中，这样就得到许多新数据。</p><p>将真的原始数据生成的code作Normalization，然后划定真code比较密集的一个范围作为我们的假code，可以使我们选择的code更有意义。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> CNN </tag>
            
            <tag> 自编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记——概率生成模型</title>
      <link href="/2024/01/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/01/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>吴恩达的视频里没提到概率生成模型，在李宏毅的作业2看到了，感觉挺有必要理解的，可以很自然而然地引出逻辑回归的模型。</p><p>参考：<a href="https://study.163.com/course/courseLearn.htm?courseId=1208946807">李宏毅机器学习中文课程 - 网易云课堂：分类：概率生成模型</a></p><h1 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h1><p>已知m组数据$x^{(1)}, x^{(2)}, …, x^{(m)}$，每组数据表示n个特征,可写为一个n维的向量<br>即：<script type="math/tex">(x^{(i)})^T = ( x^{(i)}_1, ..., x^{(i)}_n)</script><br>且每组数据的真实值$y^{(i)}$只能为0或1</p><p>给定一个数据$x$，预测$x$对应的$y$值。</p><h1 id="二、解决方案"><a href="#二、解决方案" class="headerlink" title="二、解决方案"></a>二、解决方案</h1><h2 id="1-贝叶斯公式求概率表达式"><a href="#1-贝叶斯公式求概率表达式" class="headerlink" title="1. 贝叶斯公式求概率表达式"></a>1. 贝叶斯公式求概率表达式</h2><p>由题可知，我们根据真实值将$m$组数据分为两类$C_0$和$C_1$，其中$C_0$代表真实值为$0$的数据的集合，一共有$m_0$组，$C_1$代表真实值为$1$的数据的集合，一共有$m_1$组</p><p>那么给定数据$x$，由贝叶斯公式，$x$出现在$C_0$的概率为</p><script type="math/tex; mode=display">P(C_0 | x) = \frac{P(x | C_0) P(C_0)} { P(x | C_0) P(C_0) + P(x | C_1) P(C_1) }</script><script type="math/tex; mode=display">P(C_0 | x) = \frac{1} { 1 + \frac{P(x | C_1) P(C_1)}{P(x | C_0) P(C_0)} }</script><p>其中，$P(x | C_0)$代表给定集合$C_0$，出现数据为$x$的概率，即</p><script type="math/tex; mode=display">P(x | C_0) = f_{\mu^0, \Sigma^0}(x)</script><p>其中$ \mu^0 $ 和 $ \Sigma^0 $为$ C_0 $的均值和协方差矩阵，由$C_0$确定。</p><p>同理，$ P(x | C_1) = f_{\mu^1, \Sigma^1}(x) $</p><p>而$P(C_0)$代表随机取一组数据在$C_0$的概率，即$ P(C_0) = \frac{m_0}{m_0 + m_1} $</p><p>同理，$ P(C_1) = \frac{m_1}{m_0 + m_1} $</p><h2 id="2-由数据集求概率密度函数"><a href="#2-由数据集求概率密度函数" class="headerlink" title="2. 由数据集求概率密度函数"></a>2. 由数据集求概率密度函数</h2><p>当数据足够大时，由中心极限定理，$C_0$服从正态分布</p><script type="math/tex; mode=display">f_{\mu^0, \Sigma^0}(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma^0|^{ \frac{1}{2}}} exp(-\frac{1}{2} (x - \mu^0)^T (\Sigma^0)^{-1} (x - \mu^0))</script><p>由已有数据求$ \mu^0 $和 $\Sigma^0$的最大似然估计为</p><script type="math/tex; mode=display">\mu^0 = \frac{1}{m_0} \sum_{i: y^{(i)} = 0} x^{(i)}</script><script type="math/tex; mode=display">\Sigma^0 = \frac{1}{m_0} \sum_{i: y^{(i)} = 0} (x^{(i)} - \mu^0) (x^{(i)} - \mu^0)^T</script><p>同理，由$C_1$确定的概率分布函数、均值、协方差矩阵为</p><script type="math/tex; mode=display">f_{\mu^1, \Sigma^1}(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma^1|^{ \frac{1}{2}}} exp(-\frac{1}{2} (x - \mu^1)^T (\Sigma^1)^{-1} (x - \mu^1))</script><script type="math/tex; mode=display">\mu^1 = \frac{1}{m_1} \sum_{i: y^{(i)} = 1} x^{(i)}</script><script type="math/tex; mode=display">\Sigma^1 = \frac{1}{m_1} \sum_{i: y^{(i)} = 1} (x^{(i)} - \mu^1) (x^{(i)} - \mu^1)^T</script><h2 id="3-公式整理"><a href="#3-公式整理" class="headerlink" title="3. 公式整理"></a>3. 公式整理</h2><p>对1中表达式做变形，令</p><script type="math/tex; mode=display">\frac{P(x | C_1) P(C_1)}{P(x | C_0) P(C_0)}  = e^{-z}</script><p>得</p><script type="math/tex; mode=display">P(C_0 | x) =  \frac{1} { 1 + e^{-z} } = g(z)</script><script type="math/tex; mode=display">z = -\ln {\frac{P(x | C_1) P(C_1)}{P(x | C_0) P(C_0)} } = \ln {\frac{P(x | C_0) }{ P(x | C_1)} } + \ln { \frac{ P(C_0) }{ P(C_1) } }</script><p>将2中概率分布函数代入到表达式中，得</p><script type="math/tex; mode=display">z = \ln{\frac{ |\Sigma|^1}{ | \Sigma|^0 } } - \frac{1}{2} (x - \mu^0)^T (\Sigma^0)^{-1} (x - \mu^0) + \frac{1}{2} (x - \mu^1)^T (\Sigma^1)^{-1} (x - \mu^1)) + \ln{ \frac{m_0}{m_1}}</script><p>若将$\Sigma^0$和$\Sigma^1$等同为一个值$\Sigma$（未考究），可继续化简得：</p><script type="math/tex; mode=display">z = (\mu^0 - \mu^1)^T \Sigma^{-1} x - \frac{1}{2} (\mu^0)^T \Sigma^{-1} \mu^0 + \frac{1}{2} (\mu^1)^T \Sigma^{-1} \mu^1 + \ln{ \frac{m_0}{m_1}}</script><p>令$ z = \omega x + b$，则有</p><script type="math/tex; mode=display">\omega = (\mu^0 - \mu^1)^T \Sigma^{-1}</script><script type="math/tex; mode=display">b = - \frac{1}{2} (\mu^0)^T \Sigma^{-1} \mu^0 + \frac{1}{2} (\mu^1)^T \Sigma^{-1} \mu^1 + \ln{ \frac{m_0}{m_1}}</script><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p>既然预测$x$在一个分类中的概率$P(C_0 | x)$，能被关于$x$与某一向量的线性组合的函数，即$\omega x+ b$，$g(\omega x + b) = \frac{1}{1+ e^{\omega x+ b}}$，我们不妨假设向量</p><script type="math/tex; mode=display">\theta^T x = \omega x + b</script><p>这就很好的解释了为什么要引入$g(\theta^T x)$作为逻辑回归的假设。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入分解机器学习实战作业模板代码——二分类、卷积神经网络</title>
      <link href="/2024/01/20/%E6%B7%B1%E5%85%A5%E5%88%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%BD%9C%E4%B8%9A%E6%A8%A1%E6%9D%BF%E4%BB%A3%E7%A0%81%E2%80%94%E2%80%94%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/01/20/%E6%B7%B1%E5%85%A5%E5%88%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%BD%9C%E4%B8%9A%E6%A8%A1%E6%9D%BF%E4%BB%A3%E7%A0%81%E2%80%94%E2%80%94%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>刷完理论课去找实战打，找了李宏毅的课程实战，教的是Keras，代码能一行行分析明白，但是是真的一点都不会写。于是去b站找视频补了点实战知识。</p><p>本篇文章将分解作业2和作业3提供的模板代码，提炼出模型训练时的常用操作，包括csv文件的输入输出，numpy的数组操作，keras训练模型的步骤等。</p><p>实战来自李宏毅的机器学习课程：<a href="https://study.163.com/course/courseMain.htm?courseId=1208946807">李宏毅机器学习中文课程 - 网易云课堂 (163.com)</a></p><p>Keras实战视频链接：<a href="https://www.bilibili.com/video/BV15K411k7nD">深度学习框架【Keras项目实战】</a></p><p>作业Kaggle链接：</p><ol><li>收入预测：<a href="https://www.kaggle.com/competitions/ml2019spring-hw2">ML2019SPRING-hw2 | Kaggle</a></li><li>图片情绪分类：<a href="https://www.kaggle.com/competitions/ml2019spring-hw3">ML2019SPRING-hw3 | Kaggle</a></li></ol><h1 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h1><h2 id="1-收入预测"><a href="#1-收入预测" class="headerlink" title="1. 收入预测"></a>1. 收入预测</h2><p>根据给定的个人资讯，预测此人的收入能否大于50K。</p><p>数据集X_train包含许多个人信息，Y_train对应他们年收入是否大于50K。训练一个二分类模型，对X_test作预测。</p><p>模板代码：</p><p>概率生成模型(Probabilistic Generative Model)： <a href="https://ntumlta2019.github.io/ml-web-hw2/ProbabilisticGenerativeModel.html">ProbabilisticGenerativeModel (ntumlta2019.github.io)</a></p><p>逻辑回归(Logistic Regression)：<a href="https://ntumlta2019.github.io/ml-web-hw2/LogisticRegression.html">LogisticRegression (ntumlta2019.github.io)</a></p><h2 id="2-图片情绪分类"><a href="#2-图片情绪分类" class="headerlink" title="2. 图片情绪分类"></a>2. 图片情绪分类</h2><p>给定48$*$48像素的图片，判断该图片所表达的情绪，包括0：生气, 1：厌恶, 2：恐惧, 3：高兴, 4：难过, 5：惊讶, 6：中立）</p><p>训练集x_train.csv每一行有两列，第一列label为图片的情绪，第二列为48$*$48个像素值，范围从0~255。训练一个卷积神经网络，对x_test.csv作预测</p><p>模板代码：<a href="https://hackmd.io/hlJAABuoRJa0tzpD8UUHvw">2019 Spring ML HW3 - 手把手教學 - HackMD</a></p><h1 id="二、处理输入输出"><a href="#二、处理输入输出" class="headerlink" title="二、处理输入输出"></a>二、处理输入输出</h1><h2 id="1-读取csv文件"><a href="#1-读取csv文件" class="headerlink" title="1. 读取csv文件"></a>1. 读取csv文件</h2><ul><li><p>利用np.genfromtxt() 需要添加参数delimeter=’,’ 返回的结果是<strong>列表</strong>而不是ndarray对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_train = np.genfromtxt(path, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=<span class="built_in">str</span>, skip_header=<span class="number">1</span>) <span class="comment"># skip_header=1</span></span><br></pre></td></tr></table></figure></li><li><p>引入python自带的csv包，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(path, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    raw_train = np.array(<span class="built_in">list</span>(csv.reader(csvfile))[<span class="number">1</span>:], dtype=<span class="built_in">float</span>) <span class="comment"># 取下标[1:]表示去掉表头</span></span><br></pre></td></tr></table></figure><h2 id="2-归一化-标准化-离散化"><a href="#2-归一化-标准化-离散化" class="headerlink" title="2. 归一化/标准化/离散化"></a>2. 归一化/标准化/离散化</h2><p>归一化：$x = \frac{x - x_{min}}{x_{max} - x_{min}}$</p></li></ul><p>标准化：$x = \frac{x - \mu}{\sigma}$</p><p>离散化：将数据标签1，2, …, n转化为[1, 0, …, 0], [0, 1, …, 0], …, [0, 0, …, 1]</p><h3 id="2-1-手动处理"><a href="#2-1-手动处理" class="headerlink" title="2.1 手动处理"></a>2.1 手动处理</h3><ol><li><p>归一化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.<span class="built_in">min</span> = np.<span class="built_in">min</span>(rows, axis=<span class="number">0</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">self.std = np.std(rows, axis=<span class="number">0</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">self.theta = np.ones((rows.shape[<span class="number">1</span>] + <span class="number">1</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(rows.shape[<span class="number">0</span>]):</span><br><span class="line">    rows[i, :] = (rows[i, :] - self.<span class="built_in">min</span>) / self.std</span><br></pre></td></tr></table></figure></li><li><p>标准化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.mean = np.mean(rows, axis=<span class="number">0</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">self.std = np.std(rows, axis=<span class="number">0</span>).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">self.theta = np.ones((rows.shape[<span class="number">1</span>] + <span class="number">1</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(rows.shape[<span class="number">0</span>]):</span><br><span class="line">    rows[i, :] = (rows[i, :] - self.mean) / self.std</span><br></pre></td></tr></table></figure></li></ol><p><strong>对于axis的理解</strong>：axis=i，操作沿着第i个下标变化的方向进行<br>参考：<a href="https://zhuanlan.zhihu.com/p/31275071">Numpy:对Axis的理解 - 知乎 (zhihu.com)</a></p><h3 id="2-2-利用sklearn"><a href="#2-2-利用sklearn" class="headerlink" title="2.2 利用sklearn"></a>2.2 利用sklearn</h3><ol><li>归一化：MinMaxScaler<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line">data = np.array([[-<span class="number">1</span>, -<span class="number">2</span>, -<span class="number">3</span>, -<span class="number">4</span>, -<span class="number">5</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">minMaxScaler = MinMaxScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在处理训练数据时使用fit_transform</span></span><br><span class="line">data = minMaxScaler.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出结果</span></span><br><span class="line"><span class="string">[[0.  0.  0.  0.  0. ]</span></span><br><span class="line"><span class="string"> [0.5 0.5 0.5 0.5 0.5]</span></span><br><span class="line"><span class="string"> [1.  1.  1.  1.  1. ]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_data = np.array([[-<span class="number">2</span>, -<span class="number">3</span>, -<span class="number">4</span>, -<span class="number">5</span>, -<span class="number">6</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment"># 在处理测试数据时使用transform</span></span><br><span class="line">test_data = minMaxScaler.transform(test_data)</span><br></pre></td></tr></table></figure></li><li><p>标准化：StandardScaler</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">data = np.array([[-<span class="number">1</span>, -<span class="number">2</span>, -<span class="number">3</span>, -<span class="number">4</span>, -<span class="number">5</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">stdScaler = StandardScaler()</span><br><span class="line"><span class="comment"># 在处理训练数据时使用fit_transform</span></span><br><span class="line">data = stdScaler.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出结果</span></span><br><span class="line"><span class="string">[[-1.22474487 -1.22474487 -1.22474487 -1.22474487 -1.22474487]</span></span><br><span class="line"><span class="string"> [ 0.          0.          0.          0.          0.        ]</span></span><br><span class="line"><span class="string"> [ 1.22474487  1.22474487  1.22474487  1.22474487  1.22474487]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在处理测试数据时使用transform</span></span><br><span class="line">test_data = np.array([[-<span class="number">2</span>, -<span class="number">3</span>, -<span class="number">4</span>, -<span class="number">5</span>, -<span class="number">6</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">test_data = stdScaler.transform(test_data)</span><br></pre></td></tr></table></figure></li><li><p>离散化：LabelBinarizer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"></span><br><span class="line">train_label = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">lb = LabelBinarizer()</span><br><span class="line"><span class="comment"># 在处理训练数据时使用fit_transform</span></span><br><span class="line">train_label = lb.fit_transform(train_label)</span><br><span class="line"><span class="built_in">print</span>(train_label)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出结果</span></span><br><span class="line"><span class="string">[[1 0 0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 1 0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 1 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 1 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0 1 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0 0 1]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_label = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 在处理测试数据时使用transform</span></span><br><span class="line">test_label = lb.transform(test_label)</span><br><span class="line"><span class="built_in">print</span>(test_label)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出结果</span></span><br><span class="line"><span class="string">[[0 1 0 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 1 0 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 1 0 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0 1 0]</span></span><br><span class="line"><span class="string"> [0 0 0 0 0 1]</span></span><br><span class="line"><span class="string"> [1 0 0 0 0 0]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="3-数据分割"><a href="#3-数据分割" class="headerlink" title="3. 数据分割"></a>3. 数据分割</h2><p>原始数据如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_train = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>)])</span><br><span class="line">y_train = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>)])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape[<span class="number">0</span>], y_train.shape[<span class="number">0</span>])</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">10000 10000</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p><h3 id="3-1-手动分割"><a href="#3-1-手动分割" class="headerlink" title="3.1 手动分割"></a>3.1 手动分割</h3><p>按照比例分割<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">segmentation</span>(<span class="params">x_train, y_train, proportion</span>):</span><br><span class="line">    train_data = []</span><br><span class="line">    train_label = []</span><br><span class="line">    val_data = []</span><br><span class="line">    val_label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x_train.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> i % proportion == <span class="number">0</span>:</span><br><span class="line">            val_data.append(x_train[i])</span><br><span class="line">            val_label.append(y_train[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data.append(x_train[i])</span><br><span class="line">            train_label.append(y_train[i])</span><br><span class="line">    train_data = np.array(train_data, dtype=<span class="built_in">float</span>)</span><br><span class="line">    train_label = np.array(train_label, dtype=<span class="built_in">float</span>)</span><br><span class="line">    val_data = np.array(val_data, dtype=<span class="built_in">float</span>)</span><br><span class="line">    val_label = np.array(val_label, dtype=<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">return</span> train_data, train_label, val_data, val_label</span><br><span class="line"></span><br><span class="line">train_data, train_label, val_data, val_label = segmentation(x_train, y_train, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_data), <span class="built_in">len</span>(train_label), <span class="built_in">len</span>(val_data), <span class="built_in">len</span>(val_label))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">9000 9000 1000 1000</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p><h3 id="3-2-利用sklearn"><a href="#3-2-利用sklearn" class="headerlink" title="3.2 利用sklearn"></a>3.2 利用sklearn</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SEED = <span class="number">12</span>   <span class="comment"># 指定随机数种子以便再现</span></span><br><span class="line">train_data, train_label, val_data, val_label = (</span><br><span class="line">    train_test_split(x_train, y_train, test_size=<span class="number">0.2</span>, random_state=SEED))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_data), <span class="built_in">len</span>(train_label), <span class="built_in">len</span>(val_data), <span class="built_in">len</span>(val_label))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">8000 2000 8000 2000</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h1 id="三、模型构建"><a href="#三、模型构建" class="headerlink" title="三、模型构建"></a>三、模型构建</h1><h2 id="1-概率生成模型"><a href="#1-概率生成模型" class="headerlink" title="1. 概率生成模型"></a>1. 概率生成模型</h2><p>相关文章：<a href="https://juejin.cn/post/7325652953540739083">机器学习笔记——概率生成模型 - 掘金 (juejin.cn)</a></p><p>1.概率生成模型要求先将数据集分割为两部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class_0_id = []</span><br><span class="line">class_1_id = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.data[<span class="string">&#x27;Y_train&#x27;</span>].shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> self.data[<span class="string">&#x27;Y_train&#x27;</span>][i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        class_0_id.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        class_1_id.append(i)</span><br><span class="line"></span><br><span class="line">class_0 = self.data[<span class="string">&#x27;X_train&#x27;</span>][class_0_id]</span><br><span class="line">class_1 = self.data[<span class="string">&#x27;X_train&#x27;</span>][class_1_id]</span><br></pre></td></tr></table></figure><br>这里用到了高级索引，参考：<a href="https://www.runoob.com/numpy/numpy-advanced-indexing.html">NumPy 高级索引 | 菜鸟教程 (runoob.com)</a></p><p>2.分别求两部分的均值和协方差矩阵，共享协方差矩阵是两者的加权平均和：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(class_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(class_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">n = class_0.shape[<span class="number">1</span>]</span><br><span class="line">cov_0 = np.zeros((n, n))</span><br><span class="line">cov_1 = np.zeros((n, n))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(class_0.shape[<span class="number">0</span>]):</span><br><span class="line">    cov_0 += np.dot(np.transpose([class_0[i] - mean_0]), [(class_0[i] - mean_0)]) / class_0.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(class_1.shape[<span class="number">0</span>]):</span><br><span class="line">    cov_1 += np.dot(np.transpose([class_1[i] - mean_1]), [(class_1[i] - mean_1)]) / class_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">cov = (cov_0 * class_0.shape[<span class="number">0</span>] + cov_1 * class_1.shape[<span class="number">0</span>]) / (class_0.shape[<span class="number">0</span>] + class_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>3.由概率生成模型的参数为：</p><script type="math/tex; mode=display">\omega = (\mu^0 - \mu^1)^T \Sigma^{-1}</script><script type="math/tex; mode=display">b = - \frac{1}{2} (\mu^0)^T \Sigma^{-1} \mu^0 + \frac{1}{2} (\mu^1)^T \Sigma^{-1} \mu^1 + \ln{ \frac{m_0}{m_1}}</script><p>得到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.w = np.transpose(((mean_0 - mean_1)).dot(inv(cov)))</span><br><span class="line">self.b = (- <span class="number">0.5</span>) * (mean_0).dot(inv(cov)).dot(mean_0) \</span><br><span class="line">         + <span class="number">0.5</span> * (mean_1).dot(inv(cov)).dot(mean_1) \</span><br><span class="line">         + np.log(<span class="built_in">float</span>(class_0.shape[<span class="number">0</span>]) / class_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2. 逻辑回归"></a>2. 逻辑回归</h2><p>1.打乱数据集：利用高级索引，将特征和标签同时打乱且仍然能够互相对应：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_shuffle</span>(<span class="params">X, Y</span>):</span><br><span class="line">    randomize = np.arange(X.shape[<span class="number">0</span>])</span><br><span class="line">    shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">              [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">              [<span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">X, Y = _shuffle(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure><br>输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[ 5  6]</span><br><span class="line"> [ 9 10]</span><br><span class="line"> [ 1  2]</span><br><span class="line"> [ 7  8]</span><br><span class="line"> [ 3  4]]</span><br><span class="line">[2 4 0 3 1]</span><br></pre></td></tr></table></figure></p><p>2.batch划分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(np.floor(<span class="built_in">len</span>(Y_train)/batch_size))):</span><br><span class="line">           X = X_train[idx*batch_size:(idx+<span class="number">1</span>)*batch_size]</span><br><span class="line">           Y = Y_train[idx*batch_size:(idx+<span class="number">1</span>)*batch_size]</span><br></pre></td></tr></table></figure></p><p>3.求梯度：根据梯度公式$\frac{\partial J(\theta) }{\partial \theta_j}= \frac{1}{m} [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j]$，得到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_gradient_regularization</span>(<span class="params">X, Y_label, w, b, lamda</span>):</span><br><span class="line">    <span class="comment"># return the mean of the graident</span></span><br><span class="line">    y_pred = get_prob(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = -np.mean(np.multiply(pred_error.T, X.T), <span class="number">1</span>)+lamda*w</span><br><span class="line">    b_grad = -np.mean(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, b_grad</span><br></pre></td></tr></table></figure><br>详解：梯度$w$和$b$合起来就是公式中的$\theta$，为一个$n + 1$维列向量<br>pred_error 和 X 都是$m \times n$维矩阵，两者作内积后需要对$m$所在维度求平均值，可以有以下两种实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先转置得到n*m维矩阵，再在第二个维度，即axis=1上求平均值</span></span><br><span class="line">w_grad = -np.mean(np.multiply(pred_error.T, X.T), axis=<span class="number">1</span>) + lamda*w </span><br><span class="line"><span class="comment"># 对m*n维矩阵在第一个维度求平均值，即axis=0，再转置</span></span><br><span class="line">w_grad = -np.mean(np.multiply(pred_error, X), axis=<span class="number">0</span>).T + lamda*w </span><br></pre></td></tr></table></figure></p><h2 id="3-卷积神经网络"><a href="#3-卷积神经网络" class="headerlink" title="3. 卷积神经网络"></a>3. 卷积神经网络</h2><p>搭建网络那就要按李宏毅说的三步走了！</p><ol><li>打开冰箱：定义网络结构，即选定一批函数</li><li>把大象放进冰箱里：确定损失函数和优化方法，即定义评价函数优劣的方法</li><li>关冰箱门：拟合数据集，找到最优的函数</li></ol><p>建议多查阅api文档：<a href="https://keras.io/api">Keras API文档</a></p><h3 id="3-1-定义网络结构：model-add"><a href="#3-1-定义网络结构：model-add" class="headerlink" title="3.1 定义网络结构：model.add"></a>3.1 定义网络结构：model.add</h3><h4 id="3-1-1-卷积层"><a href="#3-1-1-卷积层" class="headerlink" title="3.1.1 卷积层"></a>3.1.1 卷积层</h4><p>后面常接BatchNormalization，能加快训练和提升性能（未考究）</p><ul><li>在torch中：torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0.dilation=1, groups=1, bias=True, padding_mode=’zeros’),示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">nn.BatchNorm2d(<span class="number">64</span>)</span><br></pre></td></tr></table></figure></li><li>在keras中：keras.layers.Conv2D(<br>filters,<br>kernel_size,<br>strides=(1, 1),<br>padding=”valid”,<br>kernel_initializer=”glorot_uniform”,<br>bias_initializer=”zeros”,<br>…<br>)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(Conv2D(input_shape=(<span class="number">48</span>, <span class="number">48</span>, <span class="number">1</span>), filters=<span class="number">64</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">                 kernel_initializer=RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.05</span>, seed=SEED)))</span><br><span class="line">model.add(BatchNormalization())</span><br></pre></td></tr></table></figure><h4 id="3-1-2-激活层"><a href="#3-1-2-激活层" class="headerlink" title="3.1.2 激活层"></a>3.1.2 激活层</h4><ul><li>在torch中：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.LeakyReLU(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><ul><li>在keras中：不太理解，有的是在layers引入，有的在activations引入，也可以在layer的参数中指定<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(LeakyReLU(alpha=<span class="number">0.2</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(layers.Activation(activations.relu))</span><br></pre></td></tr></table></figure></li></ul><h4 id="3-1-3-池化层"><a href="#3-1-3-池化层" class="headerlink" title="3.1.3 池化层"></a>3.1.3 池化层</h4><ul><li>在torch中：torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>)  <span class="comment"># kernel_size=2, stride=2, padding=0</span></span><br></pre></td></tr></table></figure></li><li>在keras中：<br>keras.layers.MaxPooling2D(<br>pool_size=(2, 2), strides=None, padding=”valid”, …<br>)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure></li></ul><h4 id="3-1-4-全连接层"><a href="#3-1-4-全连接层" class="headerlink" title="3.1.4 全连接层"></a>3.1.4 全连接层</h4><ul><li>在torch中：torch.nn.Linear(in_features, out_features, bias=True)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear(<span class="number">256</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">1024</span>)</span><br></pre></td></tr></table></figure></li><li>在keras中：<br>keras.layers.Dense(<br>units,<br>activation=None,<br>use_bias=True,<br>kernel_initializer=”glorot_uniform”,<br>bias_initializer=”zeros”,<br>…<br>)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dense(units=<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br></pre></td></tr></table></figure><h4 id="3-1-5-一些优化"><a href="#3-1-5-一些优化" class="headerlink" title="3.1.5 一些优化"></a>3.1.5 一些优化</h4><p><strong>1. 添加kernel_initialization:</strong></p><ul><li>在torch中：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gaussian_weights_init</span>(<span class="params">m</span>):</span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">&#x27;Conv&#x27;</span>) != -<span class="number">1</span> <span class="keyword">and</span> classname.find(<span class="string">&#x27;Conv&#x27;</span>) == <span class="number">0</span>:</span><br><span class="line">        m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.cnn = nn.Sequential(</span><br><span class="line">           <span class="comment"># 省略大量代码</span></span><br><span class="line">        )</span><br><span class="line">self.fc = nn.Sequential(</span><br><span class="line">           <span class="comment"># 省略大量代码</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">self.cnn.apply(gaussian_weights_init)</span><br><span class="line">self.fc.apply(gaussian_weights_init)</span><br></pre></td></tr></table></figure></li><li>在keras中：在带参数的layer中添加参数kernel_initalization（好麻烦）</li></ul><p>问了gpt，他说可以这样，还没试过：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initializer = RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.05</span>)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers: </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(layer, <span class="string">&#x27;kernel_initializer&#x27;</span>): </span><br><span class="line">        layer.kernel_initializer = initializer</span><br></pre></td></tr></table></figure></p><p><strong>2. 添加Dropout层:</strong></p><p>丢弃部分神经网络的输入，减少过拟合（好像不能跟BatchNormalization一起用）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dropout(rate=<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure></p><h3 id="3-2-选择优化方法：model-complie"><a href="#3-2-选择优化方法：model-complie" class="headerlink" title="3.2 选择优化方法：model.complie"></a>3.2 选择优化方法：model.complie</h3><p>源码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&quot;rmsprop&quot;</span>,</span><br><span class="line">    loss=<span class="literal">None</span>,</span><br><span class="line">    loss_weights=<span class="literal">None</span>,</span><br><span class="line">    metrics=<span class="literal">None</span>,</span><br><span class="line">    weighted_metrics=<span class="literal">None</span>,</span><br><span class="line">    run_eagerly=<span class="literal">False</span>,</span><br><span class="line">    steps_per_execution=<span class="number">1</span>,</span><br><span class="line">    jit_compile=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    auto_scale_loss=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>optimizer: 优化器, 包括SGD, RMSprop, Adam等<br>metrics: 评价标准, 包括accuracy(准确率), binary_accuracy(二分类)、categorical_accuracy(多分类) 等<br>loss: 损失函数, 包括mse, binary_crossentropy, categorical_crossentropy等<br>verbose: 日志显示，verbose=0不显示, verbose=1为每个verbose显示进度条, verbose=2每个verbose输出一次</p><p>示例:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=Adam(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">              metrics=[keras.metrics.CategoricalAccuracy()])</span><br></pre></td></tr></table></figure></p><h3 id="3-3-拟合数据集：model-fit"><a href="#3-3-拟合数据集：model-fit" class="headerlink" title="3.3 拟合数据集：model.fit"></a>3.3 拟合数据集：model.fit</h3><p>fit的参数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x, y, batch_size, epochs, verbose, validation_split, validation_data, validation_freq)</span><br></pre></td></tr></table></figure><br>fit返回一个History对象记录了每一个epoch的数据，可用于绘图<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">H = model.fit(x_data, x_label,</span><br><span class="line">              epochs=EPOCHS,</span><br><span class="line">              batch_size=BATCH_SIZE,</span><br><span class="line">              validation_data=(val_data, val_label),</span><br><span class="line">              verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot</span></span><br><span class="line">N = np.arange(<span class="number">0</span>, EPOCHS)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(N, H.history[<span class="string">&quot;loss&quot;</span>], label=<span class="string">&quot;train loss&quot;</span>)</span><br><span class="line">plt.plot(N, H.history[<span class="string">&quot;val_loss&quot;</span>], label=<span class="string">&quot;val loss&quot;</span>)</span><br><span class="line">plt.plot(N, H.history[<span class="string">&quot;accuracy&quot;</span>], label=<span class="string">&quot;train_acc&quot;</span>)</span><br><span class="line">plt.plot(N, H.history[<span class="string">&quot;val_accuracy&quot;</span>], label=<span class="string">&quot;val_acc&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training Loss and Accuracy (Simple NN)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Epoch #&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss/Accuracy&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>fit后的模型可以对测试数据作预测<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_raw = model.predict(test_data)</span><br></pre></td></tr></table></figure><br>及时保存模型，我可不想白练了一晚<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">save_path = <span class="string">&#x27;./model_0120&#x27;</span></span><br><span class="line">model.save(save_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><blockquote><p>还有看到fit_generator的，之后再看一下</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建hexo博客时，deploy出现ssh连接超时的问题</title>
      <link href="/2024/01/17/%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%E6%97%B6%EF%BC%8Cdeploy%E5%87%BA%E7%8E%B0ssh%E8%BF%9E%E6%8E%A5%E8%B6%85%E6%97%B6%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2024/01/17/%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%E6%97%B6%EF%BC%8Cdeploy%E5%87%BA%E7%8E%B0ssh%E8%BF%9E%E6%8E%A5%E8%B6%85%E6%97%B6%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h1><p>hexo cl和hexo g都没有问题，但是在hexo d步骤出现了：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ssh: <span class="title">connect</span> <span class="title">to</span> <span class="title">host</span> <span class="title">github.com</span> <span class="title">port</span> 22: <span class="title">Connection</span> <span class="title">timed</span> <span class="title">out</span>  </span></span><br><span class="line"><span class="function"><span class="title">fatal</span>: <span class="title">Could</span> <span class="title">not</span> <span class="title">read</span> <span class="title">from</span> <span class="title">remote</span> <span class="title">repository</span>. <span class="title">Please</span> <span class="title">make</span> <span class="title">sure</span> <span class="title">you</span> <span class="title">have</span> <span class="title">the</span> <span class="title">correct</span> <span class="title">access</span> <span class="title">rights</span> <span class="title">and</span> <span class="title">the</span> <span class="title">repository</span> <span class="title">exists</span>.   </span></span><br><span class="line"><span class="function"><span class="title">FATAL</span>  <span class="title">Something</span>&#x27;<span class="title">s</span> <span class="title">wrong</span>. <span class="title">Maybe</span> <span class="title">you</span> <span class="title">can</span> <span class="title">find</span> <span class="title">the</span> <span class="title">solution</span> <span class="title">here</span>: <span class="title">https</span>://<span class="title">hexo.io</span>/<span class="title">docs</span>/<span class="title">troubleshooting.html</span> </span></span><br><span class="line"><span class="function"><span class="title">Error</span>: <span class="title">Spawn</span> <span class="title">failed</span></span></span><br></pre></td></tr></table></figure><h1 id="二、解决方案"><a href="#二、解决方案" class="headerlink" title="二、解决方案"></a>二、解决方案</h1><p>参考：<a href="https://blog.csdn.net/the__future/article/details/130038818">关于本地git通过ssh链接github时 time out问题的解决方法_github timeout-CSDN博客</a>)</p><p>在C:\Users\YourUserName\ .ssh目录下找到config文件，没有就新建一个<br>(其中，YourUserName是你的用户名，因人而异)</p><p>在windows中，这个config文件没有后缀，即没有指定类型，而在文件管理器中创建文件需要指定类型。有两种办法解决：</p><ol><li>我们可以复制同一目录下，类型仅为“文件”二字的其他文件，然后右键重命名为config</li><li>在平时写代码的IDE下可以直接创建没有指定类型的File(我知道IDEA可以)</li></ol><p>右键config文件，选择打开方式，选择记事本，将内建内容修改为如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host github.com HostName </span><br><span class="line">ssh.github.com </span><br><span class="line">User git Port 22</span><br><span class="line">PreferredAuthentications publickey </span><br><span class="line">IdentityFile C:\Users\YourUserName\.ssh\id_rsa # 解决问题的关键</span><br></pre></td></tr></table></figure><br>与引用的文章不同，我们解决问题的关键是第五行，将IdentityFile后面的文件路径修改为自己电脑下id_rsa的路径就好了</p><p>注意：ssh默认连接22端口，如果第三行将端口号修改成了443，应该在博客文件夹主配置文件_config.yaml中在deploy下指定端口，如：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span>  </span><br><span class="line">    <span class="attr">type:</span> <span class="string">git</span>  </span><br><span class="line">    <span class="attr">repo:</span> <span class="string">git@github.com:andreww1219/andreww1219.github.com.git</span>  </span><br><span class="line">    <span class="attr">branch:</span> <span class="string">master</span>  </span><br><span class="line">    <span class="attr">port:</span> <span class="number">443</span> <span class="comment"># 这句话指定端口</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 踩坑 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记——异常检测、推荐系统、大规模机器学习、图片OCR</title>
      <link href="/2024/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E5%9B%BE%E7%89%87OCR/"/>
      <url>/2024/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E5%9B%BE%E7%89%87OCR/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>完结撒花！！！但是感觉只是了解了部分算法的思想，具体的实现还得找东西练一练。</p><p>该篇笔记包括：</p><ol><li>第十五章————异常检测</li><li>第十六章————推荐系统</li><li>第十七章————大规模机器学习</li><li>第十八章————图片OCR</li></ol><p>视频链接：<a href="https://www.bilibili.com/video/BV164411b7dx">[中英字幕]吴恩达机器学习系列课程</a></p><h1 id="一、异常检测-Anomaly-Detection"><a href="#一、异常检测-Anomaly-Detection" class="headerlink" title="一、异常检测(Anomaly Detection)"></a>一、异常检测(Anomaly Detection)</h1><h2 id="1-问题动机"><a href="#1-问题动机" class="headerlink" title="1. 问题动机"></a>1. 问题动机</h2><p>在<strong>正常和异常的数据集都很大</strong>的时候，我们可以使用<strong>监督学习</strong>的算法，对正常类别以及出现各种异常类别进行区分。</p><p>但是，当<strong>异常的数据集较小而且异常种类很多</strong>时，监督学习很难对异常有明确的感觉，我们更倾向于使用接下来要提到的<strong>异常检测</strong>算法。</p><h2 id="2-异常检测算法"><a href="#2-异常检测算法" class="headerlink" title="2. 异常检测算法"></a>2. 异常检测算法</h2><h3 id="2-1-数据划分"><a href="#2-1-数据划分" class="headerlink" title="2.1 数据划分"></a>2.1 数据划分</h3><p>已知数据集有较大量的正常样本，以及少量异常样本，例如：10000正常样本，20异常样本。我们将其划分为：</p><ol><li>训练集：6000正常样本</li><li>验证集：2000正常样本，10异常样本</li><li>测试集：2000正常样本，10异常样本</li></ol><h3 id="2-2-算法描述"><a href="#2-2-算法描述" class="headerlink" title="2.2 算法描述"></a>2.2 算法描述</h3><p>给定训练集$(x^{(1)}, x^{(2)}, …, x^{(m)})$，样本的每一特征都互相独立且满足正态分布，即$x_j \sim N(\mu_j, \sigma_j^2)$</p><p>得到样本每个特征的均值和方差：</p><script type="math/tex; mode=display">\mu_j = \frac{1}{m} \sum_{i=1}^m x_j^{(i)}</script><script type="math/tex; mode=display">\sigma_j^2 = \frac{1}{m} \sum_{i=1}^m (x_j^{(i)} - \mu_j)^2</script><p>给定需要预测的样本$x$，计算该样本在已有均值和方差的情况下出现的概率，也就是正常的概率为：</p><script type="math/tex; mode=display">p(x) = p(x_1; \mu_1, \sigma_1^2) \times p(x_2; \mu_2, \sigma_2^2) \times... \times p(x_n; \mu_n, \sigma_n^2) = \prod_{j=1}^n p(x_j; \mu_j, \sigma_j^2)</script><p>该式子成立的隐含条件为样本的每一个特征互相独立</p><p>我们人为设置一个边界$ \epsilon $用于判断当$p(x) &lt; \epsilon $是，$ x $为异常点</p><h3 id="2-3-优化方法"><a href="#2-3-优化方法" class="headerlink" title="2.3 优化方法"></a>2.3 优化方法</h3><ol><li><p>选择合适的$\epsilon$：使用$F_1 = 2\frac{PR}{P+R}$评估异常检测系统，其中$P$为查准率，$R$为召回率，使用交叉验证集对$ \epsilon $作选择</p></li><li><p>对不服从正态分布的特征$x$做操作如：$x \leftarrow log(x + C)$、$ x \leftarrow x^C$ ，使其近似呈正态分布</p></li><li><p>当不同特征如$x_1$和$x_2$之间有相关性时，构造特征$x_3 = f(x_1, x_2)$如$x_3 = \frac{x_1}{x_2}$来鉴别新样本中$x_1$和$x_2$之间的关系正常</p></li><li><p>人工检查预测错误的异常点，增加对应的特征</p></li></ol><h2 id="3-多元高斯分布的异常检测"><a href="#3-多元高斯分布的异常检测" class="headerlink" title="3. 多元高斯分布的异常检测"></a>3. 多元高斯分布的异常检测</h2><h3 id="3-1-算法描述"><a href="#3-1-算法描述" class="headerlink" title="3.1 算法描述"></a>3.1 算法描述</h3><p>对每一特征都满足正态分布的训练集$(x^{(1)}, x^{(2)}, …, x^{(m)})$，求多元高斯分布的均值和协方差矩阵为</p><script type="math/tex; mode=display">\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}</script><script type="math/tex; mode=display">\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)(x^{(i)} - \mu)^T</script><p>给定需要预测的样本$x$，计算该样本是正常的概率为：</p><script type="math/tex; mode=display">p(x) = \frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}} } exp(-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu))</script><h3 id="3-2-与原本的模型比较"><a href="#3-2-与原本的模型比较" class="headerlink" title="3.2 与原本的模型比较"></a>3.2 与原本的模型比较</h3><ol><li>多元高斯分布能自动检测到各特征之间的相关性</li><li>多元高斯分布需要计算$n \times n$维矩阵$\Sigma$的逆，在$n$即特征数量很多时表现不好</li><li>多元高斯分布只有在$m \gt n$或者说矩阵$\Sigma$可逆时可以使用(未考究)</li></ol><h1 id="二、推荐系统-Recommender-Systems"><a href="#二、推荐系统-Recommender-Systems" class="headerlink" title="二、推荐系统(Recommender Systems)"></a>二、推荐系统(Recommender Systems)</h1><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>已知用户的数量$n_u$，作品的数量$n_m$，<br>$r(i, j)$表示用户$j$对作品$i$打过分，$y^{(i,j)}$表示用户用户$j$对作品$i$打的分，可记作$n_m \times n_u$维矩阵$Y$<br>$x^{(j)}$表示作品$j$的特征，可记作$n_m \times n$维矩阵$X$，$n$为特征的数量<br>$\theta^{(i)}$表示用户的偏好，即$(\theta^{(i)})^T x^{(i)}$可以用于预测$y^{(i,j)}$，可记作$n_u \times n$维矩阵$X\Theta^1T$<br>那么$X\Theta^T$可用于预测$Y$</p><ol><li>已知<strong>用户对部分作品的打分</strong>和<strong>作品的的特征</strong>，求<strong>用户的偏好</strong>，预测用户对其他作品的打分</li><li>已知<strong>用户对部分作品的打分</strong>和<strong>用户的偏好</strong>，推断<strong>作品的特征</strong></li></ol><h2 id="2-基于内容的推荐算法"><a href="#2-基于内容的推荐算法" class="headerlink" title="2. 基于内容的推荐算法"></a>2. 基于内容的推荐算法</h2><p>基于内容的推荐算法，适用的问题是：<br>已知<strong>用户对部分作品的打分</strong>和<strong>作品的的特征</strong>，求<strong>用户的偏好</strong>，预测用户对其他作品的打分</p><p>基于内容的推荐算法本质是梯度下降求解线性回归的问题，优化目标为</p><p>对于每个用户$j$，找到$\theta^{(j)}$，最小化损失函数如下</p><script type="math/tex; mode=display">J(\theta^{(j)}) = \frac{1}{2} \sum_{i: r(i, j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}( \theta_k^{(j)} )^2</script><p>由于每个用户之间互不相关，可以对每个用户的优化目标的加和作为总的优化目标，即最小化</p><script type="math/tex; mode=display">J(\theta^{(1)}, ..., \theta^{(n_u)}) = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i: r(i, j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}( \theta_k^{(j)} )^2</script><p>得到梯度下降的迭代式如下：</p><script type="math/tex; mode=display">\theta_k^{(j)} = \theta_k^{(j)} - \alpha \sum_{i: r(i, j) = 1} (\theta^{(j)})^T x^{(i)} - y^{(i, j)})x_k^{(i)} when \space k = 0</script><script type="math/tex; mode=display">\theta_k^{(j)} = \theta_k^{(j)} - \alpha ( \sum_{i: r(i, j) = 1} (\theta^{(j)})^T x^{(i)} - y^{(i, j)})x_k^{(i)} + \lambda \theta_k^{(j)})when \space k \ne 0</script><h2 id="3-协同过滤"><a href="#3-协同过滤" class="headerlink" title="3. 协同过滤"></a>3. 协同过滤</h2><p>同理可得，已知<strong>用户对部分作品的打分</strong>和<strong>用户的偏好</strong>，推断<strong>作品的特征</strong>可以转化为：</p><p>对于每个作品$i$，找到$x^{(i)}$，最小化损失函数如下：</p><script type="math/tex; mode=display">J(x^{(i)}) = \frac{1}{2} \sum_{j: r(i, j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}( x_k^{(i)} )^2</script><p>同理可得总的优化目标为最小化</p><script type="math/tex; mode=display">J(x^{(1)}, ..., x^{(n_m)}) = \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j: r(i, j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}( x_k^{(i)} )^2</script><p>与基于内容的推荐算法相结合，可以在随机初始化$\Theta$或$X$的情况下，对它们依次作梯度下降，比如：</p><script type="math/tex; mode=display">\Theta \rightarrow X \rightarrow \Theta \rightarrow X \rightarrow \Theta \cdots</script><p>或者，抽象出它们共同的优化目标，即最小化以下损失函数：</p><script type="math/tex; mode=display">J(\theta^{(1)}, ..., \theta^{(n_u)}, x^{(1)}, ..., x^{(n_m)}) = \frac{1}{2}  \sum_{(i, j): r(i, j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}( \theta_k^{(j)} )^2  + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}( x_k^{(i)} )^2</script><p>在<strong>仅有用户对部分作品打分</strong>的情况下，同时求解$\Theta$和$X$，其梯度下降的迭代式为</p><script type="math/tex; mode=display">x_k^{(i)} = x_k^{(i)} - \alpha ( \sum_{j: r(i, j) = 1} (\theta^{(j)})^T x^{(i)} - y^{(i, j)})\theta_k^{(j)} + \lambda x_k^{(i)})</script><script type="math/tex; mode=display">\theta_k^{(j)} = \theta_k^{(j)} - \alpha ( \sum_{i: r(i, j) = 1} (\theta^{(j)})^T x^{(i)} - y^{(i, j)})x_k^{(i)} + \lambda \theta_k^{(j)})</script><h2 id="4-实现细节"><a href="#4-实现细节" class="headerlink" title="4. 实现细节"></a>4. 实现细节</h2><ol><li>对矩阵$Y$作均值化，可以使模型对未曾打分的用户的预测有意义</li><li>可以用$|x^{(i)} - x^{(j)}|$表示两个作品的相似程度</li></ol><h1 id="三、大规模机器学习-Large-Scale-Machine-Learning"><a href="#三、大规模机器学习-Large-Scale-Machine-Learning" class="headerlink" title="三、大规模机器学习(Large Scale Machine Learning)"></a>三、大规模机器学习(Large Scale Machine Learning)</h1><h2 id="1-随机梯度下降、Min-Batch梯度下降和在线学习"><a href="#1-随机梯度下降、Min-Batch梯度下降和在线学习" class="headerlink" title="1. 随机梯度下降、Min-Batch梯度下降和在线学习"></a>1. 随机梯度下降、Min-Batch梯度下降和在线学习</h2><p>不同于原本的批量梯度下降(Batch Gradient Descend)每次迭代需要遍历并减去每个样本的偏导数的加和，随机梯度下降(Stochastic Gradient Descend, or SGD)的步骤为：</p><ol><li>打乱数据的顺序</li><li>遍历样本，每次迭代只减去一个样本的偏导数</li><li>重复以上步骤直到模型收敛</li></ol><p>注意：随机梯度下降只能使模型在最优解周围徘徊</p><p>Min-Batch梯度下降介于Batch梯度下降和随机梯度下降之间，需要选择每轮迭代使用的样本数$b$，步骤为：</p><ol><li>遍历样本，每次迭代减去$b$个样本求偏导数的加和</li><li>重复以上步骤直到模型收敛</li></ol><p>在线学习(Online Learning)应用在有不断涌入的用户流和数据流的情况，直接对每个用户提供的数据作拟合</p><h2 id="2-优化技巧"><a href="#2-优化技巧" class="headerlink" title="2. 优化技巧"></a>2. 优化技巧</h2><ol><li>画出学习曲线，预先检查模型是否是低偏差，是否能在大数据集的情况下被优化</li><li>设定随机梯度下降的学习率$\alpha = \frac{const_1}{iterationsCounts + cosnt_2}$，在迭代次数增加时，学习率下降，模型会收敛到最优解</li><li>Map-reduce：将数据分散到多个计算机或者内核去并行处理，最后汇总到中心服务器。</li></ol><h1 id="四、图片OCR-Photo-Optical-Character-Recognition"><a href="#四、图片OCR-Photo-Optical-Character-Recognition" class="headerlink" title="四、图片OCR(Photo Optical Character Recognition)"></a>四、图片OCR(Photo Optical Character Recognition)</h1><p>从Photo OCR去看设计复杂的机器学习系统的一些理念</p><ol><li>将一个复杂的机器学习系统分解为流水线上多个机器学习模块。例如，将图片OCR分解为：文本检测、字符分割、字符识别。滑动窗口+监督学习实现文本检测，监督学习完成字符分割和字符识别</li><li>人工数据：人工合成全新的数据 或者 在已有的数据上添加噪声</li><li>上限分析(Ceiling Analysis)：人为提高某个模块的准确率，观察其对整个系统的准确率的影响，评估出最值得优化的一个模块</li></ol>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 梯度下降 </tag>
            
            <tag> 异常检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记——机器学习系统设计、支持向量机(SVM)、K-Means算法、主成分分析法(PCA)</title>
      <link href="/2024/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E3%80%81%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM)%E3%80%81K-Means%E7%AE%97%E6%B3%95%E3%80%81%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95(PCA)/"/>
      <url>/2024/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E3%80%81%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM)%E3%80%81K-Means%E7%AE%97%E6%B3%95%E3%80%81%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95(PCA)/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这两天刷了四章视频，再来复盘一下：</p><ol><li>第十一章————机器学习系统设计</li><li>第十二章————支持向量机</li><li>第十三章————K-Means算法</li><li>第十四章————主成分分析法</li></ol><p>视频链接：<a href="https://www.bilibili.com/video/BV164411b7dx">[中英字幕]吴恩达机器学习系列课程</a></p><h1 id="一、机器学习系统设计"><a href="#一、机器学习系统设计" class="headerlink" title="一、机器学习系统设计"></a>一、机器学习系统设计</h1><ol><li>快速构建一个较为简单的模型，然后对模型进行验证，再决定下一步应该怎么优化，而不是过早地去优化。</li><li>在引入新的想法时，可对有无引入该想法地不同模型作验证，由<strong>单一指标</strong>确定引入新想法是否有效</li><li>在对偏斜类（或者 不对称类）作误差评估时，应该考虑到<strong>查准率</strong>(Percision, or P)和<strong>召回率</strong>(Recall, or R)，定义$F = 2\frac{PR}{P+R}$，通过比较$F$值选择模型保证两者处于较高的水平</li><li>在模型低偏差的情况下，增大数据集保证低方差，能有效提高模型效能。</li></ol><h1 id="二、支持向量机-Support-Vector-Machine-or-SVM"><a href="#二、支持向量机-Support-Vector-Machine-or-SVM" class="headerlink" title="二、支持向量机(Support Vector Machine, or SVM)"></a>二、支持向量机(Support Vector Machine, or SVM)</h1><h2 id="1-逻辑回归的局限性"><a href="#1-逻辑回归的局限性" class="headerlink" title="1. 逻辑回归的局限性"></a>1. 逻辑回归的局限性</h2><p>在分类问题中，当两个类别之间间隔较大时，我们能够得到许多不同的决策边界将其正确划分。但是不同的决策边界之间也有优劣性：<br>当决策边界与样本点相隔很近时，新的样本稍微有点不同就可能导致模型对其错误分类，也就是模型的<strong>泛化性</strong>不强。</p><p>因此，我们的优化目标就是找到一个最优决策边界，使得<strong>样本点到决策边界的间隔最大</strong>。</p><p><strong>为什么逻辑回归不能得到最优决策边界</strong></p><p>我们知道逻辑回归的损失函数为</p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{m} \sum_{i=1}^m y_i\ln h_\theta(x^{(i)}) + (1-y_i)\ln (1-h_\theta(x^{(i)})) + \lambda \sum_{j=1}^n \theta_j^2</script><p>当模型已经能够正确分类时，前一项几乎为0，我们的的优化目标就是最小化后一项，即最小化$\lambda \sum_{j=1}^n \theta_j^2 = \lambda | \theta |^2$</p><p>在逻辑回归中，决策边界为$\theta^T x = 0$，那么$\theta = (\theta_0, \theta_1, …, \theta_n)$为决策边界的法向量。</p><blockquote><p>二维的例子就是，当$\theta = (0, A, B)$，决策边界为直线$Ax + By = 0$时，直线方向向量$l = (-B, A)$与向量$\theta$互相垂直。</p></blockquote><p>那么样本点到决策边界的间隔可表示为向量$x$在向量$\theta$上的投影的绝对值，记投影</p><script type="math/tex; mode=display">p = \| x\| cos<\theta, x></script><p>其中$cos&lt;\theta, x&gt;$为向量$\theta$和向量$x$之间夹角的cos值。</p><p>决策边界的两边分别满足，$\theta^T x \ge 0$和$\theta^T x \lt 0$，又$\theta^T x = | \theta | | x| cos&lt;\theta, x&gt;$，得到</p><p>$ | \theta |  p\ge 0$ 和 $ | \theta |  p \lt 0$</p><p>这是$p$和$|\theta|$之间的限定关系，可知$p$的变化对$|\theta|$的减少没有什么帮助，于是逻辑回归自然就不会选择$p$更大的结果，也就是不会选择最优决策边界。</p><h2 id="2-SVM的引入"><a href="#2-SVM的引入" class="headerlink" title="2. SVM的引入"></a>2. SVM的引入</h2><p>由1可知，如果改变$p$和$|\theta|$之间的限定关系为</p><p>$ | \theta |  p\ge 1$ 和 $ | \theta |  p \lt -1$</p><p>为了使$|\theta|$减小，模型就会自动地去使$p$的绝对值增大，即选择最优决策边界。</p><p>为了实现这一目标，我们需要在原先的逻辑回归上做些手脚，修改损失函数为：</p><script type="math/tex; mode=display">J(\theta) = C\sum_{i=1}^m [y^{(i)} cost_1(\theta^T x^{(i)}) + (1-y^{(i)})cost_0(\theta^T x^{(i)}] + \frac{1}{2} \sum_{j=1}^n \theta_j^2</script><p>其中，</p><p>$cost_1(\theta^T x^{(i)}) 与 -\ln h_\theta(x^{(i)})$相似，但是在自变量$z$属于$(1,+\infty)$时，$cost_1(z)$的值为0</p><p>$cost_0(\theta^T x^{(i)}) 与 -\ln (1-h_\theta(x^{(i)}))$相似，但是在自变量$z$属于$(-\infty, -1)$时，$cost_0(z)$的值为0</p><p>也就是说，当$y = 1$时，我们希望$\theta^T x^{(i)} &gt;= 1$，当$y = 0$时，我们希望$\theta^T x^{(i)} &lt; -1$</p><p>这就是SVM之所以被叫做大间隔分类器的数学原理</p><h2 id="3-核函数-Kernel-Function"><a href="#3-核函数-Kernel-Function" class="headerlink" title="3. 核函数(Kernel Function)"></a>3. 核函数(Kernel Function)</h2><p>参考文献：<a href="https://zhuanlan.zhihu.com/p/261061617">详解SVM模型——核函数是怎么回事 - 知乎 (zhihu.com)</a></p><p>在实际问题中，我们在已有的维度无法找到一个线性的边界将两个类别划分开。所以我们需要通过一个映射关系，将低维的数据映射到高维，再从高维找到一个超平面将不同类别的样本划分。</p><p>所以核函数就是为了得到非线性决策边界，<strong>实现低维到高维映射关系</strong>且<strong>不增加运算的复杂度</strong>的函数。也有译作Kernel Trick，所有核函数本质是一种运算技巧。</p><p>一个使用频率很高的核函数是高斯核：<script type="math/tex">similarity(x_i, x_j) = exp(- \frac{\|x_i - x_j\|^2}{2\sigma^2})</script></p><p>由于SVM的数学性质(未考究)，把核函数与SVM结合在一起的表现特别出色。</p><p>例如，将高斯核与SVM相结合：</p><p>定义特征$f = (f_1, f_2, …, f_m)^T$取代损失函数中的$x$，其中$f_j = similarity(x, l^{(j)})$，$l^{(j)}$为样本中的某一点，那么有</p><script type="math/tex; mode=display">J(\theta) = C\sum_{i=1}^m [y^{(i)} cost_1(\theta^T f^{(i)}) + (1-y^{(i)})cost_0(\theta^T f^{(i)}] + \frac{1}{2} \sum_{j=1}^n \theta_j^2</script><blockquote><p>呜呜写不下去了，不知道SVM是怎么求解的</p></blockquote><h1 id="三、K-Means算法"><a href="#三、K-Means算法" class="headerlink" title="三、K-Means算法"></a>三、K-Means算法</h1><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>已知我们要将没有标签的数据分为$K$类，要怎么分才最合理？</p><p>设每个$c^{i}$为样本点$x^{(i)}$被分配到的聚类的下标<br>每个聚类中心的坐标为$\mu_k$<br>则$\mu_{c^{(i)}}$为样本点$x^{(i)}$被分配到的聚类的坐标</p><p>我们可以用<strong>样本点到其分配到的聚类中心的距离</strong>作为衡量分类合理性的标准，将问题转化为</p><p>如何最小化损失函数$J(c^{(1)}, …, c^{(m)}, \mu_1, …, \mu_K) = \frac{1}{m} \sum_{i=1}^m | x^{(i)} - \mu_{c^{(i)}}|^2$</p><p>我们也将它叫作失真代价函数(Cost Distortion)</p><h2 id="2-迭代过程"><a href="#2-迭代过程" class="headerlink" title="2. 迭代过程"></a>2. 迭代过程</h2><ol><li>修改每一个$c^{(i)}$为离x$^{(i)}$最近的聚类中心的下标，$1\le i \le m$</li><li>修改每一个$\mu_k$为聚类$k$中所有点的均值，$1\le k \le K$</li><li>重复以上两个步骤直到聚类中心不变</li></ol><h2 id="3-优化方法"><a href="#3-优化方法" class="headerlink" title="3. 优化方法"></a>3. 优化方法</h2><h3 id="3-1-随机初始化"><a href="#3-1-随机初始化" class="headerlink" title="3.1 随机初始化"></a>3.1 随机初始化</h3><p>不同的初始化结果会导致不同的聚类结果，我们可以选择其中损失函数最小的值作为最终的结果:  </p><p>进行一定次数：随机初始化聚类中心为$m$个点中的任意$k$点</p><ol><li>修改每一个$c^{(i)}$为离x$^{(i)}$最近的聚类中心的下标，$1\le i \le m$ </li><li>修改每一个$\mu_k$为聚类$k$中所有点的均值，$1\le k \le K$ </li><li>重复1、2步骤直到聚类中心不变 </li><li>计算$J(c^{(1)}, …, c^{(m)}, \mu_1, …, \mu_K) = \frac{1}{m} \sum_{i=1}^m | x^{(i)} - \mu_{c^{(i)}}|^2$</li></ol><p>从中找出$J$值最小的情况作为结果</p><h3 id="3-2-选择聚类数量"><a href="#3-2-选择聚类数量" class="headerlink" title="3.2 选择聚类数量"></a>3.2 选择聚类数量</h3><ol><li>手肘法: 画出聚类数量K与失真代价函数J的关系曲线，选择开始变平缓的第一个点</li><li>当手肘发不管用时，根据实际业务需求选择K的值</li></ol><h1 id="四、主成分分析法-Priciple-Component-Analysis-or-PCA"><a href="#四、主成分分析法-Priciple-Component-Analysis-or-PCA" class="headerlink" title="四、主成分分析法(Priciple Component Analysis, or PCA)"></a>四、主成分分析法(Priciple Component Analysis, or PCA)</h1><h2 id="1-问题描述-1"><a href="#1-问题描述-1" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>在使用原始数据需要的内存和计算量过于庞大以至于模型工作效率低的情况下，我们考虑对高维的数据$x^{(i)}$压缩为低维$z^{(i)}$：过$x^{(i)}$作一低维的超平面，将$x^{(i)}$投影到超平面上得到对应$z^{(i)}$。</p><p>对数据压缩应该尽可能保留原来的信息，$x^{(i)}$距超平面的距离越远将丢失越多的信息。将均值点与$x^{(i)}$连线，为使$x^{(i)}$距超平面的距离减小，那么$x^{(i)}$到超平面的垂足到均值点的距离应该更大，也就是要保证投影后的数据<strong>方差</strong>更大。</p><p>于是我们的优化目标为：找到一个新的坐标系，使得在新坐标系下数据的<strong>方差最大</strong>，为了减少数据的冗余，我们希望数据在每个方向上都是不相关的，也就是<strong>协方差为零</strong>。</p><h2 id="2-数据压缩"><a href="#2-数据压缩" class="headerlink" title="2. 数据压缩"></a>2. 数据压缩</h2><h3 id="2-1-数据预处理"><a href="#2-1-数据预处理" class="headerlink" title="2.1 数据预处理"></a>2.1 数据预处理</h3><ol><li>为了方便求协方差矩阵，我们将所有数据减去其均值</li><li>由于我们是用方差度量优化结果，为了防止方差较大的特征主导度量结果，应该做特征缩放。</li></ol><h3 id="2-2-PCA算法"><a href="#2-2-PCA算法" class="headerlink" title="2.2 PCA算法"></a>2.2 PCA算法</h3><p>参考：<a href="https://zhuanlan.zhihu.com/p/7751308">【机器学习】降维——PCA（非常详细） - 知乎 (zhihu.com)</a></p><p>参考：<a href="https://www.zhihu.com/question/330814299/answer/2540307765">怎么通俗地解释svd奇异值分解以及作用？ - 石溪的回答 - 知乎</a></p><p>已知原有的数据$X$为$n \times m$维的矩阵，其协方差矩阵$C = \frac{1}{m} XX^T$<br>求压缩后的矩阵$Y=PX$，且$Y$的协方差矩阵$D$是对角矩阵(协方差为零)，且对角线的元素尽可能大(方差最大化)</p><p>则有$ D = \frac{1}{m} (PX)(PX)^T$</p><p>$ D = \frac{1}{m} PX X^T P^T $</p><p>$ D = P \frac{1}{m}  X X^T P^T = PCP^T$</p><p>又协方差矩阵$C$是对称矩阵，设$E$为$C$的特征向量组成的矩阵，$\Lambda$为以特征值为对角线的矩阵，则有</p><p>$CE = E\Lambda$</p><p>由于对称矩阵的不同特征值对应的特征向量互相正交，则有$E^T = E^{-1}$，得</p><p>$E^T CE = \Lambda $</p><p>我们发现矩阵$\Lambda$正是我们需要的$D$，而取$E^T$的前$k$列正是我们需要的变换$P$</p><p>另一种方法是对$X$求奇异值分解:$[U, S, V] = svd(X)$，其中，<br>$U$表示变换后新坐标系的标准正交向量，其前$k$列正是我们需要的变换$P$，<br>奇异值矩阵$S$相当于上述矩阵$\Lambda$的开方，<br>$V$表示变换前坐标系的标准正交向量的转置</p><h2 id="3-主成分数量选择"><a href="#3-主成分数量选择" class="headerlink" title="3. 主成分数量选择"></a>3. 主成分数量选择</h2><p>主成分数量越少，丢失的信息量越多。保留的信息量可用2.2中奇异值分解得到的矩阵$S$表示：</p><script type="math/tex; mode=display">\frac{\sum_{j=1}^k S_{j, j}} {\sum_{i=1}^n S_{i, i}}</script><p>可人为要求该值大于一定比例，如0.99，0.95等，确保数据压缩能保留足够多的信息</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> PCA </tag>
            
            <tag> K-Means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文讲清楚机器学习中的正则化、神经网络、机器学习诊断法</title>
      <link href="/2024/01/14/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%8A%E6%96%AD%E6%B3%95/"/>
      <url>/2024/01/14/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%8A%E6%96%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>学习吴恩达机器学习视频的第二篇笔记，内容包括：</p><ol><li>第七章————正则化</li><li>第八、九章————神经网络</li><li>第十章————机器学习诊断法</li></ol><p>个人博客页：<a href="https://andreww1219.github.io/2024/01/14/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%8A%E6%96%AD%E6%B3%95/">一文讲清楚机器学习中的正则化、神经网络、机器学习诊断法 | Andrew的个人博客 (andreww1219.github.io)</a></p><h1 id="一、正则化"><a href="#一、正则化" class="headerlink" title="一、正则化"></a>一、正则化</h1><h2 id="1-应用背景"><a href="#1-应用背景" class="headerlink" title="1. 应用背景"></a>1. 应用背景</h2><p>在前面对线性回归、逻辑回归的学习中，我们都是构造一个假设$h_\theta(x)$，然后得到他的损失函数$J(\theta)$，对其进行梯度下降操作使损失函数的值尽可能小。总而言之，就是对我们的训练集作出最好的拟合。</p><p>然而，当我们增加特征的维度，完美地去拟合我们的训练集时，会得到一条相当扭曲，不具有泛化性地曲线，这就是过拟合的问题。解决过拟合问题主要有以下两个手段：</p><ol><li>减少特征的数量$n$</li><li>采用正则化（也就是我们今天要介绍的内容）</li></ol><h2 id="2-什么是正则化？"><a href="#2-什么是正则化？" class="headerlink" title="2. 什么是正则化？"></a>2. 什么是正则化？</h2><p>我们采集了许多特征，并且相信这些特征对假设的预测值是有用的，并不想去舍弃它，又担心过拟合的问题。所以我们尽可能减小每个特征或某个特征对整体预测值的影响，在损失函数中增加<strong>对高特征值的惩罚</strong>，这就是正则化的思想。</p><h2 id="3-线性回归中的正则化"><a href="#3-线性回归中的正则化" class="headerlink" title="3. 线性回归中的正则化"></a>3. 线性回归中的正则化</h2><h3 id="3-1-梯度下降中的正则化"><a href="#3-1-梯度下降中的正则化" class="headerlink" title="3.1 梯度下降中的正则化"></a>3.1 梯度下降中的正则化</h3><p>由2可知，我们要在损失函数$J(\theta)$增加对高特征值的惩罚，得到：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n \theta_j^2]</script><p>得到 <script type="math/tex">\frac{\partial J(\theta) }{\partial \theta_j}= \frac{1}{m} [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j]</script></p><p>那么有：  </p><script type="math/tex; mode=display">\begin{cases}\theta_0 = \theta_0 - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \\\theta_j = \theta_j - \frac{\alpha}{m} [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j] = \theta_j(1 - \frac{\alpha\lambda}{m}) - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\end{cases}</script><h3 id="3-2-正规方程中的正则化"><a href="#3-2-正规方程中的正则化" class="headerlink" title="3.2 正规方程中的正则化"></a>3.2 正规方程中的正则化</h3><p>将损失函数表示为矩阵形式，可得：</p><p>$J(\theta) = \frac{1}{2m} [(X\theta - Y)^T(X\theta - Y) + \lambda \theta^T \theta]$</p><p>得到 <script type="math/tex">\frac{\partial J(\theta) }{\partial \theta}= \frac{1}{m} [X^TX\theta - X^TY + \lambda \theta]</script></p><p>令  <script type="math/tex">\frac{\partial J(\theta) }{\partial \theta} = 0</script>，得</p><script type="math/tex; mode=display">\theta = (X^TX - \lambda E)X^TY</script><p>由于约定俗成的规则，$\theta_0$不参与正则化（实际参与得到的结果也差别不大），所以将上述式子中$E_{i,j}$的位置修改为0，就是正规方程带正则化的表达式。</p><h2 id="4-逻辑回归中的正则化"><a href="#4-逻辑回归中的正则化" class="headerlink" title="4. 逻辑回归中的正则化"></a>4. 逻辑回归中的正则化</h2><p>同线性回归中的梯度下降，我们可以得到相同的结果即：</p><script type="math/tex; mode=display">\begin{cases}\theta_0 = \theta_0 - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\\\theta_j = \theta_j - \frac{\alpha}{m} [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j] = \theta_j(1 - \frac{\alpha\lambda}{m}) - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\end{cases}</script><h1 id="二、神经网络"><a href="#二、神经网络" class="headerlink" title="二、神经网络"></a>二、神经网络</h1><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>输入层(Input Layer)：已知m组数据$x^{(1)}, x^{(2)}, …, x^{(m)}$，每组数据表示n个特征,可写为一个n+1维的向量<br>即：<script type="math/tex">(x^{(i)})^T = (x^{(i)}_0, x^{(i)}_1, ..., x^{(i)}_n)</script>，其中，$x^{(i)}_0=1$。</p><p>隐层(Hidden Layer)：介于输入层和输出层之间，每个隐层的每个结点接收上一个隐层（或输入层）的信号，经过一定处理后得到新的信号并传递给下一个隐层（或输出层）</p><p>输出层(Output Layer)：接受最后一个隐层的信号，经过加权处理后输出$y^{(i)}$。在多元分类问题中，向量$y^{(i)}$的维度就是所需要区分的类别的数量。</p><h2 id="2-前向传播-Forward-Propagation"><a href="#2-前向传播-Forward-Propagation" class="headerlink" title="2. 前向传播(Forward Propagation)"></a>2. 前向传播(Forward Propagation)</h2><p>我们用$z_i^{(j)}$表示第$j$层的第$i$个结点接收到的前一层的值加权后的总和，$a_i^{(j)}$表示第$j$层的第$i$个结点的输出，矩阵$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权值，行数为$j+1$层的节点数，列数为$j$层的节点数+1（存在常数固定层，可看作激活阈值），即</p><p>$z_i^{(j+1)} = \sum_{k=0}^{第j层的结点数} \Theta_{i, k}^{(j)} a_k^j$</p><p>$a_i^{(j+1)} = g(z_i^{(j+1)})$</p><p>写成矩阵形式，则有 $ a^{(j+1)} = g( \Theta^{(j)} a^{(j)} )$</p><p>于是我们可以写出神经网络的假设$h_\theta(x^{(i)}) = g( \Theta^{(L-1)} a^{(L-1)} ))$，其中$L$为神经网络的层数。</p><h2 id="3-反向传播-Back-Propagation"><a href="#3-反向传播-Back-Propagation" class="headerlink" title="3. 反向传播(Back Propagation)"></a>3. 反向传播(Back Propagation)</h2><h3 id="3-1-定义损失函数"><a href="#3-1-定义损失函数" class="headerlink" title="3.1 定义损失函数"></a>3.1 定义损失函数</h3><p>有了假设，我们想要优化现有的模型，就需要对原有参数进行梯度下降，需要定义损失函数$J(\Theta)$，并对每一个参数即$\Theta_{i, k}^{(j)}$求偏导数$ \frac{\partial J(\Theta)}{\partial \Theta_{i, k}^{(j)}} $</p><p>于是我们有预测值$h_\theta(x) = a^{(L)}$，真实值$y$，第$j$层的节点数为$s_j$，则有</p><p>每个样本的均方误差 $E = \frac{1}{2} \sum_{i=1}^{s_L} (a_i^{(L)} - y_i)^2$</p><p>损失函数$J(\Theta) = \frac{1}{m} \sum_{i=1}^m E^{(i)} $</p><p>损失函数对参数的偏导数$\frac{\partial J(\Theta)}{\partial \Theta^{(j)}} = \frac{1}{m} \sum_{i=1}^m \frac{\partial  E^{(i)}}{\partial \Theta^{(j)}} $</p><h3 id="3-2-引入链式求导法"><a href="#3-2-引入链式求导法" class="headerlink" title="3.2 引入链式求导法"></a>3.2 引入链式求导法</h3><p>我们想要直接求$E$对$\Theta_{i, k}^{(j)}$的偏导数不太容易，因为我们的预测值是前面一步一步传递过来的，离输出层越远，$E$和$\Theta_{i, k}^{(j)}$的关系就越复杂。但是其中项之间的关系简单，偏导数易求，所以我们可以通过链式求导法则，由中间项之间的偏导数累乘得到$E$对$\Theta_{i, k}^{(j)}$的偏导数。</p><p>对输出层的参数，则有$ \frac{\partial E}{\partial \Theta_{i, k}^{(L-1)}}  =<br>\frac{\partial E}{\partial a_i^{(L)}}<br>\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}}<br>\frac{\partial z_i^{(L)}}{\partial \Theta_{i, k}^{(L-1)}}$</p><p>其中，$\frac{\partial E}{\partial a_i^{(L)}} = a_i^{(L)} - y_i,<br>\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}} = g’(z_i^{(L)}) = a_i^{(L)}(1 - a_i^{(L)}),<br>\frac{\partial z_i^{(L)}}{\partial \Theta_{i, k}^{(L-1)}} =a_k^{(L-1)} $</p><p>得$ \frac{\partial E}{\partial \Theta_{i, k}^{(L-1)}}  =<br>(a_i^{(L)} - y_i) a_i^{(L)}(1 - a_i^{(L)}) a_k^{(L-1)}$</p><p>我们将$\frac{\partial E}{\partial z_i^{(L)}} =<br>\frac{\partial E}{\partial a_i^{(L)}}<br>\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}}  =<br>(a_i^{(L)} - y_i) a_i^{(L)}(1 - a_i^{(L)})$ 记作输出层的误差(error)为$\delta_i^{(L)}$，即</p><p>$\delta^{(L)} = (a^{(L)} - y) .<em> g’(z^{(L)})$<br><em>*(存疑，视频中取的$\delta^{(L)} = a^{(L)} - y$)</em></em></p><p>于是我们用$\delta^{(l)}$表示<strong>均方误差$E$对第$l$层$z$的偏导</strong></p><h3 id="3-3-矩阵形式"><a href="#3-3-矩阵形式" class="headerlink" title="3.3 矩阵形式"></a>3.3 矩阵形式</h3><p>矩阵/向量/标量之间求导参考：<a href="https://zhuanlan.zhihu.com/p/262751195?utm_id=0">机器学习中的数学理论1：三步搞定矩阵求导</a></p><p>我们希望得到$E$对结果层参数$\Theta^{(L-1)}$的偏导数，即标量对矩阵的求导，使用<strong>分母布局</strong>，得到与$\Theta^{(L-1)}$相同维度的矩阵，方便减法运算作梯度下降。</p><p>由链式求导法则，有</p><p>$ \frac{\partial E}{\partial \Theta^{(L-1)}}  =<br>\frac{\partial E}{\partial z^{(L)}}<br>\frac{\partial z^{(L)}}{\partial \Theta^{(L-1)}}$</p><p>由于$\frac{\partial E}{\partial z^{(L)}} = \delta^{(L)}$，</p><p>又$z^{(L)} = \Theta^{(L-1)} a^{(L-1)}$，则有$ \frac{\partial z^{(L)}}{\partial \Theta^{(L-1)}} =<br>\frac{\partial \Theta^{(L-1)} \alpha^{(L-1)}}{\partial \Theta^{(L-1)}} = (\alpha^{(L-1)})^T<br>$（使用分母布局）</p><p>综上所述，$ \frac{\partial E}{\partial \Theta^{(L-1)}}  = \delta^{(L)} (\alpha^{(L-1)})^T$</p><p>记均方误差$E$对$\Theta^{(j)}$的偏导为$\Delta^{(j)}$，则有$ \Delta^{(L-1)} = \delta^{(L)} (\alpha^{(L-1)})^T$</p><h3 id="3-3-推导递推公式"><a href="#3-3-推导递推公式" class="headerlink" title="3.3 推导递推公式"></a>3.3 推导递推公式</h3><p>已知$\delta^{(l-1)} =<br>\frac{\partial E}{\partial z^{(l-1)}}$，那么</p><p>$\delta^{(l-1)} =<br>\frac{\partial E}{\partial z^{(l)}}<br>\frac{\partial z^{(l)}}{\partial z^{(l-1)}} =<br>\delta^{(l)}<br>\frac{\partial z^{(l)}}{\partial a^{(l-1)}}<br>\frac{\partial a^{(l-1)}}{\partial z^{(l-1)}} =<br>(\Theta^{(l-1)})^T \delta^{(l)} .* g’(z^{(l-1)})<br>$</p><p>即$\delta^{(l-1)} =<br>(\Theta^{(l-1)})^T \delta^{(l)} .* g’(z^{(l-1)})<br>$ ，这就是误差反向传导的递推公式</p><p>又$\frac{\partial z^{(l-1)}}{\partial \Theta^{(l-1)}} =<br>(a^{(l-1)})^T<br>$<br>同理</p><p>$\Delta^{(l-1)} = \frac{\partial E}{\partial \Theta^{(l-1)}}  = \delta^{(l)} (a^{(l-1)})^T$</p><h3 id="3-4-小结"><a href="#3-4-小结" class="headerlink" title="3.4 小结"></a>3.4 小结</h3><p>记损失函数对$\Theta^{(l)}$的偏导数为$D^{(l)} = \frac{\partial J(\Theta)}{\partial \Theta^{(l)}}$，则有</p><p>$D_{i, j}^{(l)} = \frac{1}{m} \sum^m \Delta_{i, j}^{(l)} =<br>\frac{1}{m} \sum^m \delta_i^{(l)} a_j^{(l-1)}$</p><p>若引入正则化项，则有</p><p>$D_{i, j}^{(l)} = \frac{1}{m} \sum^m \Delta_{i, j}^{(l)} =<br>\frac{1}{m} \sum^m \delta_i^{(l)} a_j^{(l-1)} + \lambda \Theta_{i, j}^{(l)}$</p><h1 id="三、机器学习诊断法"><a href="#三、机器学习诊断法" class="headerlink" title="三、机器学习诊断法"></a>三、机器学习诊断法</h1><h2 id="1-问题描述-1"><a href="#1-问题描述-1" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>我们需要评估我们训练出来的模型，判断是否过拟合，即高方差(variance)，还是欠拟合，即高偏差(bias)。通常情况下，我们有许多方式去改进模型，但是不知道应该选哪一种。我们将讨论各个参数与偏差、方差的关系，帮助我们确定模型是处于高方差还是高偏差，应该怎样采取调整策略。</p><h2 id="2-讨论变量"><a href="#2-讨论变量" class="headerlink" title="2.讨论变量"></a>2.讨论变量</h2><p>我们将遵循同一分布的数据集划分为训练集和验证集，训练集用来训练得到某一变量在不同值下的模型，由验证集对该变量进行选择。</p><p>记训练集误差为$J_{train}(\theta)$，验证集误差为$J_{cv}(\theta)$</p><p>我们选择测试集误差最小的模型作为结果</p><p>但是，要如何验证我们模型的泛化能力？即模型对不同但具有相似分布的数据集(测试集)的表现。</p><p>所以我们一开始将数据集划分为训练集、验证集和测试集，由验证集对变量进行选择，由测试集对模型的泛化能力进行评估，<strong>不能将测试集用于调整参数，否则会导致过拟合</strong></p><h3 id="2-1-特征数-d"><a href="#2-1-特征数-d" class="headerlink" title="2.1 特征数$d$"></a>2.1 特征数$d$</h3><p>我们记参数的多项式次数（相当于特征数）为d</p><p>当特征数过少时，训练集误差$J_{train}(\theta)$和验证集误差$J_{cv}(\theta)$都很大，模型欠拟合；<br>当特征数过多时，训练集误差$J_{train}(\theta)$很小，而验证集误差$J_{cv}(\theta)$较大，模型过拟合</p><h3 id="2-2-正则化参数-lambda"><a href="#2-2-正则化参数-lambda" class="headerlink" title="2.2 正则化参数$\lambda$"></a>2.2 正则化参数$\lambda$</h3><p>当$\lambda$很小时，训练集误差$J_{train}(\theta)$很小，而验证集误差$J_{cv}(\theta)$较大，模型过拟合;<br>当$\lambda$很大时，训练集误差$J_{train}(\theta)$和验证集误差$J_{cv}(\theta)$都很大，模型欠拟合</p><h3 id="2-3-数据集规模-m"><a href="#2-3-数据集规模-m" class="headerlink" title="2.3 数据集规模$m$"></a>2.3 数据集规模$m$</h3><p>我们将数据集规模$m$与误差关系的曲线叫做学习曲线</p><p>在高偏差即欠拟合的情况下，随着数据集规模$m$的增大，训练集误差将非常接近于验证集误差<br>在高方差即过拟合的情况下，随着数据集规模$m$的增大，验证集误差能有效减小，说明在过拟合情况下，增大数据规模是有效的</p><h2 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a>3. 小结</h2><p>当模型高方差时，我们可以选择：</p><ol><li>增大数据集规模</li><li>减小特征数$d$</li><li>增大正则化参数$\lambda$</li></ol><p>当模型高偏差时，我们可以选择：</p><ol><li>增加特征数$d$</li><li>减小正则化参数$\lambda$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> BP神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文讲清楚线性回归(Linear Regression)、逻辑回归（Logistic Regression）</title>
      <link href="/2024/01/11/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(Linear%20Regression)%E3%80%81%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic%20Regression%EF%BC%89/"/>
      <url>/2024/01/11/%E4%B8%80%E6%96%87%E8%AE%B2%E6%B8%85%E6%A5%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(Linear%20Regression)%E3%80%81%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic%20Regression%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>被各种ddl加上期末考试耽搁了一段时间，准备继续肝机器学习了，对前面学的东西做个阶段性总结。<br>刷的是吴恩达的课：<a href="https://www.bilibili.com/video/BV164411b7dxZ0">[中英字幕]吴恩达机器学习系列课程</a></p><h1 id="一、线性回归-Linear-Regression"><a href="#一、线性回归-Linear-Regression" class="headerlink" title="一、线性回归(Linear Regression)"></a>一、线性回归(Linear Regression)</h1><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>已知m组数据$x^{(1)}, x^{(2)}, …, x^{(m)}$，每组数据表示n个特征,可写为一个n+1维的向量<br>即：<script type="math/tex">(x^{(i)})^T = (x^{(i)}_0, x^{(i)}_1, ..., x^{(i)}_n)</script>，其中，$x^{(i)}_0=1$<br>每组数据对应一个真实值$y^{(i)}$，所有$y$值在一块可表示为一个m维向量$y$<br>我们需要构造一个预测值：$\widehat{y}=\theta_0+\theta_1x_1+…+\theta_nx_n$<br>该函数叫做我们的hypothesis(假设)，记为$h_\theta(x)$，<br>那么有$h_\theta(x)=\theta^Tx$，其中$\theta^T$和$x$都为n+1维的向量，我们要求解的就是<strong>使预测值最准确的$\theta$值</strong></p><h2 id="2-梯度下降-Gradient-Descend"><a href="#2-梯度下降-Gradient-Descend" class="headerlink" title="2. 梯度下降(Gradient Descend)"></a>2. 梯度下降(Gradient Descend)</h2><p>那么要怎么表示我们的预测和真实值的误差呢？我们引入cost function(代价函数 或 <strong>损失函数</strong>)，记为$J(\theta)$，在线性回归中，我们通常使用平方误差作为损失函数，即：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y)^2</script><p>现在我们要做的，就是找到一个$\theta$值，使得损失函数的值最小，最直接的方法就是求导找极值点，故有：<br>$\frac{\partial J(\theta)}{\partial\theta} = \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y)x^{(i)}$，前面取$x^{(i)}_0=1$是为了使这里的偏导都符合同一形式。</p><p>在这里，当$\frac{\partial J(\theta)}{\partial\theta}&gt;0$时，我们的损失函数会随着$\theta$的增大而增大，当$\frac{\partial J(\theta)}{\partial\theta}&lt;0$时，我们的们的损失函数会随着$\theta$的增大而减小。</p><p>故只要将$\theta$减去偏导数的一定倍数（我们称之为<strong>学习率$\alpha$</strong>），就可保证我们的$\theta$更优。之后重复这一过程，直到$\theta$收敛到一定区间，说明我们的$\theta$达到局部最优，这就是梯度下降法。</p><p><strong>需要注意的是</strong>：$\theta$会达到全局最优的条件是：我们的损失函数$J(\theta)$是<strong>凹函数</strong>，即$\frac{\partial^2 J(\theta)}{\partial\theta^2}&gt;0$。在使用平方误差作为我们的预测值时，能保证这一条件，故可使用梯度下降。</p><p>梯度下降中$\theta$的迭代公式为: $\theta_j = \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta}, j = 1, 2, …, n$<br>使用平方误差时: $\theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y)x_j^{(i)}$</p><h3 id="2-1-特征缩放-Feature-Scaling"><a href="#2-1-特征缩放-Feature-Scaling" class="headerlink" title="2.1 特征缩放(Feature Scaling)"></a>2.1 特征缩放(Feature Scaling)</h3><p>每组数据$x$包含m个维度，每个维度的数据范围不尽相同，例如$-200&lt;x_1&lt;400$，而$-3&lt;x_2&lt;3$，对他们的预测值求平方误差后，对$\theta_j$的影响幅度不同，这导致了$\theta$之间形成的等高线会相当陡峭，拟合的过程相对曲折。</p><p>所以，我们最好将每个维度的数据都放缩到接近$-1&lt;x_j&lt;1$的范围，下面有个示例：<br>已知：$-200&lt;x_1&lt;400$，取$x_1 = \frac{x_1 - 100}{600} (100 = \frac{-200+400}{2}, 600 = 400-(-200))$，得到新的$-0.5&lt;x_1&lt;0.5$</p><h3 id="2-2-学习率-Learning-Rate"><a href="#2-2-学习率-Learning-Rate" class="headerlink" title="2.2 学习率(Learning Rate)"></a>2.2 学习率(Learning Rate)</h3><p>选择学习率是一件需要技巧的事情，过低的学习率会使$\theta$收敛较慢，过高的学习率可能会使$\theta$无法准确收敛，甚至出现发散。</p><p>吴恩达的视频里只是讨论了学习率过高和学习率过低的影响，我就在想既然$\theta$是在计算中一步一步迭代的，那有没有方法也能对$\alpha$进行迭代。网上果然能搜到许多方案，后面再深入了解一下吧。<br><a href="https://zhuanlan.zhihu.com/p/390261440">可能是深度学习中最重要的超参数：学习率 - 知乎 (zhihu.com)</a></p><h2 id="3-正规方程-Normal-Equation"><a href="#3-正规方程-Normal-Equation" class="headerlink" title="3. 正规方程(Normal Equation)"></a>3. 正规方程(Normal Equation)</h2><p>将m组数据表示为$m*(n+1)$维的矩阵，记为$X$，每组数据对应的真实值表示为一个m维向量$y$<br>则我们所需要的最优的$\theta=(X^T X)^{-1}X^Ty$<br>视频中没有给出证明，根据<a href="https://zhuanlan.zhihu.com/p/60719445">详解正规方程（Normal Equation） - 知乎 (zhihu.com)</a>，先用矩阵形式表示损失函数即$Cost(\theta) = \frac{1}{2m} (X\theta - Y)^T(X\theta - Y)$,对其求偏导，经过很多多多步化简，得到$\Delta = \frac{\partial Cost(\theta)}{\partial \theta} = \frac{1}{m}(X^T X\theta - X^TY)$ 令$\Delta = 0$，得到正规方程$\theta=(X^T X)^{-1}X^Ty$</p><h2 id="4-梯度下降和正规方程的比较"><a href="#4-梯度下降和正规方程的比较" class="headerlink" title="4. 梯度下降和正规方程的比较"></a>4. 梯度下降和正规方程的比较</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><ol><li>需要选择学习率</li><li>需要多步迭代，且每一步迭代都需要对每个维度的偏导数进行计算</li><li>在$n$较大，即数据维度很大时表现很好</li></ol><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><ol><li>不需要选择学习率，不需要迭代，一步算出答案</li><li>矩阵的逆的计算复杂度很高，在$n$较大，即数据维度很大时表现不好</li></ol><h1 id="二、逻辑回归-Logistic-Regression"><a href="#二、逻辑回归-Logistic-Regression" class="headerlink" title="二、逻辑回归(Logistic Regression)"></a>二、逻辑回归(Logistic Regression)</h1><h2 id="1-问题描述-1"><a href="#1-问题描述-1" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>已知m组数据$x^{(1)}, x^{(2)}, …, x^{(m)}$，每组数据表示n个特征,可写为一个n+1维的向量<br>即：<script type="math/tex">(x^{(i)})^T = (x^{(i)}_0, x^{(i)}_1, ..., x^{(i)}_n)</script>，其中，$x^{(i)}_0=1$<br>且每组数据的真实值$y^{(i)}$只能为0或1<br>现在我们需要构造一个预测值$h_\theta(x)$，同时求解使预测值最准确的$\theta$值</p><h2 id="2-Sigmoid函数的引入"><a href="#2-Sigmoid函数的引入" class="headerlink" title="2. Sigmoid函数的引入"></a>2. Sigmoid函数的引入</h2><p>更加自然的引入请见：<a href="https://andreww1219.github.io/2024/01/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">机器学习笔记——概率生成模型 | Andrew的个人博客 (andreww1219.github.io)</a></p><p>————————分割线——————————</p><p>以下是未了解概率生成模型时的解释</p><p>前面说到的线性回归，是对$x$的每一个维度$x_j$引入一个权值$\theta_j$，简单来说就是，通过$\theta$作用于$x$中每个维度的信息，得到预测值，而我们想要的预测值范围可以是实数域上任意的数，所以就直接用$\theta^Tx$作为预测值。</p><p>而现在，真实值是离散的，只能为0或1，使用$h_\theta(x)=\theta^Tx$可能会得到$\infty$这样的结果。我们需要的是一个关于$\theta^Tx$的函数，即$h_\theta(x)=g(\theta^Tx)$，且函数$g(z)$是能够表示离散值取值的函数</p><p>因此引入sigmoid函数$g(z)=\frac{1}{1+e^{-z}}$，这个函数有很好的性质如$0&lt;g(z)=\frac{1}{1+e^{-z}}&lt;1$，可以用它来表示某一事件的概率。我们将$z$替换为$h_\theta(x)=\theta^Tx$，可得$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$，表示在给定$\theta$的情况下，数据$x$使y值得到1的概率。</p><h2 id="3-用概率论引出损失函数"><a href="#3-用概率论引出损失函数" class="headerlink" title="3. 用概率论引出损失函数"></a>3. 用概率论引出损失函数</h2><p>参考文献：<a href="https://blog.csdn.net/github_39421713/article/details/89213747">伯努利分布的最大似然估计_伯努利分布的似然函数-CSDN博客</a></p><p>上文中我们定义了$h_\theta(x)$为y值等于1的概率，我们将其记为$p$<br>在伯努利分布中：$P(Y=0) = 1 - p， P(Y=1) = p$<br>由于Y只能取0和1，我们可以将其概率分布用一个式子表示：$P(Y=y) = p^y(1-p)^{(1-y)}$</p><p>我们记$f(y_i |p)=P(Y=y_i)$表示给定p下，我们取到真实值的概率，及预测的准确率<br>我们将取到所有真实值的概率累乘，得到关于p的似然函数$L(p) = \prod_{i=1}^m{f(y_i|p)}$<br>那么当似然函数$L(p)$即$L(h_\theta(x)$的取值越大时，我们的预测越准确，<br>反之，$-L(p)$即$-L(h_\theta(x))$的取值越大时，我们的预测越不准确，是的你没听错，这个函数反映了我们预测的误差，这正是我们要找的损失函数……的雏形。<br>先别着急，我们想要的损失函数应该方便求导之后做梯度下降，而似然函数是许多$p$的指数形式连乘，不方便求导，于是对两边取对数<br>则有$-\ln L(p) = -\sum_{i=1}^m y_i\ln p+(1-y_i)\ln (1-p)$<br>即$-\ln L(h_\theta(x)) = -\sum_{i=1}^m y_i\ln h_\theta(x^{(i)}) + (1-y_i)\ln (1-h_\theta(x^{(i)}))$<br>两边再除以$m$取个平均值，得到损失函数得表达式如下：</p><p>$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y_i\ln h_\theta(x^{(i)}) + (1-y_i)\ln (1-h_\theta(x^{(i)}))$</p><p>之后就是进入梯度下降的步骤了</p><p>$\theta_j = \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta}, j = 1, 2, …, n$</p><p><strong>为什么我们费了这么大劲就是为了找这个损失函数，用平方误差不好吗？</strong><br>还记得我们前面说的吗？：<br>$\theta$会达到全局最优的条件是：我们的损失函数$J(\theta)$是凹函数<br>之前我们的预测值$h_\theta(x) = \theta^Tx$时，平方误差确实是凹函数，而我们现在的预测值改成了Sigmoid函数，即$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$，平方误差就不再是凹函数了，这点是是很致命的。</p><p><strong>那为什么我们现在的损失函数就是凹函数呢</strong><br>这点可以通过求$\frac{\partial^2 J(\theta)}{\partial\theta^2}&gt;0$去证明<br>可以参考：<a href="https://blog.csdn.net/lafengxiaoyu/article/details/109916750">为什么不用平方误差（MSE）作为Logistic回归的损失函数？_平方误差成本函数不适用于逻辑回归-CSDN博客</a></p><p>最终得到逻辑回归中梯度下降的表达式：<br>$\theta_j = \theta_j - \alpha\sum_{i=1}^m (h_\theta(x^{(i)})-y)x_j^{(i)}$<br>我们惊奇地发现！！它长得居然跟原来线性回归的梯度下降一样<br>不过特别要注意，这里的$h_\theta(x^{(i)})$跟线性回归有本质上的区别</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DS图-最小生成树</title>
      <link href="/2024/01/05/DS%E5%9B%BE-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"/>
      <url>/2024/01/05/DS%E5%9B%BE-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>复习数据结构打OJ顺便梳理一下Prim和Kruskal求最小生成树的思路</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>根据输入创建无向网。分别用Prim算法和Kruskal算法构建最小生成树。（假设：输入数据的最小生成树唯一。）</p><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>顶点数n\<br>n个顶点\<br>边数m\<br>m条边信息,格式为：顶点1顶点2权值\<br>Prim算法的起点v</p><blockquote><p>6\<br>v1 v2 v3 v4 v5 v6 \<br>10\<br>v1 v2 6\<br>v1 v3 1\<br>v1 v4 5\<br>v2 v3 5\<br>v2 v5 3\<br>v3 v4 5\<br>v3 v5 6\<br>v3 v6 4\<br>v4 v6 2\<br>v5 v6 6\<br>v1</p><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>输出最小生成树的权值之和\<br>对两种算法，按树的生长顺序，输出边信息(Kruskal中边顶点按数组序号升序输出)<br>15\<br>prim:\<br>v1 v3 1\<br>v3 v6 4\<br>v6 v4 2\<br>v3 v2 5\<br>v2 v5 3\<br>kruskal:\<br>v1 v3 1\<br>v4 v6 2\<br>v2 v5 3\<br>v3 v6 4\<br>v2 v3 5</p></blockquote><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="Prim"><a href="#Prim" class="headerlink" title="Prim"></a>Prim</h3><ol><li>维护一个数组dis，表示节点到最小生成树直接距离（有相邻节点在最小生成树中，否则为正无穷，初始为正无穷），已放到最小生成树中则为0。<blockquote><p>当有相邻节点被放入最小生成树中，且与该节点直接距离小于原本dis数组中保存的值时，更新dis中的值</p></blockquote></li><li>维护一个数组parent，表示节点与最小生成树相连时，直接连接的另一节点<blockquote><p>无需初始化，当dis更新时同步更新parent</p></blockquote></li><li>从起点开始，初始化各点的dis值为各点到起点的直接距离（如有），同时更新parent值也为起点，将起点放入最小生成树（dis值设为0）。</li><li>进入循环，每次取出dis值最小且非0的节点，将该节点与其父节点相连的边纳入最小生成树，并更新相邻节点的dis值和parent值。当最小生成树的边为节点数-1时退出循环<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Edge* <span class="title">Prim</span><span class="params">(<span class="type">const</span> string&amp; start)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> startIndex = <span class="built_in">strToIndex</span>(start);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)</span><br><span class="line">        dis[i] = INF;</span><br><span class="line">    dis[startIndex] = <span class="number">0</span>;                    <span class="comment">//将起点放入最小生成树（dis值设为0）。</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)&#123;</span><br><span class="line">        <span class="keyword">if</span>(mat[i][startIndex])&#123;</span><br><span class="line">            dis[i] = mat[i][startIndex];    <span class="comment">//初始化各点的dis值为各点到起点的直接距离（如有）</span></span><br><span class="line">            parent[i] = startIndex;         <span class="comment">//同时更新parent值也为起点，</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Edge *mst = <span class="keyword">new</span> Edge[n<span class="number">-1</span>];</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(cnt != n<span class="number">-1</span>)&#123;<span class="comment">//当最小生成树的边为节点数-1时退出循环</span></span><br><span class="line">        <span class="comment">//每次取出dis值最小且非0的节点</span></span><br><span class="line">        <span class="type">int</span> minIndex, min = INF;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(dis[i] &amp;&amp; dis[i] &lt; min)</span><br><span class="line">                minIndex = i, min = dis[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//将该节点与其父节点相连的边纳入最小生成树</span></span><br><span class="line">        mst[cnt++] = <span class="built_in">Edge</span>(parent[minIndex], minIndex, min);</span><br><span class="line">        dis[minIndex] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//更新相邻节点的dis值和parent值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(mat[i][minIndex] &amp;&amp; mat[i][minIndex] &lt; dis[i])&#123;</span><br><span class="line">                dis[i] = mat[i][minIndex];</span><br><span class="line">                parent[i] = minIndex;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> mst;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kruscal"><a href="#Kruscal" class="headerlink" title="Kruscal"></a>Kruscal</h3></li><li>维护一个优先队列pq，在初始化边的时候将每一条边放入优先队列，边的权值越小越靠前。</li><li>维护一个并查集set，当有边被纳入最小生成树时，将该边两点作并操作。一条边的两个点在同一集合中时,说明再将该边纳入最小生成树会使树成环，故丢弃该边。</li><li>进入循环，每次循环取出pq的队头（未纳入最小生成树的最短的一条边），判断该边是否合法（是否会使最小生成树成环），合法则将其纳入最小生成树，否则丢弃。当最小生成树的边为节点数-1时退出循环<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//并查集</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Set</span>&#123;</span><br><span class="line">    <span class="type">int</span> *parent;</span><br><span class="line">    <span class="built_in">Set</span>(<span class="type">int</span> range)&#123;</span><br><span class="line">        parent = <span class="keyword">new</span> <span class="type">int</span>[range];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;range;++i)</span><br><span class="line">            parent[i] = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">join</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">        parent[a] = b;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">root</span><span class="params">(<span class="type">int</span> a)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(parent[a] == <span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">return</span> a;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">root</span>(parent[a]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_same</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">root</span>(a) == <span class="built_in">root</span>(b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//Kruskal</span></span><br><span class="line"><span class="function">Edge *<span class="title">Kruskal</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Edge *mst = <span class="keyword">new</span> Edge[n<span class="number">-1</span>];</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(!pq.<span class="built_in">empty</span>() &amp;&amp; cnt != n<span class="number">-1</span>)&#123;</span><br><span class="line">        Edge temp = pq.<span class="built_in">top</span>();</span><br><span class="line">        pq.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span>(!set-&gt;<span class="built_in">is_same</span>(temp.index1, temp.index2))&#123;</span><br><span class="line">            mst[cnt++] = temp;</span><br><span class="line">            set-&gt;<span class="built_in">join</span>(temp.index1, temp.index2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> mst;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="处理输入输出"><a href="#处理输入输出" class="headerlink" title="处理输入输出"></a>处理输入输出</h3><ol><li>节点：string类型数组nodes保存所有节点，下标表示节点的id，方便用二维数组表示节点间的权值</li><li>最小生成树：自定义Edge类型表示一条边，成员变量int index1, index2, val 分别表示边的两个端点下标及边的权重。最小生成树为Edge类型的数组，即一组边。</li><li>int类型二维数组mat(matrix)保存保存节点间的权值(Prim中用于更新dis数组)\<br>Edge类型数组保存输入的每一条边，在初始化时push到优先队列中(Kruskal中用于贪心)<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> INF 0x3F3F3F3F</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX 10000</span></span><br><span class="line"><span class="comment">//图</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Graph</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    string nodes[MAX];</span><br><span class="line">    <span class="comment">//用于Prim算法的mat二维数组，dis数组和parent数组</span></span><br><span class="line">    <span class="type">int</span> mat[MAX][MAX];</span><br><span class="line">    <span class="type">int</span> dis[MAX];  </span><br><span class="line">    <span class="type">int</span> parent[MAX];    </span><br><span class="line">    <span class="comment">//用于Kruskal算法的最小堆（优先队列）和并查集</span></span><br><span class="line">    priority_queue&lt;Edge&gt; pq;    </span><br><span class="line">    Set *set;    </span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n, m;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">// init nodes</span></span><br><span class="line">       std::cin&gt;&gt;n;</span><br><span class="line">       <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)</span><br><span class="line">           std::cin&gt;&gt;nodes[i];</span><br><span class="line">        <span class="comment">//init edges</span></span><br><span class="line">       <span class="built_in">memset</span>(mat, <span class="number">0</span>, <span class="built_in">sizeof</span>(mat));</span><br><span class="line">       std::cin&gt;&gt;m;</span><br><span class="line">       string node1, node2;</span><br><span class="line">       <span class="type">int</span> val;</span><br><span class="line">       <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;++i)&#123;</span><br><span class="line">           std::cin&gt;&gt;node1&gt;&gt;node2&gt;&gt;val;</span><br><span class="line">           <span class="type">int</span> index1 = <span class="built_in">strToIndex</span>(node1), index2 = <span class="built_in">strToIndex</span>(node2);</span><br><span class="line">           /初始化mat二维数组</span><br><span class="line">           mat[index1][index2] = val;</span><br><span class="line">           mat[index2][index1] = val;</span><br><span class="line">           <span class="comment">//初始化优先队列</span></span><br><span class="line">           pq.<span class="built_in">push</span>(<span class="built_in">Edge</span>(index1, index2, val));</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="comment">// init union-find-set</span></span><br><span class="line">       set = <span class="keyword">new</span> <span class="built_in">Set</span>(n);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">strToIndex</span><span class="params">(<span class="type">const</span> string &amp;str)</span></span>&#123;</span><br><span class="line">        <span class="comment">//找到字符串对应id</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;++i)</span><br><span class="line">            <span class="keyword">if</span>(str == nodes[i])</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Edge* <span class="title">Prim</span><span class="params">(<span class="type">const</span> string&amp; start)</span></span>&#123;</span><br><span class="line">        <span class="comment">//上面已实现</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Edge *<span class="title">Kruskal</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//上面已实现</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">FindMST</span><span class="params">(<span class="type">const</span> string &amp;start)</span></span>&#123;</span><br><span class="line">        Edge *mst1 = <span class="built_in">Prim</span>(start);</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;++i)</span><br><span class="line">            sum += mst1[i].val;</span><br><span class="line">        std::cout&lt;&lt;sum&lt;&lt;std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;prim:&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;++i)&#123;</span><br><span class="line">            std::cout&lt;&lt;nodes[mst1[i].index1]&lt;&lt;<span class="string">&quot; &quot;</span></span><br><span class="line">                     &lt;&lt;nodes[mst1[i].index2]&lt;&lt;<span class="string">&quot; &quot;</span></span><br><span class="line">                     &lt;&lt;mst1[i].val&lt;&lt;std::endl;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;kruskal:&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">        Edge *mst2 = <span class="built_in">Kruskal</span>();</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;++i)&#123;</span><br><span class="line">            std::cout&lt;&lt;nodes[mst2[i].index1]&lt;&lt;<span class="string">&quot; &quot;</span></span><br><span class="line">                     &lt;&lt;nodes[mst2[i].index2]&lt;&lt;<span class="string">&quot; &quot;</span></span><br><span class="line">                     &lt;&lt;mst2[i].val&lt;&lt;std::endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Graph *graph = <span class="keyword">new</span> Graph;</span><br><span class="line">    graph-&gt;<span class="built_in">init</span>();</span><br><span class="line">    </span><br><span class="line">    string start;</span><br><span class="line">    std::cin&gt;&gt;start;</span><br><span class="line">    graph-&gt;<span class="built_in">FindMST</span>(start);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prim算法 </tag>
            
            <tag> Kruskal算法 </tag>
            
            <tag> 最小生成树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI导论实验之网格游戏（马尔可夫决策）</title>
      <link href="/2023/12/22/AI%E5%AF%BC%E8%AE%BA%E5%AE%9E%E9%AA%8C%E4%B9%8B%E7%BD%91%E6%A0%BC%E6%B8%B8%E6%88%8F%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%EF%BC%89/"/>
      <url>/2023/12/22/AI%E5%AF%BC%E8%AE%BA%E5%AE%9E%E9%AA%8C%E4%B9%8B%E7%BD%91%E6%A0%BC%E6%B8%B8%E6%88%8F%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="问题1：价值迭代"><a href="#问题1：价值迭代" class="headerlink" title="问题1：价值迭代"></a>问题1：价值迭代</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>实验来源于<a href="https://inst.eecs.berkeley.edu/~cs188/fa19/project3/">伯克利CS188</a>\<br>项目已将问题形式化为<strong>马尔可夫决策过程</strong>（Markov decision process，MDP），即MDP\<br>在 valueIterationAgents.py 中，ValueIterationAgent 类构造方法接收MDP（无折扣因子）及折扣因子\<br>我们需要做的是实现以下方法包括：</p><ul><li>runValueIteration: 执行价值迭代</li><li>computeActionFromValues(state)：根据self.values给出的值函数计算最佳行动</li><li>computeQValueFromValues(state, action)：返回根据self.values给出的值函数给出的（状态, 动作）对的Q值<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2></li></ul><p><a href="https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/#34-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B">马尔可夫决策过程</a>由五元组$<S,A,P,r,γ>$构成，项目的MDP已经提供了以下接口：</p><ul><li>mdp.getStates()：获取MDP的所有状态</li><li>mdp.getPossibleActions(state)：当状态确定时，可能发生的所有动作</li><li>mdp.getTransitionStatesAndProbs(state, action)：当状态和动作确定时，所有可能的下一状态及其发生的概率</li><li>mdp.getReward(state, action, nextState)：当状态，动作及下一状态确定时，得到的即时回报</li><li>mdp.isTerminal(state)：判断状态是否为终止状态</li></ul><p>价值迭代算法\<br>对每个状态$s$, 找到动作$a$使动作价值最大为$Q_m$, 更新s对应的状态价值$V=Q_m$及对应的策略为$a$</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 价值迭代模板</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">runValueIteration</span>(<span class="params">self</span>):</span><br><span class="line">    delta = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        new_values = util.Counter()     <span class="comment"># new values after iteration</span></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> self.mdp.getStates():</span><br><span class="line">            max_value = -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> self.mdp.getPossibleActions(state):</span><br><span class="line">                value = self.computeQValueFromValues(state=state, action=action)</span><br><span class="line">                max_value = <span class="built_in">max</span>(value, max_value)</span><br><span class="line">            new_values[state] = max_value <span class="keyword">if</span> max_value != -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            delta = <span class="built_in">max</span>(<span class="built_in">abs</span>(new_values[state] - self.values[state]), delta))</span><br><span class="line">        self.values = new_values        <span class="comment"># update values, also policy</span></span><br><span class="line">        <span class="keyword">if</span> self.theta &lt; delta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 实验要求命令行输入迭代次数 -i &lt;iterations&gt;, 故修改为</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">runValueIteration</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.iterations):</span><br><span class="line">        new_values = util.Counter()     <span class="comment"># new values after iteration</span></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> self.mdp.getStates():</span><br><span class="line">            max_value = -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> self.mdp.getPossibleActions(state):</span><br><span class="line">                value = self.computeQValueFromValues(state=state, action=action)</span><br><span class="line">                max_value = <span class="built_in">max</span>(value, max_value)</span><br><span class="line">            new_values[state] = max_value <span class="keyword">if</span> max_value != -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            delta = <span class="built_in">max</span>(<span class="built_in">abs</span>(new_values[state] - self.values[state]), delta))</span><br><span class="line">        self.values = new_values        <span class="comment"># update values, also policy</span></span><br></pre></td></tr></table></figure><p>计算动作价值Q<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据公式应该使用的函数模板</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeQValueFromValues</span>(<span class="params">self, state, action</span>):</span><br><span class="line">    <span class="built_in">sum</span> = self.mdp.getReward(state, action)</span><br><span class="line">    <span class="keyword">for</span> nextState, prob <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(state=state, action=action):</span><br><span class="line">        <span class="built_in">sum</span> += self.discount * prob * self.getValue(nextState)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"><span class="comment"># 由于mdp提供的getReward需要三个参数state, action, nextState, 故修改为</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeQValueFromValues</span>(<span class="params">self, state, action</span>):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> nextState, prob <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(state=state, action=action):</span><br><span class="line">        <span class="built_in">sum</span> += prob * </span><br><span class="line">            (self.mdp.getReward(state, action) + self.discount * self.getValue(nextState) )</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br></pre></td></tr></table></figure><br>根据state得到最好的action<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">computeActionFromValues</span>(<span class="params">self, state</span>):</span><br><span class="line">    max_value = -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">    best_action = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> self.mdp.getPossibleActions(state):</span><br><span class="line">        <span class="comment"># 与价值迭代中求Q值最大的action类似</span></span><br><span class="line">        value = self.computeQValueFromValues(state=state, action=action)</span><br><span class="line">        <span class="keyword">if</span> value &gt; max_value:</span><br><span class="line">            best_action = action</span><br><span class="line">            max_value = value</span><br><span class="line">    <span class="keyword">return</span> best_action</span><br></pre></td></tr></table></figure></p><h1 id="问题2：过桥分析"><a href="#问题2：过桥分析" class="headerlink" title="问题2：过桥分析"></a>问题2：过桥分析</h1><p>由题，noise表示智能体在执行操作时以意外的后继状态结束的频率，有一定概率不遵循我们计算得到的策略。故将noise设置为0.0，即可通过测试。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">question2</span>():</span><br><span class="line">    answerDiscount = <span class="number">0.9</span></span><br><span class="line">    answerNoise = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> answerDiscount, answerNoise</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 课程实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 马尔可夫决策过程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java利用RMI实现http请求服务</title>
      <link href="/2023/12/17/Java%E5%88%A9%E7%94%A8RMI%E5%AE%9E%E7%8E%B0http%E8%AF%B7%E6%B1%82%E6%9C%8D%E5%8A%A1/"/>
      <url>/2023/12/17/Java%E5%88%A9%E7%94%A8RMI%E5%AE%9E%E7%8E%B0http%E8%AF%B7%E6%B1%82%E6%9C%8D%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="一、题目描述"><a href="#一、题目描述" class="headerlink" title="一、题目描述"></a>一、题目描述</h1><p>编写Java程序，实现RMI远程调用。客户端指定某个http网站，把这个网址传递给服务器端，服务器端提供http下载服务，通过http get 请求访问http网站，把对应的html文件返回给客户端</p><h1 id="二、实现思路"><a href="#二、实现思路" class="headerlink" title="二、实现思路"></a>二、实现思路</h1><h2 id="1-客户端"><a href="#1-客户端" class="headerlink" title="1. 客户端"></a>1. 客户端</h2><p>在GUI界面输入访问的网址，按下按钮提交请求\<br>监听按钮点击事件，引用远程访问的类，将处理结果反馈到文本框中</p><h2 id="2-服务器端"><a href="#2-服务器端" class="headerlink" title="2. 服务器端"></a>2. 服务器端</h2><p>定义服务接口，需要继承自Remote类\<br>定义实现服务接口的类，在类中编写业务逻辑，需要继承 UnicastRemoteObject 类\<br>在主函数中注册服务</p><h1 id="三、实现过程"><a href="#三、实现过程" class="headerlink" title="三、实现过程"></a>三、实现过程</h1><h2 id="1-客户端-1"><a href="#1-客户端-1" class="headerlink" title="1. 客户端"></a>1. 客户端</h2><p>编写基本的GUI界面(不熟。。要是写前端三件套就好了<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RMIClient</span> <span class="keyword">extends</span> <span class="title class_">JFrame</span> &#123;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;  </span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">RMIClient</span>(<span class="string">&quot;Client&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">RMIClient</span><span class="params">(String <span class="keyword">var</span>)</span>&#123;  </span><br><span class="line">        <span class="built_in">super</span>(<span class="keyword">var</span>);  </span><br><span class="line">        <span class="built_in">this</span>.setSize(<span class="number">1000</span>, <span class="number">1500</span>);  </span><br><span class="line">        <span class="built_in">this</span>.setTitle(<span class="string">&quot;RAMClient&quot;</span>);  </span><br><span class="line"></span><br><span class="line">        <span class="type">JPanel</span> <span class="variable">jp1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JPanel</span>();  </span><br><span class="line">        <span class="type">JLabel</span> <span class="variable">jl1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JLabel</span>(<span class="string">&quot;Please type the URL you want to visit: &quot;</span>);  </span><br><span class="line">        <span class="type">JTextField</span> <span class="variable">jt1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JTextField</span>(<span class="number">30</span>);  </span><br><span class="line">        <span class="type">JButton</span> <span class="variable">jb1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JButton</span>(<span class="string">&quot;Confirm&quot;</span>);  </span><br><span class="line">        jp1.add(jl1);  </span><br><span class="line">        jp1.add(jt1);  </span><br><span class="line">        jp1.add(jb1);  </span><br><span class="line"></span><br><span class="line">        <span class="type">JTextArea</span> <span class="variable">jTextArea</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JTextArea</span>();  </span><br><span class="line">        jTextArea.setSize(<span class="number">250</span>, <span class="number">1200</span>);  </span><br><span class="line">        jTextArea.setEditable(<span class="literal">false</span>);  </span><br><span class="line">        <span class="type">JScrollPane</span> <span class="variable">sp</span> <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">JScrollPane</span>();  </span><br><span class="line">        sp.setViewportView(jTextArea);</span><br></pre></td></tr></table></figure><br>监听JButton事件，将返回数据逐行添加到JTextArea<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">jb1.addActionListener((e)-&gt;&#123;  </span><br><span class="line">    <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> jt1.getText();  </span><br><span class="line">    File htmlFile;  </span><br><span class="line">    <span class="comment">// lookup method to find reference  </span></span><br><span class="line">    <span class="keyword">try</span>&#123;  </span><br><span class="line">        <span class="type">DownloadService</span> <span class="variable">access</span> <span class="operator">=</span> (DownloadService) Naming.lookup(<span class="string">&quot;rmi://localhost:1099/download&quot;</span>);  </span><br><span class="line">        htmlFile = access.getHTMLfile(input);  </span><br><span class="line">        <span class="keyword">if</span>(htmlFile != <span class="literal">null</span>)&#123;  </span><br><span class="line">            <span class="type">BufferedReader</span> <span class="variable">bufferedReader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">FileReader</span>(htmlFile));  </span><br><span class="line">            <span class="type">String</span> <span class="variable">temp</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br><span class="line">            <span class="keyword">while</span>((temp = bufferedReader.readLine()) != <span class="literal">null</span>)&#123;  </span><br><span class="line">                jTextArea.append(temp+<span class="string">&#x27;\n&#x27;</span>);  </span><br><span class="line">                <span class="comment">//System.out.println(temp+&#x27;\n&#x27;);  </span></span><br><span class="line">            &#125;  </span><br><span class="line">            bufferedReader.close();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;<span class="keyword">catch</span> (Exception err)&#123;  </span><br><span class="line">        err.printStackTrace();  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><br>将组件添加到JFrame中并显示<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">        <span class="built_in">this</span>.setLayout(<span class="keyword">new</span> <span class="title class_">GridLayout</span>(<span class="number">3</span>, <span class="number">4</span>));  </span><br><span class="line">        <span class="built_in">this</span>.add(jp1);  </span><br><span class="line">        <span class="built_in">this</span>.add(sp);  </span><br><span class="line">        <span class="built_in">this</span>.setVisible(<span class="literal">true</span>);  </span><br><span class="line">        <span class="built_in">this</span>.pack();  </span><br><span class="line">        <span class="built_in">this</span>.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="2-服务器端-1"><a href="#2-服务器端-1" class="headerlink" title="2. 服务器端"></a>2. 服务器端</h2><p>定义服务接口DownloadService<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">DownloadService</span> <span class="keyword">extends</span> <span class="title class_">Remote</span> &#123;  </span><br><span class="line">    File <span class="title function_">getHTMLfile</span><span class="params">(String urlStr)</span> <span class="keyword">throws</span> RemoteException, MalformedURLException;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>定义实现上述接口的服务类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DownloadServiceImpl</span> <span class="keyword">extends</span> <span class="title class_">UnicastRemoteObject</span> <span class="keyword">implements</span> <span class="title class_">DownloadService</span> &#123;  </span><br><span class="line">    <span class="keyword">protected</span> <span class="title function_">DownloadServiceImpl</span><span class="params">()</span> <span class="keyword">throws</span> RemoteException&#123;  </span><br><span class="line">        <span class="built_in">super</span>();  </span><br><span class="line">    &#125;  </span><br></pre></td></tr></table></figure><br>初始化Http连接对象以及响应文件对象<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> File <span class="title function_">getHTMLfile</span><span class="params">(String urlStr)</span> <span class="keyword">throws</span> RemoteException, MalformedURLException &#123;  </span><br><span class="line">    <span class="comment">// initialization  </span></span><br><span class="line">    <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;result.html&quot;</span>);  </span><br><span class="line">    <span class="type">FileWriter</span> <span class="variable">fileWriter</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="keyword">if</span>(!file.exists())&#123;  </span><br><span class="line">            file.createNewFile();  </span><br><span class="line">        &#125;  </span><br><span class="line">        fileWriter = <span class="keyword">new</span> <span class="title class_">FileWriter</span>(file);  </span><br><span class="line">    &#125;<span class="keyword">catch</span> (IOException e)&#123;  </span><br><span class="line">        e.printStackTrace();  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="type">HttpURLConnection</span> <span class="variable">conn</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br><span class="line">    <span class="type">InputStream</span> <span class="variable">inputStream</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br><span class="line">    <span class="type">BufferedReader</span> <span class="variable">bufferedReader</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br></pre></td></tr></table></figure><br>建立连接，逐行读取返回的HTML响应，保存到文件中<br>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment">// create connection  </span></span><br><span class="line">        <span class="keyword">try</span>&#123;  </span><br><span class="line">            <span class="type">URL</span> <span class="variable">url</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URL</span>(urlStr);  </span><br><span class="line">            conn = (HttpURLConnection) url.openConnection();  </span><br><span class="line">            conn.setRequestMethod(<span class="string">&quot;GET&quot;</span>);  </span><br><span class="line">            conn.setConnectTimeout(<span class="number">12000</span>);  </span><br><span class="line">            conn.connect();  </span><br><span class="line">            System.out.println(<span class="string">&quot;Wait for response&quot;</span>);  </span><br><span class="line">            <span class="keyword">if</span>(conn.getResponseCode() == <span class="number">200</span>)&#123;  </span><br><span class="line">                <span class="comment">// if conn success  </span></span><br><span class="line">                inputStream = conn.getInputStream();  </span><br><span class="line">                <span class="keyword">if</span> (inputStream != <span class="literal">null</span>)&#123;  </span><br><span class="line">                    bufferedReader = <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(inputStream, <span class="string">&quot;UTF-8&quot;</span>));  </span><br><span class="line">                    <span class="type">String</span> <span class="variable">temp</span> <span class="operator">=</span> <span class="literal">null</span>;  </span><br><span class="line">                    <span class="keyword">while</span>((temp = bufferedReader.readLine())!= <span class="literal">null</span>)&#123;  </span><br><span class="line">                        fileWriter.write(temp);  </span><br><span class="line">                        fileWriter.write(<span class="string">&#x27;\n&#x27;</span>);  </span><br><span class="line">                    &#125;  </span><br><span class="line">                    fileWriter.close();  </span><br><span class="line">                &#125;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;  </span><br><span class="line">            e.printStackTrace();  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> file;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>在主函数中注册服务<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RMIServer</span> &#123;  </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;  </span><br><span class="line">        <span class="keyword">try</span>&#123;  </span><br><span class="line">            <span class="type">DownloadService</span> <span class="variable">downloadService</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DownloadServiceImpl</span>();  </span><br><span class="line">            LocateRegistry.createRegistry(<span class="number">1099</span>);  </span><br><span class="line">            Naming.rebind(<span class="string">&quot;rmi://localhost:1099/download&quot;</span>, downloadService);  </span><br><span class="line">            System.out.println(<span class="string">&quot;Server is ready!&quot;</span>);  </span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;  </span><br><span class="line">            e.printStackTrace();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="三、运行结果"><a href="#三、运行结果" class="headerlink" title="三、运行结果"></a>三、运行结果</h1><p>启动服务端<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">D:\<span class="title">idea</span> <span class="title">project</span>\<span class="title">javaCourrse</span>\<span class="title">javaEX6</span>\<span class="title">javaEX6</span>\<span class="title">src</span>&gt;<span class="title">java</span> <span class="title">RMIdemo</span>/<span class="title">RMIServer</span></span></span><br><span class="line"><span class="function"><span class="title">Server</span> <span class="title">is</span> <span class="title">ready</span>!</span></span><br></pre></td></tr></table></figure><br>启动客户端<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">D:\<span class="title">idea</span> <span class="title">project</span>\<span class="title">javaCourrse</span>\<span class="title">javaEX6</span>\<span class="title">javaEX6</span>\<span class="title">src</span>&gt;<span class="title">java</span> <span class="title">RMIdemo</span>/<span class="title">RMIClient</span></span></span><br></pre></td></tr></table></figure></p><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddf754dff32644dcb632deb737b69fff~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=771&amp;h=371&amp;s=16584&amp;e=png&amp;b=f5f5f5" alt="image.png"><br>输入网址并点击按钮<br><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fa20d157c3074b199839f6334863ecbb~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=771&amp;h=354&amp;s=24419&amp;e=png&amp;b=f4f4f4" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 课程实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> RMI </tag>
            
            <tag> http </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java利用UDP实现客户端之间通信</title>
      <link href="/2023/12/17/Java%E5%88%A9%E7%94%A8UDP%E5%AE%9E%E7%8E%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"/>
      <url>/2023/12/17/Java%E5%88%A9%E7%94%A8UDP%E5%AE%9E%E7%8E%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><p>利用数据报通信方式编写一程序，该程序生成两个客户端，一个服务器端，两个客户端可以相互进行简短的文字交流。</p><h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>为使客户端能够接收到其他客户端的数据报，客户端在创建时应该<strong>指定端口号</strong>，开启接收数据的线程和等待发送消息的线程，当接收到数据或发送数据后，重启线程。并且发送方在数据报中应包含接收端的地址，由服务器端对地址进行解析并转达数据包。</p><h1 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h1><h2 id="1-服务器端："><a href="#1-服务器端：" class="headerlink" title="1. 服务器端："></a>1. 服务器端：</h2><p>在Server.java中实现Server类，静态变量指定服务器端口，接收数据报的大小</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Server</span> &#123;  </span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">serverPort</span> <span class="operator">=</span> <span class="number">8000</span>;  </span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">packetSize</span> <span class="operator">=</span> <span class="number">1024</span>;  </span><br></pre></td></tr></table></figure><p>进入主函数时，初始化数据报套接字及接收数据报</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;  </span><br><span class="line">        <span class="type">DatagramSocket</span> <span class="variable">server</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DatagramSocket</span>(serverPort);  </span><br><span class="line">        <span class="type">byte</span>[] data = <span class="keyword">new</span> <span class="title class_">byte</span>[packetSize];  </span><br><span class="line">        <span class="type">DatagramPacket</span> <span class="variable">packet</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DatagramPacket</span>(data, packetSize);  </span><br><span class="line">        System.out.println(<span class="string">&quot;server is ready!!&quot;</span>);</span><br></pre></td></tr></table></figure><p>使用死循环处理每一个客户端发送的数据报;\<br>将数据报分割为地址及信息两部分;\<br>根据地址及信息创建新数据报，并发送到目标客户端，完成对消息的转述</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;  </span><br><span class="line">    server.receive(packet);  </span><br><span class="line">    System.out.println(<span class="string">&quot;server has received data from sender&quot;</span>);  </span><br><span class="line">    <span class="type">String</span> <span class="variable">dataStr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(data, <span class="number">0</span>, packet.getLength());  </span><br><span class="line">    String[] dataSegments = dataStr.split(<span class="string">&quot; &quot;</span>);  </span><br><span class="line">    <span class="keyword">if</span>(dataSegments.length &gt;= <span class="number">2</span>)&#123;  </span><br><span class="line">        <span class="type">InetAddress</span> <span class="variable">receiverAddress</span> <span class="operator">=</span> InetAddress.getByName(dataSegments[<span class="number">0</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">0</span>]);  </span><br><span class="line">        <span class="type">int</span> <span class="variable">receiverPort</span> <span class="operator">=</span> Integer.parseInt(dataSegments[<span class="number">0</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]);  </span><br><span class="line">        <span class="type">DatagramPacket</span> <span class="variable">trulyData</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DatagramPacket</span>(dataSegments[<span class="number">1</span>].getBytes(),  </span><br><span class="line">        dataSegments[<span class="number">1</span>].length(), receiverAddress, receiverPort);  </span><br><span class="line">        System.out.println(<span class="string">&quot;the message: &quot;</span>+dataSegments[<span class="number">1</span>]);  </span><br><span class="line">        server.send(trulyData);  </span><br><span class="line">        System.out.println(<span class="string">&quot;server has sent data to receiver&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-客户端"><a href="#2-客户端" class="headerlink" title="2. 客户端"></a>2. 客户端</h2><p>客户端在创建时需指定端口号，这里使用命令行参数传入</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Client</span> &#123;  </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;  </span><br><span class="line">        <span class="type">int</span> port;  </span><br><span class="line">        <span class="keyword">if</span> (args.length &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">            port = Integer.parseInt(args[<span class="number">0</span>]);  </span><br><span class="line">            <span class="type">DatagramSocket</span> <span class="variable">socket</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DatagramSocket</span>(port);  </span><br><span class="line">            System.out.println(<span class="string">&quot;client is ready!!&quot;</span>);</span><br></pre></td></tr></table></figure><p>创建接收消息的线程及发送消息的线程等待输入</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">            <span class="comment">// create a receiver thread  </span></span><br><span class="line">            <span class="type">Thread</span> <span class="variable">receiveThread</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">ReceiverRunnable</span>(socket));  </span><br><span class="line">            receiveThread.start(); <span class="comment">// receive message at any time  </span></span><br><span class="line">            <span class="comment">// create a sender thread  </span></span><br><span class="line">            <span class="type">Thread</span> <span class="variable">sendThread</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">ClientSenderRunnable</span>(socket));  </span><br><span class="line">            sendThread.start();  </span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;  </span><br><span class="line">            System.out.println(<span class="string">&quot;you must indicate the port by adding argument&quot;</span>);  </span><br><span class="line">        &#125;   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（1）接收消息的线程</p><p>在Receiver.java中实现一个实现Runnable接口的可执行类\<br>在构造方法中传入客户端数据报套接字，初始化接收端\<br>在静态变量中定义数据报大小，以及接收数据报的字符数组</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ReceiverRunnable</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span> &#123;  </span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">dataSize</span> <span class="operator">=</span> <span class="number">1024</span>;  </span><br><span class="line">    DatagramPacket receivePacket;  </span><br><span class="line">    <span class="type">byte</span>[] data;  </span><br><span class="line">    <span class="keyword">public</span> DatagramSocket socket;  </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ReceiverRunnable</span><span class="params">(DatagramSocket socket)</span>&#123;  </span><br><span class="line">        <span class="built_in">this</span>.socket = socket;  </span><br><span class="line">        data = <span class="keyword">new</span> <span class="title class_">byte</span>[dataSize];  </span><br><span class="line">        receivePacket = <span class="keyword">new</span> <span class="title class_">DatagramPacket</span>(data, dataSize);  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>重写run方法，接收数据报并打印消息\<br>在线程终点启动下一个接收消息的线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span>  </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span>&#123;  </span><br><span class="line">        <span class="keyword">try</span>&#123;  </span><br><span class="line">            socket.receive(receivePacket);  </span><br><span class="line">            System.out.println(<span class="string">&quot;You have received a message: &quot;</span>+<span class="keyword">new</span> <span class="title class_">String</span>(data, dataSize));  </span><br><span class="line">            <span class="type">Thread</span> <span class="variable">thread</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">ReceiverRunnable</span>(socket));  </span><br><span class="line">            thread.start();  </span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;  </span><br><span class="line">            e.fillInStackTrace();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）发送消息的线程</p><p>在ClientSenderRunnable.java中实现一个实现Runnable接口的可执行类\<br>在构造方法中传入客户端数据报套接字，初始化发送端\<br>在静态变量中指定服务器地址及端口，用于初始化数据报</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ClientSenderRunnable</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span>&#123;  </span><br><span class="line">    <span class="keyword">static</span> <span class="type">String</span> <span class="variable">serverName</span> <span class="operator">=</span> <span class="string">&quot;localhost&quot;</span>;  </span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">serverPort</span> <span class="operator">=</span> <span class="number">8000</span>;  </span><br><span class="line">    <span class="keyword">public</span> DatagramSocket socket;  </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ClientSenderRunnable</span><span class="params">(DatagramSocket socket)</span>&#123;  </span><br><span class="line">        <span class="built_in">this</span>.socket = socket;  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>重写run方法，读取键盘输入</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;  </span><br><span class="line">    System.out.println(<span class="string">&quot;type as (address:port message):&quot;</span>);  </span><br><span class="line">    <span class="type">Scanner</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);  </span><br><span class="line">    String dataStr;  </span><br><span class="line">    <span class="keyword">if</span> (scanner.hasNextLine()) &#123;  </span><br><span class="line">        dataStr = scanner.nextLine();  </span><br><span class="line">        System.out.println(<span class="string">&quot;your header and message: &quot;</span> + dataStr);  </span><br><span class="line">        <span class="type">byte</span>[] data = dataStr.getBytes();</span><br></pre></td></tr></table></figure><p>根据静态变量指定的服务端地址打包数据报并发送</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pack up the data  </span></span><br><span class="line">            <span class="keyword">try</span> &#123;  </span><br><span class="line">                <span class="type">InetAddress</span> <span class="variable">serverAddress</span> <span class="operator">=</span> InetAddress.getByName(serverName);  </span><br><span class="line">                <span class="type">DatagramPacket</span> <span class="variable">packet</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DatagramPacket</span>(data, data.length, serverAddress, serverPort);  </span><br><span class="line">                <span class="comment">// create socket and send data packed  </span></span><br><span class="line">                socket.send(packet);  </span><br><span class="line">                System.out.println(<span class="string">&quot;client has sent data to server!!&quot;</span>);  </span><br><span class="line">            &#125; <span class="keyword">catch</span> (UnknownHostException e) &#123;  </span><br><span class="line">                e.printStackTrace();  </span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;  </span><br><span class="line">                e.printStackTrace();  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>在线程终点启动下一个发送消息的线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">        <span class="type">Thread</span> <span class="variable">thread</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">ClientSenderRunnable</span>(socket));  </span><br><span class="line">        thread.start();  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p>启动服务端<br><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/279f6e561c7041b09a71efc24a4ba2b6~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667\&amp;h=74\&amp;s=197908\&amp;e=png\&amp;b=0d0d0d" alt="image.png"></p><p>启动两个客户端<br><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fd72f59f5de74c16ae9a8baa6cf850de~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=74&amp;s=197908&amp;e=png&amp;b=0d0d0d" alt="image.png"><br><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a946f76f3604fb2b7a8b0a7b37b728a~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=74&amp;s=197908&amp;e=png&amp;b=0d0d0d" alt="image.png"> </p><p>由客户端一(8001端口)向客户端二(8002端口)发送消息<br><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2b13a7bd3fb643deae42af910a4686a9~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=167&amp;s=446520&amp;e=png&amp;b=0c0c0c" alt="image.png"></p><p>服务器端成功接收到消息并转发<br><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e054e46df9e04c0d9bb5c9e119351f6e~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=122&amp;s=326220&amp;e=png&amp;b=0c0c0c" alt="image.png"></p><p>客户端二(8002端口)成功监听到接收的消息<br><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2559eac5a5de4915ab83f0dd84c1aa0f~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=106&amp;s=283451&amp;e=png&amp;b=0d0d0d" alt="image.png"></p><p>同理，由客户端二向客户端一发送消息也同样得以完成</p><p>客户端二：<br><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d10d3f78c39d492fadc0b0f18c78ee7c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=667&amp;h=106&amp;s=283451&amp;e=png&amp;b=0d0d0d" alt="image.png"></p><p>服务器端：<br><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bde43b7626314dcda64a63d90cbcfed5~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=476&amp;h=74&amp;s=141283&amp;e=png&amp;b=0c0c0c" alt="image.png"></p><p>客户端一：<br><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b78f1f8ebb674a1e9cb1dc3722f16c04~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=492&amp;h=27&amp;s=53329&amp;e=png&amp;b=0c0c0c" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 课程实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> UDP </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
